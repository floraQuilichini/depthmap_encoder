{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import pandas as pd\n",
    "from skimage import io, transform\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy import signal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "torch.set_printoptions(precision=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 1, 128, 128])\n",
      "1 torch.Size([1, 1, 128, 128])\n",
      "2 torch.Size([1, 1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntransform = transforms.Compose([\\n    transforms.Pad(12, padding_mode='reflect'),\\n    transforms.ToTensor()])\\n\\ntrainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, depthmap_dir, mask_dir, segmentation_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depthmap_dir (string): Directory with all the depthmaps.\n",
    "            mask_dir (string): Directory with all the masks.\n",
    "            segmentation_dir (string): Directory with all the segmentation of the depthmaps.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.depthmap_dir = depthmap_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        path, dirs, files = next(os.walk(self.depthmap_dir))\n",
    "        file_count = len(files)\n",
    "        return file_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path_d, dirs_d, files_d = next(os.walk(self.depthmap_dir))\n",
    "        img_d_name = os.path.join(self.depthmap_dir, files_d[idx])\n",
    "        image_d = io.imread(img_d_name)\n",
    "        \n",
    "        path_m, dirs_m, files_m = next(os.walk(self.mask_dir))\n",
    "        img_m_name = os.path.join(self.mask_dir, files_m[idx])\n",
    "        image_m = io.imread(img_m_name)\n",
    "        \n",
    "        path_s, dirs_s, files_s = next(os.walk(self.segmentation_dir))\n",
    "        img_s_name = os.path.join(self.segmentation_dir, files_s[idx])\n",
    "        image_s = io.imread(img_s_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image_d = self.transform(image_d)\n",
    "            image_m = self.transform(image_m)\n",
    "            image_s = self.transform(image_s)\n",
    "\n",
    "        return image_d, image_m, image_s\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        if h!= new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top = 0\n",
    "        if w!= new_w:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0\n",
    "     \n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        \n",
    "        image = np.expand_dims(image, axis=2)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class Downsample(object):\n",
    "    \"\"\"Downsample the image\n",
    "\n",
    "    Args:\n",
    "        downsampling_factor (int or tuple): Desired downsampling factor for rows and columns.\n",
    "        If the downsampling factor is an int, then both rows and columns are sampled by the same factor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampling_factor):\n",
    "        assert isinstance(downsampling_factor, (int, tuple))\n",
    "        if isinstance(downsampling_factor, int):\n",
    "            self.downsampling_factor = (downsampling_factor, downsampling_factor)\n",
    "        else:\n",
    "            assert len(downsampling_factor) == 2\n",
    "            self.downsampling_factor = downsampling_factor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        down_fact_h, down_fact_w = self.downsampling_factor\n",
    "        image = image[::down_fact_h,\n",
    "                      ::down_fact_w]\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class ConvertDepthToColor(object):\n",
    "    \"\"\" convert a 1xmxn 16-bits depthmap to a 2xmxn 8-bits colormap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if len(image.shape[:]) <3:\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            \n",
    "        h, w = image.shape[:2]\n",
    "        image_r_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color[(image > 2**8 - 1)] = image[(image > 2**8 - 1)] >> 8\n",
    "        image_r_color = image - (image_g_color <<8)\n",
    "\n",
    "        return np.concatenate((image_r_color, image_g_color), axis=2)\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in images to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, images):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        images = images.transpose((2, 0, 1))\n",
    "        images = images.astype(float)\n",
    "        return torch.from_numpy(images)\n",
    "    \n",
    "\n",
    "    \n",
    "class ConvertColorToDepth(object):\n",
    "    \"\"\" convert a 2xmxn 8-bits colormap to a 1xmxn 16-bits depthmap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, color_image):\n",
    "            \n",
    "        depth_image = color_image[:, 0, :, :]\n",
    "        depth_image += color_image[:, 1, :, :] << 8\n",
    "\n",
    "        return depth_image\n",
    "\n",
    "    \n",
    "def show_image_batch(images_batch):\n",
    "    \"\"\"Show image for a batch of samples.\"\"\"\n",
    "    if images_batch.size(1) == 1:\n",
    "        images_batch_normed = images_batch/torch.max(images_batch)\n",
    "        grid = utils.make_grid(images_batch_normed)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    else:\n",
    "        print(images_batch.size())\n",
    "        images_b_batch = torch.zeros(images_batch.size(0), 1, images_batch.size(2) , images_batch.size(3))\n",
    "        images_color_batch = torch.cat((images_batch, images_b_batch), 1) \n",
    "        print(images_color_batch.size())\n",
    "        grid = utils.make_grid(images_color_batch)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Load training dataset\n",
    "\n",
    "#mirror_padding_transform = transforms.Compose([transforms.ToPILImage(), transforms.Pad(padding=12, padding_mode='reflect'), transforms.ToTensor()])\n",
    "    # the  datasets contain depthmaps (first composant), masks (second composant) and segmentations (third composant)\n",
    "transformed_datasets = ImageDataset(depthmap_dir='D:/autoencoder_data/depthmaps/training/dilated', \n",
    "                                mask_dir='D:/autoencoder_data/depthmaps/training/mask',\n",
    "                                segmentation_dir='D:/autoencoder_data/depthmaps/training/segmentation_roipoly',\n",
    "                                transform=transforms.Compose([RandomCrop((384, 640)), Downsample(( 3, 5)), ToTensor()])\n",
    "                                 )\n",
    "dataloader = DataLoader(transformed_datasets, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "for i_batch, batch_images in enumerate(dataloader):\n",
    "    print(i_batch, batch_images[2].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 2:\n",
    "        plt.figure()\n",
    "        show_image_batch(batch_images[2])\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(12, padding_mode='reflect'),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define additionnnal functions\n",
    "def periodic_shuffling(T, C):\n",
    "    T_copy = T.clone()\n",
    "    batch_size = T.size()[0]\n",
    "    H = T.size()[2]\n",
    "    W = T.size()[3]\n",
    "    T = T.view(batch_size, C, H*2, W*2)\n",
    "    \"\"\"\n",
    "    for k in range(C):\n",
    "        for i in range(2*H):\n",
    "            for j in range(2*W):\n",
    "                T[:, k, i, j] = T_copy[:, C*((j&1)<<1)+C*(i&1)+k, i>>1, j>>1]\n",
    "    \"\"\"\n",
    "                \n",
    "    T[:, :, ::2, ::2] = T_copy[:, 0:C, :, :]\n",
    "    T[:, :, 1::2, ::2] = T_copy[:, C:2*C, :, :]\n",
    "    T[:, :, ::2, 1::2] = T_copy[:, 2*C:3*C, :, :]\n",
    "    T[:, :, 1::2, 1::2] = T_copy[:, 3*C:4*C, :, :]\n",
    "\n",
    "    return T\n",
    "    \n",
    "    \n",
    "def mirror_padding(x, padding_size):\n",
    "    up_line = x[:, :, 0:padding_size, :].flip(2)\n",
    "    left_col = x[:, :, :, 0:padding_size].flip(3)\n",
    "    right_col = x[:, :, :, -padding_size:].flip(3)\n",
    "    bottom_line = x[:, :, -padding_size:, :].flip(2)\n",
    "    left_up_corner = left_col[:, :, 0:padding_size, :].flip(2)\n",
    "    right_up_corner = right_col[:, :, 0:padding_size, :].flip(2)\n",
    "    left_bottom_corner = left_col[:, :, -padding_size:, :].flip(2)\n",
    "    right_bottom_corner = right_col[:, :, -padding_size:, :].flip(2)\n",
    "\n",
    "    x_mirror_pad = torch.cat((torch.cat((left_up_corner, up_line, right_up_corner), 3), torch.cat((left_col, x, right_col), 3), torch.cat((left_bottom_corner, bottom_line, right_bottom_corner), 3)), 2)\n",
    "    return x_mirror_pad\n",
    "\n",
    "\n",
    "\n",
    "def normalize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    max_value = torch.max(x)\n",
    "    min_value = torch.min(x)\n",
    "    radius = max(max_value, abs(min_value))\n",
    "    x_centered_normalized = x_centered/radius\n",
    "    return x_centered_normalized, radius, mean_channels\n",
    "\n",
    "def standardize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    var = torch.sum(x_centered**2, (2, 3))/(x.size()[2]*x.size()[3])\n",
    "    x_standardized = x_centered / torch.sqrt(var.view(x_centered.size()[0], x_centered.size()[1], 1, 1))\n",
    "    return x_standardized, mean_channels, var\n",
    "    \n",
    "def denormalize_output(x, radius, mean_channels):\n",
    "    x_denormalized = x*radius\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_denormalized_centered = x_denormalized + mean_channels_images\n",
    "    return x_denormalized_centered\n",
    "\n",
    "def destandardize_output(x, mean_channels, var):\n",
    "    x_rescaled = x*torch.sqrt(var.view(x.size()[0], x.size()[1], 1, 1))\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_destandardized = x_rescaled + mean_channels_images\n",
    "    return x_destandardized\n",
    "\n",
    "\n",
    "\n",
    "def compute_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    return gsm\n",
    "\n",
    "\n",
    "def sum_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    #gsm_sum = (torch.log2(gsm)).sum()\n",
    "    gsm_sum = gsm.sum()\n",
    "    return gsm_sum\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    h = x.size(-1)\n",
    "    w = x.size(-2)\n",
    "    #print(\"input : \", input)\n",
    "    coeff = torch.sqrt(1.0 / (2 * np.pi * var))\n",
    "    #print(\"coeff : \", coeff)\n",
    "    x_resized = x.repeat((nScale, 1, 1, 1, 1))\n",
    "    #print(\"input : \", input_resized)\n",
    "    exponent = (-0.5*(x_resized ** 2)/var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w))\n",
    "    #print(\"exponent : \", exponent)\n",
    "    coeffs_resized = coeff.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    #print(\"coeffs : \", coeffs_resized)\n",
    "    gaussian = coeffs_resized * torch.exp(exponent)\n",
    "    #print(\"gaussian : \", gaussian)\n",
    "    phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    phi_gaussian = phi_resized*gaussian\n",
    "    sum_phi_gaussian = phi_gaussian.sum(dim=0)\n",
    "    #print(\"sum over scales : \", sum_phi_gaussian)\n",
    "    result = -torch.log2(sum_phi_gaussian).sum()\n",
    "\n",
    "    return result\n",
    "    \"\"\"\n",
    "    \n",
    "def compute_mask(nb_ones, dims):\n",
    "    mask = torch.zeros(dims)\n",
    "    indices = np.arange(nb_ones)\n",
    "    mask_flatten = mask.view(-1, 1, 1, 1)\n",
    "    mask_flatten[indices] = 1\n",
    "    mask_reshaped = mask_flatten.view(dims)\n",
    "    return mask_reshaped\n",
    "\n",
    "\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    integral_u = torch.trapz(gsm_sum, u)\n",
    "    #print(\"gsm sum : \", gsm_sum)\n",
    "    #print(\"integral over u : \", integral_u)\n",
    "    entropy = -torch.log2(integral_u)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def mean_bit_per_px(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = []\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    nb_bits = (-torch.log2(torch.clamp(integral_u, min=np.exp(-10**2), max=1))).sum()\n",
    "    if nb_bits.isnan():\n",
    "        print(\"nb bits is nan\")\n",
    "    if nb_bits < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"nb_bits negative : \", nb_bits)\n",
    "    return nb_bits/reduce(lambda x, y: x*y, list(x_quantized.size()))\n",
    "\n",
    "\n",
    "def entropy_dist(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = []\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    prob = torch.clamp(integral_u, min=np.exp(-10**2), max=1)\n",
    "    entropy = (-prob*torch.log2(prob)).sum()\n",
    "    if entropy.isnan():\n",
    "        print(\"entropy is nan\")\n",
    "    if entropy < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"entropy negative : \", entropy)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def distortion_pc(x, x_reconstructed, focal_length, skew, scaling_factor, image_size, principal_point):\n",
    "    # compute intrinsic matrix and its inverse\n",
    "    K = torch.tensor([[focal_length, skew, principal_point[0]], [ 0.0, focal_length, principal_point[1]], [0.0, 0.0, 1.0]])\n",
    "    K_ext = torch.eye(4)\n",
    "    K_ext[0:3:1, 0:3:1] = K\n",
    "    K_ext_inv = torch.inverse(K_ext).cuda()\n",
    "    \n",
    "    size_batch = x.size()[0]\n",
    "    total_mse = 0.0\n",
    "    for example in range(size_batch):\n",
    "        # compute x,y,z coords from u,v coords and gray-level value\n",
    "        z_reconstructed = x_reconstructed[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        z = x[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        bool_matrix = torch.logical_and(z_reconstructed != 0 , z != 0)\n",
    "            # for reconstructed depthmap\n",
    "        v_reconstructed = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v_reconstructed = v_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        u_reconstructed = (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u_reconstructed = u_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        z_reconstructed = z_reconstructed[bool_matrix].view(1, -1)\n",
    "        homogeneous_reconstructed_camera_points = torch.cat((u_reconstructed, v_reconstructed, torch.ones(1, u_reconstructed.size()[1]).cuda(), 1.0/z_reconstructed), 0)\n",
    "        homogeneous_reconstructed_coords = z_reconstructed*(torch.mm(K_ext_inv,homogeneous_reconstructed_camera_points))\n",
    "        coords_reconstructed = homogeneous_reconstructed_coords[0:3:1, :]\n",
    "            # for original depthmap\n",
    "        v = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v = v[bool_matrix].view(1, -1).cuda()\n",
    "        u =  (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u = u[bool_matrix].view(1, -1).cuda()    \n",
    "        z = z[bool_matrix].view(1, -1)\n",
    "        homogeneous_camera_points = torch.cat((u, v, torch.ones(1, u.size()[1]).cuda(), 1.0/z))\n",
    "        homogeneous_coords = z*(torch.mm(K_ext_inv,homogeneous_camera_points))\n",
    "        coords = homogeneous_coords[0:3:1, :]\n",
    "        \n",
    "        print(\" x original : \", torch.squeeze(z)[10]*scaling_factor)\n",
    "        print(\" original coords : \", coords[:, 10])\n",
    "        \n",
    "        # compute MSE on points\n",
    "        total_mse += torch.sum(torch.sqrt(torch.sum((coords_reconstructed - coords)**2, 0)))\n",
    "    return total_mse\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.05, 0.05).cuda()   \n",
    "    sum_log_gsm = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        sum_log_gsm[i] = (-torch.log2(gsm)).sum()\n",
    "    \n",
    "    entropy = torch.trapz(sum_log_gsm, u)\n",
    "    if entropy < 0:\n",
    "        print(\"negative entropy\")\n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def clip(x):\n",
    "    x_n = (x - torch.min(x))/(torch.max(x) - torch.min(x))\n",
    "    x_clipped = torch.round(255*x_n).float()\n",
    "    return x_clipped\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MyQuantization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input*10)/10\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "        \n",
    "        \n",
    "class MyClipping(torch.autograd.Function):\n",
    "  \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input).clamp(min=0, max=2**16-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "    \n",
    "class MyConv2d_par(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d_par, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernel_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_channels, 1, self.n_channels, self.kernel_size_number))\n",
    "        #self.weight.data.uniform_(0, 1)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.out_channels, 1))\n",
    "        #self.bias.data.uniform_(0, 1)\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x, x_segmented):\n",
    "        if x.is_cuda:\n",
    "            self.weight = self.weight.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        result_seg = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        \n",
    "        windows_depth = self.calculateWindows(x)\n",
    "        windows_seg = self.calculateWindows(x_segmented)\n",
    "        windows_seg[windows_seg < 1] = -1\n",
    "        windows_seg_centers = windows_seg[0, :, :, windows_seg.size()[3]//2].view(1, windows_seg.size()[1], windows_seg.size()[2], 1)\n",
    "        windows_seg = windows_seg * windows_seg_centers\n",
    "        windows_seg[windows_seg < 1] = 0 \n",
    "        windows_depth_seg = windows_depth * windows_seg\n",
    "        \n",
    "        \n",
    "        # compute result\n",
    "        ponderation = torch.sum(windows_seg, (2,3))\n",
    "        result = torch.sum(windows_depth_seg * self.weight , (2, 3))/ponderation + self.bias\n",
    "        result = result.reshape(x.shape[0], self.out_channels, width, height)\n",
    "        \n",
    "        # compute result_seg\n",
    "        windows_seg_seg = self.calculateWindows(x_segmented) * windows_seg\n",
    "        result_seg = torch.sum(windows_seg_seg * self.weight , (2, 3)) + self.bias\n",
    "        result_seg = result_seg.reshape(x_segmented.shape[0], self.out_channels, width, height)\n",
    "        #result_seg = torch.clamp(result_seg, min=0, max=1)\n",
    "        thresh = (torch.max(result_seg) + torch.min(result_seg))/2.0\n",
    "        result_seg = (result_seg > thresh).double()\n",
    "        \n",
    "        return result, result_seg\n",
    "        \n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(1, -1, x.shape[1], self.kernel_size_number)\n",
    "        #windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1\n",
    "    \n",
    "\n",
    "class MyConv2d_inc(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d_inc, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernel_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernel_size_number))\n",
    "        #self.weight.data.uniform_(0, 1)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.out_channels))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x, x_segmented):\n",
    "        if x.is_cuda:\n",
    "            self.weight = self.weight.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        result_seg = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        \n",
    "        windows_depth = self.calculateWindows(x)\n",
    "        windows_seg = self.calculateWindows(x_segmented)\n",
    "        windows_seg[windows_seg < 1] = -1\n",
    "        windows_seg_centers = windows_seg[:, :, windows_seg.size()[2]//2].view(windows_seg.size()[0], windows_seg.size()[1], 1)\n",
    "        windows_seg = windows_seg * windows_seg_centers\n",
    "        windows_seg[windows_seg < 1] = 0 \n",
    "        windows_depth_seg = windows_depth * windows_seg\n",
    "        \n",
    "        \n",
    "        # compute result\n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows_depth_seg[channel], self.weight[i_convNumber][channel].view(-1, 1))\n",
    "                xx = xx.view(-1, width, height)/torch.sum(windows_seg[channel], 1).view(-1, width, height)\n",
    "                result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "            result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] /= x.shape[1]\n",
    "\n",
    "        result = result.view(x.shape[0], self.out_channels, width, height)\n",
    " \n",
    "        # compute result_seg\n",
    "        windows_seg_seg = self.calculateWindows(x_segmented) * windows_seg\n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows_seg_seg[channel], self.weight[i_convNumber][channel].view(-1, 1))\n",
    "                xx = xx.view(-1, width, height)\n",
    "                result_seg[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "\n",
    "        result_seg = result_seg.view(x_segmented.shape[0], self.out_channels, width, height)\n",
    "        result_seg = torch.clamp(result_seg, min=0, max=1)\n",
    "        \n",
    "        return result, result_seg\n",
    "        \n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernel_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(CustomConv2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernel_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernel_size_number))\n",
    "        self.weight.data.uniform_(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.is_cuda:\n",
    "            self.weight = self.weight.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        windows = self.calculateWindows(x)\n",
    "        \n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows[channel], self.weight[i_convNumber][channel])\n",
    "                xx = xx.view(-1, width, height)/self.kernel_size_number\n",
    "                result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "            result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] /= x.shape[1]\n",
    "\n",
    "        result = result.view(x.shape[0], self.out_channels, width, height)\n",
    "        return result\n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernel_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entrÃ©e : 128*128\n",
    "class LossyCompAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = MyConv2d_par(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = MyConv2d_par(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = MyConv2d_par(128, 32, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = MyConv2d_par(32, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = MyConv2d_par(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = MyConv2d_par(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = MyConv2d_par(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(32).view(1, 32, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchNormed = nn.BatchNorm2d(32, momentum = False, affine=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, x_seg, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # get zero mask\n",
    "            #dilatation_mask = (x>0)\n",
    "                # removing black pixels\n",
    "            #x = black_pixels_removal_by_dilatation(x)\n",
    "            #if torch.any(x.isnan()):\n",
    "                #print(\"x after dilatation is nan\", x)\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            #x, radius, mean_channels = normalize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            x_seg = mirror_padding(x_seg, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x, x_seg = self.conv1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.conv2(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x_c1 = x.clone()\n",
    "            x_seg_c1 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"mean channels : \", mean_channels)\n",
    "                print(\"standard deviation : \", torch.sqrt(var))\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x, x_seg = self.resConv1_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv1_2(x, x_seg)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c1, min=0, max=1)\n",
    "            x_seg_c2 = x_seg\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weight gradients: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x, x_seg = self.resConv2_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv2_2(x, x_seg)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c2, min=0, max=1)\n",
    "            x_seg_c3 = x_seg\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "            \n",
    "                # residual block 3\n",
    "            x, x_seg = self.resConv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv3_2(x, x_seg)\n",
    "            x += x_c3\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c3, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "            \n",
    "                # output block\n",
    "            x, x_seg = self.conv3(x, x_seg)\n",
    "            x_latent_before_quantization = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x, x_seg = self.subpix1(x, x_seg)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_seg = periodic_shuffling(x_seg, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            x_seg_c4 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                # residual block 1\n",
    "            x, x_seg = self.deconv1_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv1_2(x, x_seg)\n",
    "            x += x_c4\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c4, min=0, max=1)\n",
    "            x_c5 = x.clone()\n",
    "            x_seg_c5 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                   # residual block 2\n",
    "            x, x_seg = self.deconv2_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv2_2(x, x_seg)\n",
    "            x += x_c5\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c5, min=0, max=1)\n",
    "            x_c6 = x.clone()\n",
    "            x_seg_c6 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "            \n",
    "                   # residual block 3\n",
    "            x, x_seg = self.deconv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv3_2(x, x_seg)\n",
    "            x += x_c6\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c6, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "            \n",
    "                    # subpixel 2\n",
    "            x, x_seg = self.subpix2(x, x_seg)\n",
    "            x = periodic_shuffling(x, 256//4)\n",
    "            x_seg = periodic_shuffling(x_seg, 256//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                    # subpixel 3\n",
    "            x, x_seg = self.subpix3(x, x_seg)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            #x = denormalize_output(x, radius, mean_channels)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "                    # replace black pixels\n",
    "           # x = x*dilatation_mask\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized, x_latent_before_quantization\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                x_seg = mirror_padding(x_seg, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x, x_seg))\n",
    "                x_seg = F.relu(self.conv1(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x = F.relu(self.conv2(x, x_seg))\n",
    "                x_seg = F.relu(self.conv2(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x, x_seg))\n",
    "                x = self.resConv1_2(x, x_seg)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x, x_seg))\n",
    "                x = self.resConv2_2(x, x_seg)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x, x_seg))\n",
    "                x = self.resConv3_2(x, x_seg)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x, x_seg)\n",
    "                x_seg = F.relu(self.conv3(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "            \n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            # batch normalization\n",
    "            x = self.batchNormed(x)\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x, x_seg)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_seg = self.subpix1(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                    print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x, x_seg))\n",
    "                x = self.deconv1_2(x, x_seg)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x, x_seg))\n",
    "                x = self.deconv2_2(x, x_seg)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x, x_seg))\n",
    "                x = self.deconv3_2(x, x_seg)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x, x_seg)\n",
    "                x = periodic_shuffling(x, 256//4)\n",
    "                x_seg = self.subpix2(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 256//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                    print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                    print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x, x_seg)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                x_seg = self.subpix3(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                    print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                    print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entrÃ©e : 128*128\n",
    "class LossyCompAutoencoder_bis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder_bis, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = nn.Conv2d(128, 32, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = nn.Conv2d(32, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = nn.Conv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = nn.Conv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(32).view(1, 32, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.resConv1_1(x))\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x = F.relu(self.resConv2_1(x))\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "                # residual block 3\n",
    "            x = F.relu(self.resConv3_1(x))\n",
    "            x = self.resConv3_2(x)\n",
    "            x += x_c3\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.deconv1_1(x))\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                   # residual block 2\n",
    "            x = F.relu(self.deconv2_1(x))\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                   # residual block 3\n",
    "            x = F.relu(self.deconv3_1(x))\n",
    "            x = self.deconv3_2(x)\n",
    "            x += x_c6\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = F.relu(periodic_shuffling(x, 256//4))\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "\n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = F.relu(periodic_shuffling(x, 256//4))\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entrÃ©e : 128*128\n",
    "class LossyCompAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = CustomConv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = CustomConv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = CustomConv2d(128, 16, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = CustomConv2d(16, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = CustomConv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = MyConv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = CustomConv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = CustomConv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 16))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 16))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(16).view(1, 16, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchNormed = nn.BatchNorm2d(16, momentum = False, affine=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # get zero mask\n",
    "            #dilatation_mask = (x>0)\n",
    "                # removing black pixels\n",
    "            #x = black_pixels_removal_by_dilatation(x)\n",
    "            #if torch.any(x.isnan()):\n",
    "                #print(\"x after dilatation is nan\", x)\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            #x, radius, mean_channels = normalize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after normalization is inf\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after mirror padding is inf\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"mean channels : \", mean_channels)\n",
    "                print(\"standard deviation : \", torch.sqrt(var))\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after input block is inf\", x)\n",
    "                # residual block 1\n",
    "            x = self.resConv1_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weight gradients: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 1 is inf\", x)\n",
    "                # residual block 2\n",
    "            x = self.resConv2_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 2 is inf\", x)\n",
    "            \"\"\"\n",
    "                # residual block 3\n",
    "            x, x_seg = self.resConv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv3_2(x, x_seg)\n",
    "            x += x_c3\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c3, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "            \"\"\"\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after output block is inf\", x)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after quantization block is inf\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after mask is inf\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after subpix 1 is inf\", x)\n",
    "                # residual block 1\n",
    "            x = self.deconv1_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 1 is inf\", x)\n",
    "                   # residual block 2\n",
    "            x = self.deconv2_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 2 is inf\", x)\n",
    "            \"\"\"\n",
    "                   # residual block 3\n",
    "            x, x_seg = self.deconv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv3_2(x, x_seg)\n",
    "            x += x_c6\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c6, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "            \"\"\"\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = periodic_shuffling(x, 256//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after subpix 2 is inf\", x)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after subpix 3 is inf\", x)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            #x = denormalize_output(x, radius, mean_channels)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after denormalization is inf\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after clipping is inf\", x)\n",
    "                    # replace black pixels\n",
    "           # x = x*dilatation_mask\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "            \n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            # batch normalization\n",
    "            x = self.batchNormed(x)\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                    print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = periodic_shuffling(x, 256//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                    print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                    print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                    print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                    print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossyCompAutoencoder(\n",
      "  (conv1): MyConv2d_par()\n",
      "  (conv2): MyConv2d_par()\n",
      "  (resConv1_1): MyConv2d_par()\n",
      "  (resConv1_2): MyConv2d_par()\n",
      "  (resConv2_1): MyConv2d_par()\n",
      "  (resConv2_2): MyConv2d_par()\n",
      "  (resConv3_1): MyConv2d_par()\n",
      "  (resConv3_2): MyConv2d_par()\n",
      "  (conv3): MyConv2d_par()\n",
      "  (subpix1): MyConv2d_par()\n",
      "  (deconv1_1): MyConv2d_par()\n",
      "  (deconv1_2): MyConv2d_par()\n",
      "  (deconv2_1): MyConv2d_par()\n",
      "  (deconv2_2): MyConv2d_par()\n",
      "  (deconv3_1): MyConv2d_par()\n",
      "  (deconv3_2): MyConv2d_par()\n",
      "  (subpix2): MyConv2d_par()\n",
      "  (subpix3): MyConv2d_par()\n",
      "  (batchNormed): BatchNorm2d(32, eps=1e-05, momentum=False, affine=False, track_running_stats=True)\n",
      ")\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# instanciate the model\n",
    "model = LossyCompAutoencoder()\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.25 GiB (GPU 0; 5.00 GiB total capacity; 364.78 MiB already allocated; 1.71 GiB free; 2.12 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c933832f15ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mbatch_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mbatch_segs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mdecoded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_quantized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_latent_before_quantization\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_segs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m#[decoded_images, x_quantized] = model(batch_images, 1, True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bbf667753a13>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_seg, mask, return_xq, is_lambda)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[1;31m# subpixel 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubpix2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperiodic_shuffling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperiodic_shuffling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_seg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-4c3029f671a2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_segmented)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# compute result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[0mponderation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindows_seg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindows_depth_seg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mponderation\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.25 GiB (GPU 0; 5.00 GiB total capacity; 364.78 MiB already allocated; 1.71 GiB free; 2.12 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# transfert du model au gpu\n",
    "model.to(device)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "        \n",
    "      \n",
    "\n",
    "# define beta\n",
    "beta = 0.00001\n",
    "#beta = 1000\n",
    "\n",
    "# incremental update of coefficients  \n",
    "# define threshold and loss_init\n",
    "threshold = 0.95\n",
    "loss_init = float(\"Inf\")\n",
    "nb_ones = 1\n",
    "iteration = 0\n",
    "mask=(compute_mask(1, (32, 16, 16)).unsqueeze(0)).cuda()\n",
    "dim_latent = 16*16*32\n",
    "output_flag = False\n",
    "\n",
    "mask=1\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch : \", epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    if epoch==100:\n",
    "        # define a new learning rate and so a new optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \"\"\"\n",
    "        \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        #print(\"ibatch : \", i_batch)\n",
    "        batch_images = data[0].to(device).float()\n",
    "        batch_masks = data[1].to(device).float()\n",
    "        batch_segs = data[2].to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = model(batch_images, batch_segs.detach(), mask, True)\n",
    "        #[decoded_images, x_quantized] = model(batch_images, 1, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        #loss_dist = distortion_pc(batch_images, decoded_images, 525.0, 0, 1000.0, (128, 128), (319.5, 239.5))\n",
    "        loss_bit = entropy_dist(x_quantized, model.phi, model.var)\n",
    "        #print(\" loss distortion : \", loss_dist)\n",
    "        #print(\"loss bit : \", loss_bit)\n",
    "        #loss = loss_dist + beta*loss_bit\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(\"loss : \", loss)\n",
    "        loss.backward()\n",
    "        #print(\"input weights conv 1 gradient: \", model.conv1.weight.grad)\n",
    "        #print(\"input bias conv 1 gradient: \",  model.conv1.bias.grad)\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        # check the value of the loss to see if another coefficient can be enabled\n",
    "        if (nb_ones<dim_latent):\n",
    "            if (loss.item() < loss_init*threshold or iteration > 0):\n",
    "                nb_ones +=1\n",
    "                loss_init = loss.item()\n",
    "                iteration = 0\n",
    "                mask = (compute_mask(nb_ones, tuple(x_quantized.size()[1:])).unsqueeze(0)).cuda()\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        iteration += 1\n",
    "        x_latent_before_quantization_copy = torch.clone(x_latent_before_quantization).detach().view(1, -1).cpu().numpy()\n",
    "        np.savetxt(\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\latent\\\\\" + str(i_batch)+\"_latent.txt\", x_latent_before_quantization_copy)\n",
    "        x_quantized_copy = torch.clone(x_quantized).detach().view(1, -1).cpu().numpy()\n",
    "        np.savetxt(\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\quantized\\\\\" + str(i_batch)+\"_quantized.txt\", x_quantized_copy) \n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.data)\n",
    "\n",
    "\n",
    "print(\"input weights conv 1 gradient: \", model.conv1.weight.grad)\n",
    "print(\"conv1.weights grad: \", params[0].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# transfert du model au gpu\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_segmented_Incremental.pth'))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "        \n",
    "      \n",
    "\n",
    "# define beta\n",
    "beta = 0.00001\n",
    "#beta = 1000\n",
    "\n",
    "# incremental update of coefficients  \n",
    "# define threshold and loss_init\n",
    "threshold = 0.95\n",
    "loss_init = float(\"Inf\")\n",
    "nb_ones = 1\n",
    "iteration = 0\n",
    "mask=(compute_mask(1, (32, 16, 16)).unsqueeze(0)).cuda()\n",
    "dim_latent = 16*16*32\n",
    "output_flag = False\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 2\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch : \", epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    if epoch==100:\n",
    "        # define a new learning rate and so a new optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \"\"\"\n",
    "        \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data[0].to(device).float()\n",
    "        batch_masks = data[1].to(device).float()\n",
    "        batch_segs = data[2].to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = model(batch_images, batch_segs.detach(), mask, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        loss_bit = entropy_dist(x_quantized, model.phi, model.var)\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        loss.backward()\n",
    "        \n",
    "        # check the value of the loss to see if another coefficient can be enabled\n",
    "        if (nb_ones<dim_latent):\n",
    "            if (loss.item() < loss_init*threshold or iteration > 0):\n",
    "                nb_ones +=1\n",
    "                loss_init = loss.item()\n",
    "                iteration = 0\n",
    "                mask = (compute_mask(nb_ones, tuple(x_quantized.size()[1:])).unsqueeze(0)).cuda()\n",
    "    \n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        iteration += 1\n",
    "        x_quantized_copy = torch.clone(x_quantized).detach().view(1, -1).cpu().numpy()\n",
    "        np.savetxt(\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\latent\\\\\" + str(i_batch)+\"_latent.txt\", x_quantized_copy) \n",
    "        #torch.save(x_quantized_copy,\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\latent\\\\\" + str(i_batch)+\"_latent.txt\")\n",
    "        #decoded_images_copy = torch.clone(decoded_images).detach()\n",
    "        #cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\reconstructed\\\\\" + \"img\" + str(i_batch)+\".png\", np.squeeze(decoded_images_copy.cpu()).numpy().astype(np.uint16))\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save(model.state_dict(), './model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_segmented_Incremental.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "    \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "# transfert du model au gpu\n",
    "model_bis.to(device)\n",
    "\n",
    "# general update of coefficients    \n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(model_bis.parameters(), lr=0.00001)\n",
    "    # define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "    #Epochs\n",
    "n_epochs = 420\n",
    "beta = 0.00001\n",
    "nb_updates = 0\n",
    "learning_rate = 0.0001\n",
    "tau = 10000\n",
    "kappa = 0.8\n",
    "\n",
    "    # Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "          \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        learning_rate *= tau**kappa/(tau + nb_updates)**kappa\n",
    "        print(\"learning_ rate : \", learning_rate)\n",
    "        optimizer = torch.optim.Adam(model_bis.parameters(), lr=learning_rate)\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = model_bis(batch_images, 1, True, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        loss_bit = mean_bit_per_px(x_quantized, model_bis.phi, model_bis.var)\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(loss)\n",
    "            \n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        nb_updates += 1\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "\n",
    "model_bis = LossyCompAutoencoder_bis()\n",
    "model_bis.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model_bis.train()\n",
    "model_bis.to(device)\n",
    "\n",
    "\n",
    "weights_model_bis = list(model_bis.parameters())\n",
    "print(\"nb parameters of the model : \", len(weights_model_bis))\n",
    "for (name_bis, parameter_bis), (name, parameter) in zip(model_bis.named_parameters(), model.named_parameters()) :\n",
    "    \"\"\"\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter_bis)\n",
    "        parameter_bis.data = torch.FloatTensor(1, 96, 1, 1).uniform_(0.9, 1.1)\n",
    "        print(\"lamb new parameter : \", parameter_bis)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name_bis)\n",
    "    print(\"size of user-defined parameters : \", parameter_bis.size())\n",
    "    parameter.data = parameter_bis.data\n",
    "\n",
    "for name_bis, parameter_bis in model_bis.named_parameters():\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"check lamb parameter : \", parameter_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model = list(model.parameters())\n",
    "for i in range(0, 39, 1):\n",
    "    weights_model[i] = weights_model_bis[i]\n",
    "    print(\"size of user-defined parameters : \", weights_model_bis[i].size())\n",
    "    print(\"weights model: \" ,weights_model_bis[i])\n",
    "model.train()    \n",
    "#weights_model[39] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "#weights_model[40] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    \"\"\"\n",
    "    if name == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter)\n",
    "        parameter.data = torch.FloatTensor(1, 96, 1, 1).uniform_(1, 1)\n",
    "        print(\"lamb new parameter : \", parameter)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name)\n",
    "    print(\"size of user-defined parameters : \", parameter.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_segmented_Incremental.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "#test_dataset = ImageDataset(root_dir='./data/kodac/', transform=transforms.Compose([RandomCrop(128), ToTensor()]))\n",
    "#test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "test_dataset = ImageDataset(depthmap_dir='D:/autoencoder_data/depthmaps/test/dilated', \n",
    "                                mask_dir='D:/autoencoder_data/depthmaps/test/mask',\n",
    "                                segmentation_dir='D:/autoencoder_data/depthmaps/test/segmentation_roipoly',\n",
    "                                transform=transforms.Compose([RandomCrop((128, 128)), ToTensor()])\n",
    "                                 )\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():            \n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i][0].unsqueeze(0).to(device).float()\n",
    "        test_mask = test_dataset[i][1].unsqueeze(0).to(device).float()\n",
    "        test_seg = test_dataset[i][2].unsqueeze(0).to(device).float()\n",
    "        #test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "               \n",
    "        if test_image.size(2)>test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "            test_seg = test_seg.permute(0, 1, 3, 2)\n",
    "\n",
    "        #[reconstructed_image, vec_latent] = model(test_image, 1, True)\n",
    "        [reconstructed_image, vec_latent, vec_latent_before_quantization] =  model(test_image, test_seg, 1, True)\n",
    "        print(\"min vec latent : \", torch.min(vec_latent))\n",
    "        print(\"max vec latent : \", torch.max(vec_latent))\n",
    "\n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(torch.squeeze(reconstructed_image.int().cpu()))\n",
    "\n",
    "        \"\"\"\n",
    "        # We can set the number of bins with the `bins` kwarg\n",
    "        bins_list = [-6.5, -5.5, -4.5, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n",
    "        for k in range(96):\n",
    "            # plot histograms\n",
    "            fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "            ax.hist((vec_latent[:, k, :, :]).view(-1).cpu(), bins=bins_list)\n",
    "            plt.savefig(\"D:\\\\autoencoder_data\\\\histograms\\\\depthmap\\\\\" + \"img\" + str(i)+ \"hist\" + str(k) + \".png\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = model(test_image)\n",
    "        reconstructed_depthmap = np.squeeze(reconstructed_image.cpu()).numpy()\n",
    "        cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\reconstructed\\\\beta_1\\\\\" + \"img\" + str(i)+\".png\", reconstructed_depthmap.astype(np.uint16))\n",
    "        #save_image(reconstructed_depthmap, \"D:\\\\autoencoder_data\\\\depthmaps\\\\reconstructed\\\\beta_0001\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PSNR for each image of the test set and its reconstruction\n",
    "\n",
    "def write_data(filepath , tensor_data):\n",
    "    batch, channel, h, w = tensor_data.size()\n",
    "    matrix = tensor_data.cpu().numpy()\n",
    "    file = open(filepath, \"w\")\n",
    "    for image in range(batch):\n",
    "        np.savetxt(file, matrix[image, :, :, :].reshape(channel*h, w), fmt ='%.0f')\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "psnr_sum = 0.0\n",
    "bit_rate_sum = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model(test_image, 1, True)\n",
    "        #write_data('.\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2\\\\latent_vect\\\\' + 'vec' + str(i) +'.txt', im_quantized)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(2)*test_image.size(3))\n",
    "        psnr_sum+= psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0)\n",
    "        bit_rate_sum += nbpp\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0))\n",
    "print( \"mean nb bits per pixel : \", bit_rate_sum/len(test_dataset))\n",
    "print(\"psnr mean : \", psnr_sum/len(test_dataset) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "       \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "# Load previous model\n",
    "model_prev = LossyCompAutoencoder()\n",
    "model_prev.load_state_dict(torch.load('./model_parameters/lossy_comp_params_with_rate_beta2_incremental_2.pth'))\n",
    "model_prev.eval()\n",
    "model_prev.to(device)\n",
    "\n",
    "\n",
    "# And run test \n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model_prev(test_image,1, True)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(1)*test_image.size(2)*test_image.size(3))\n",
    "        print(\"nb_symbols : \", nb_symbols)\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 255.0))\n",
    "    \n",
    "# And print figures\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        if test_image.size(2)<test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "        \n",
    "        reconstructed_image = model_prev(test_image, 1,  False)\n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.squeeze(reconstructed_image.int().cpu()).permute(1, 2, 0))\n",
    "        \n",
    "# And save figures\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = np.squeeze(model_prev(test_image).cpu())\n",
    "        print(reconstructed_image.type())\n",
    "        save_image(reconstructed_image, \".\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2_incremental_bis\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.01, 0.01).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    entropy = torch.trapz(gsm_sum, u)\n",
    "    \n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "# Load incremental model\n",
    "incremental_model = LossyCompAutoencoder()\n",
    "incremental_model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta1_without_bitrate.pth'))\n",
    "incremental_model.train()\n",
    "incremental_model.to(device)\n",
    "\n",
    "# train again the model starting form incremental-learned weights\n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(incremental_model.parameters(), lr=0.00001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "\n",
    "\n",
    "# define beta\n",
    "beta = 1.0\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 800\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "           \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = incremental_model(batch_images, 1, True, False)\n",
    "        optimizer.zero_grad()\n",
    "        #entropy = entropy_rate(x_quantized, incremental_model.phi, incremental_model.var)\n",
    "        #print(\"entropy : \", entropy)\n",
    "        dist = distortion(decoded_images, batch_images)\n",
    "        #print(\"distortion : \", dist)\n",
    "        #loss = beta * dist + entropy\n",
    "        loss = beta*dist\n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from range_coder import RangeEncoder, RangeDecoder, prob_to_cum_freq\n",
    "import os\n",
    "\n",
    "# Load previous model\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_params_with_rate_beta0005_incremental.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "nb_bits = 0.0\n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():  \n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "            # compute symbol probabilities\n",
    "        min_val = data_comp.min()\n",
    "        if min_val <0:\n",
    "            data_comp -= min_val\n",
    "            min_val = 0\n",
    "        max_val = data_comp.max()\n",
    "        nb_bins = max_val - min_val + 1\n",
    "        hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "        prob = hist/hist.sum()\n",
    "        #print(\"data comp : \", data_comp)\n",
    "        #print(prob)\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(torch.nonzero(prob)) != len(prob):\n",
    "            zero_indices = ((prob == 0).nonzero())\n",
    "            for j in reversed(range(0, len(zero_indices), 1)):\n",
    "                data_comp[data_comp > int(zero_indices[j])+min_val] -=1\n",
    "            min_val = data_comp.min()\n",
    "            max_val = data_comp.max()\n",
    "            nb_bins = max_val - min_val + 1\n",
    "            hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "            prob = hist/hist.sum()\n",
    "            print(min_val)\n",
    "            print(max_val)\n",
    "            print(\"data comp : \", data_comp)\n",
    "            print(prob)\n",
    "         \"\"\" \n",
    "            \n",
    "            # convert probabilities to cumulative integer frequency table\n",
    "        #cumFreq = prob_to_cum_freq(torch.clamp(prob, min=np.finfo(np.float32).eps).cpu(), resolution=128)\n",
    "        cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "        #print(cumFreq)\n",
    "        \n",
    "        # encode data\n",
    "        filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(i) + \".bin\"\n",
    "        encoder = RangeEncoder(filepath_to_write)\n",
    "        #print(torch.flatten(data_comp).cpu().tolist())\n",
    "        encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "        encoder.close()\n",
    "        \n",
    "        \n",
    "        file_size = os.path.getsize(filepath_to_write)*8 #number of bits in the file\n",
    "        print(file_size)\n",
    "        nb_bits += file_size\n",
    "        \n",
    "    nb_bits_per_image = nb_bits/len(test_dataset)\n",
    "    print(nb_bits_per_image)\n",
    "    nb_bits_per_pixel = nb_bits_per_image/(512*768)\n",
    "    print(nb_bits_per_pixel)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "    test_image = test_dataset[1].unsqueeze(0).to(device).float()\n",
    "    [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "        # compute symbol probabilities\n",
    "    min_val = data_comp.min()\n",
    "    print(min_val)\n",
    "    print(data_comp.max())\n",
    "    if min_val <0:\n",
    "        data_comp -= min_val\n",
    "        min_val = 0\n",
    "    max_val = data_comp.max()\n",
    "    \n",
    "    print(max_val)\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    print(nb_bins)\n",
    "    hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    prob = hist/hist.sum()\n",
    "    print(prob)\n",
    "         # convert probabilities to cumulative integer frequency table\n",
    "    cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "    #print(cumFreq)\n",
    "\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    \n",
    "    \n",
    "        # encode data\n",
    "    filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(1) + \".bin\"\n",
    "    encoder = RangeEncoder(filepath_to_write)\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "    encoder.close()   \n",
    "\"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "#fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_image = test_dataset[i].unsqueeze(0).float().cuda()\n",
    "\n",
    "    #test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "    test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "    cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\dilated\\\\\" + \"img\" + str(i)+\".png\", np.squeeze(test_image_without_black_px.cpu().numpy()).astype(np.uint16))\n",
    "    #ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "    #plt.imshow(torch.squeeze(test_image_without_black_px).cpu())\n",
    "\n",
    "\"\"\"    \n",
    "i = 0\n",
    "test_image = test_dataset[i].unsqueeze(0).float()\n",
    "\n",
    "#test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "plt.imshow(torch.squeeze(test_image_without_black_px))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
