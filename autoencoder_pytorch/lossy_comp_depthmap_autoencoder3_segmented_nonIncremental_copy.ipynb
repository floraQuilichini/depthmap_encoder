{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import pandas as pd\n",
    "from skimage import io, transform\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy import signal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "torch.set_printoptions(precision=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([7, 1, 64, 64])\n",
      "1 torch.Size([7, 1, 64, 64])\n",
      "2 torch.Size([7, 1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntransform = transforms.Compose([\\n    transforms.Pad(12, padding_mode='reflect'),\\n    transforms.ToTensor()])\\n\\ntrainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, depthmap_dir, mask_dir, segmentation_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depthmap_dir (string): Directory with all the depthmaps.\n",
    "            mask_dir (string): Directory with all the masks.\n",
    "            segmentation_dir (string): Directory with all the segmentation of the depthmaps.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.depthmap_dir = depthmap_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        path, dirs, files = next(os.walk(self.depthmap_dir))\n",
    "        file_count = len(files)\n",
    "        return file_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path_d, dirs_d, files_d = next(os.walk(self.depthmap_dir))\n",
    "        img_d_name = os.path.join(self.depthmap_dir, files_d[idx])\n",
    "        image_d = io.imread(img_d_name)\n",
    "        \n",
    "        path_m, dirs_m, files_m = next(os.walk(self.mask_dir))\n",
    "        img_m_name = os.path.join(self.mask_dir, files_m[idx])\n",
    "        image_m = io.imread(img_m_name)\n",
    "        \n",
    "        path_s, dirs_s, files_s = next(os.walk(self.segmentation_dir))\n",
    "        img_s_name = os.path.join(self.segmentation_dir, files_s[idx])\n",
    "        image_s = io.imread(img_s_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image_d = self.transform(image_d)\n",
    "            image_m = self.transform(image_m)\n",
    "            image_s = self.transform(image_s)\n",
    "\n",
    "        return image_d, image_m, image_s\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        if h!= new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top = 0\n",
    "        if w!= new_w:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0\n",
    "     \n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        \n",
    "        image = np.expand_dims(image, axis=2)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class Downsample(object):\n",
    "    \"\"\"Downsample the image\n",
    "\n",
    "    Args:\n",
    "        downsampling_factor (int or tuple): Desired downsampling factor for rows and columns.\n",
    "        If the downsampling factor is an int, then both rows and columns are sampled by the same factor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampling_factor):\n",
    "        assert isinstance(downsampling_factor, (int, tuple))\n",
    "        if isinstance(downsampling_factor, int):\n",
    "            self.downsampling_factor = (downsampling_factor, downsampling_factor)\n",
    "        else:\n",
    "            assert len(downsampling_factor) == 2\n",
    "            self.downsampling_factor = downsampling_factor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        down_fact_h, down_fact_w = self.downsampling_factor\n",
    "        image = image[::down_fact_h,\n",
    "                      ::down_fact_w]\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class ConvertDepthToColor(object):\n",
    "    \"\"\" convert a 1xmxn 16-bits depthmap to a 2xmxn 8-bits colormap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if len(image.shape[:]) <3:\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            \n",
    "        h, w = image.shape[:2]\n",
    "        image_r_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color[(image > 2**8 - 1)] = image[(image > 2**8 - 1)] >> 8\n",
    "        image_r_color = image - (image_g_color <<8)\n",
    "\n",
    "        return np.concatenate((image_r_color, image_g_color), axis=2)\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in images to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, images):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        images = images.transpose((2, 0, 1))\n",
    "        images = images.astype(float)\n",
    "        return torch.from_numpy(images)\n",
    "    \n",
    "\n",
    "    \n",
    "class ConvertColorToDepth(object):\n",
    "    \"\"\" convert a 2xmxn 8-bits colormap to a 1xmxn 16-bits depthmap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, color_image):\n",
    "            \n",
    "        depth_image = color_image[:, 0, :, :]\n",
    "        depth_image += color_image[:, 1, :, :] << 8\n",
    "\n",
    "        return depth_image\n",
    "\n",
    "    \n",
    "def show_image_batch(images_batch):\n",
    "    \"\"\"Show image for a batch of samples.\"\"\"\n",
    "    if images_batch.size(1) == 1:\n",
    "        images_batch_normed = images_batch/torch.max(images_batch)\n",
    "        grid = utils.make_grid(images_batch_normed)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    else:\n",
    "        print(images_batch.size())\n",
    "        images_b_batch = torch.zeros(images_batch.size(0), 1, images_batch.size(2) , images_batch.size(3))\n",
    "        images_color_batch = torch.cat((images_batch, images_b_batch), 1) \n",
    "        print(images_color_batch.size())\n",
    "        grid = utils.make_grid(images_color_batch)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Load training dataset\n",
    "\n",
    "#mirror_padding_transform = transforms.Compose([transforms.ToPILImage(), transforms.Pad(padding=12, padding_mode='reflect'), transforms.ToTensor()])\n",
    "    # the  datasets contain depthmaps (first composant), masks (second composant) and segmentations (third composant)\n",
    "transformed_datasets = ImageDataset(depthmap_dir='C:/Users/Flora/autoencoder/training/dilated', \n",
    "                                mask_dir='C:/Users/Flora/autoencoder/training/mask',\n",
    "                                segmentation_dir='C:/Users/Flora/autoencoder/training/segmentation_roipoly',\n",
    "                                transform=transforms.Compose([RandomCrop((384, 640)), Downsample(( 3*2, 5*2)), ToTensor()])\n",
    "                                 )\n",
    "dataloader = DataLoader(transformed_datasets, batch_size=7, shuffle=True, num_workers=0)\n",
    "\n",
    "for i_batch, batch_images in enumerate(dataloader):\n",
    "    print(i_batch, batch_images[2].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 2:\n",
    "        plt.figure()\n",
    "        show_image_batch(batch_images[2])\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(12, padding_mode='reflect'),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define additionnnal functions\n",
    "def periodic_shuffling(T, C):\n",
    "    T_copy = T.clone()\n",
    "    batch_size = T.size()[0]\n",
    "    H = T.size()[2]\n",
    "    W = T.size()[3]\n",
    "    T = T.view(batch_size, C, H*2, W*2)\n",
    "    \"\"\"\n",
    "    for k in range(C):\n",
    "        for i in range(2*H):\n",
    "            for j in range(2*W):\n",
    "                T[:, k, i, j] = T_copy[:, C*((j&1)<<1)+C*(i&1)+k, i>>1, j>>1]\n",
    "    \"\"\"\n",
    "                \n",
    "    T[:, :, ::2, ::2] = T_copy[:, 0:C, :, :]\n",
    "    T[:, :, 1::2, ::2] = T_copy[:, C:2*C, :, :]\n",
    "    T[:, :, ::2, 1::2] = T_copy[:, 2*C:3*C, :, :]\n",
    "    T[:, :, 1::2, 1::2] = T_copy[:, 3*C:4*C, :, :]\n",
    "\n",
    "    return T\n",
    "    \n",
    "    \n",
    "def mirror_padding(x, padding_size):\n",
    "    up_line = x[:, :, 0:padding_size, :].flip(2)\n",
    "    left_col = x[:, :, :, 0:padding_size].flip(3)\n",
    "    right_col = x[:, :, :, -padding_size:].flip(3)\n",
    "    bottom_line = x[:, :, -padding_size:, :].flip(2)\n",
    "    left_up_corner = left_col[:, :, 0:padding_size, :].flip(2)\n",
    "    right_up_corner = right_col[:, :, 0:padding_size, :].flip(2)\n",
    "    left_bottom_corner = left_col[:, :, -padding_size:, :].flip(2)\n",
    "    right_bottom_corner = right_col[:, :, -padding_size:, :].flip(2)\n",
    "\n",
    "    x_mirror_pad = torch.cat((torch.cat((left_up_corner, up_line, right_up_corner), 3), torch.cat((left_col, x, right_col), 3), torch.cat((left_bottom_corner, bottom_line, right_bottom_corner), 3)), 2)\n",
    "    return x_mirror_pad\n",
    "\n",
    "\n",
    "\n",
    "def normalize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    max_value = torch.max(x)\n",
    "    min_value = torch.min(x)\n",
    "    radius = max(max_value, abs(min_value))\n",
    "    x_centered_normalized = x_centered/radius\n",
    "    return x_centered_normalized, radius, mean_channels\n",
    "\n",
    "def standardize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    var = torch.sum(x_centered**2, (2, 3))/(x.size()[2]*x.size()[3])\n",
    "    x_standardized = x_centered / torch.sqrt(var.view(x_centered.size()[0], x_centered.size()[1], 1, 1))\n",
    "    return x_standardized, mean_channels, var\n",
    "    \n",
    "def denormalize_output(x, radius, mean_channels):\n",
    "    x_denormalized = x*radius\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_denormalized_centered = x_denormalized + mean_channels_images\n",
    "    return x_denormalized_centered\n",
    "\n",
    "def destandardize_output(x, mean_channels, var):\n",
    "    x_rescaled = x*torch.sqrt(var.view(x.size()[0], x.size()[1], 1, 1))\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_destandardized = x_rescaled + mean_channels_images\n",
    "    return x_destandardized\n",
    "\n",
    "\n",
    "def upsample(x, factor):\n",
    "    result = torch.ones(x.size()[0], x.size()[1], x.size()[2]*factor, x.size()[3]*factor)\n",
    "    if torch.cuda.is_available():\n",
    "        result = result.cuda()\n",
    "    for i in range(factor):\n",
    "        result[:, :, i::factor, i::factor]= x\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def compute_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    return gsm\n",
    "\n",
    "\n",
    "def sum_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    #gsm_sum = (torch.log2(gsm)).sum()\n",
    "    gsm_sum = gsm.sum()\n",
    "    return gsm_sum\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    h = x.size(-1)\n",
    "    w = x.size(-2)\n",
    "    #print(\"input : \", input)\n",
    "    coeff = torch.sqrt(1.0 / (2 * np.pi * var))\n",
    "    #print(\"coeff : \", coeff)\n",
    "    x_resized = x.repeat((nScale, 1, 1, 1, 1))\n",
    "    #print(\"input : \", input_resized)\n",
    "    exponent = (-0.5*(x_resized ** 2)/var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w))\n",
    "    #print(\"exponent : \", exponent)\n",
    "    coeffs_resized = coeff.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    #print(\"coeffs : \", coeffs_resized)\n",
    "    gaussian = coeffs_resized * torch.exp(exponent)\n",
    "    #print(\"gaussian : \", gaussian)\n",
    "    phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    phi_gaussian = phi_resized*gaussian\n",
    "    sum_phi_gaussian = phi_gaussian.sum(dim=0)\n",
    "    #print(\"sum over scales : \", sum_phi_gaussian)\n",
    "    result = -torch.log2(sum_phi_gaussian).sum()\n",
    "\n",
    "    return result\n",
    "    \"\"\"\n",
    "    \n",
    "def compute_mask(nb_ones, dims):\n",
    "    mask = torch.zeros(dims)\n",
    "    indices = np.arange(nb_ones)\n",
    "    mask_flatten = mask.view(-1, 1, 1, 1)\n",
    "    mask_flatten[indices] = 1\n",
    "    mask_reshaped = mask_flatten.view(dims)\n",
    "    return mask_reshaped\n",
    "\n",
    "\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    integral_u = torch.trapz(gsm_sum, u)\n",
    "    #print(\"gsm sum : \", gsm_sum)\n",
    "    #print(\"integral over u : \", integral_u)\n",
    "    entropy = -torch.log2(integral_u)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def mean_bit_per_px(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = []\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    nb_bits = (-torch.log2(torch.clamp(integral_u, min=np.exp(-10**2), max=1))).sum()\n",
    "    if nb_bits.isnan():\n",
    "        print(\"nb bits is nan\")\n",
    "    if nb_bits < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"nb_bits negative : \", nb_bits)\n",
    "    return nb_bits/reduce(lambda x, y: x*y, list(x_quantized.size()))\n",
    "\n",
    "\"\"\"\n",
    "def entropy_dist(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = [] \n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    \n",
    "    prob = torch.clamp(integral_u, min=np.exp(-10**2), max=1)\n",
    "    entropy = (-prob*torch.log2(prob)).sum()\n",
    "    if entropy.isnan():\n",
    "        print(\"entropy is nan\")\n",
    "    if entropy < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"entropy negative : \", entropy)\n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "def entropy_dist(x_quantized, phi, var):\n",
    "    entropy_tot = 0.0\n",
    "    for i in range(x_quantized.size()[0]):\n",
    "        for j in range(x_quantized.size()[1]):\n",
    "            x_q = torch.squeeze(x_quantized[i, j, :, :])\n",
    "            min_val = torch.min(x_q)\n",
    "            if min_val <0:\n",
    "                x_q -= min_val\n",
    "                min_val = torch.zeros(1)\n",
    "            max_val = torch.max(x_q)\n",
    "            print(\"min val : \", min_val.item())\n",
    "            print(\"max val : \", max_val.item())\n",
    "            nb_bins = max_val.item() - min_val.item() + 1\n",
    "            hist = torch.histc(x_q, bins=int(nb_bins), min=min_val.item(), max=max_val.item())\n",
    "            probs = torch.ones(len(hist))\n",
    "            for k in range(len(hist)):\n",
    "                if hist[k] > 0:\n",
    "                    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "                    gsm_stacked = []\n",
    "                    for l in range(len(u)):\n",
    "                        x = min_val.item() + k + u[l]\n",
    "                        gsm = compute_gsm(x, var[:, j].view(-1, 1), phi[:, j].view(-1, 1), 6)\n",
    "                        gsm_stacked.append(gsm)\n",
    "                    gsms = torch.squeeze(torch.stack(gsm_stacked, dim=0))\n",
    "                    integral = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "                    print(\"integral : \", integral)\n",
    "                    probs[k] = torch.clamp(integral, min=np.exp(-10**2), max=1)\n",
    "            entropy = (-probs*torch.log2(probs)).sum()\n",
    "            entropy_tot = entropy_tot + entropy\n",
    "            \n",
    "    mean_entropy = entropy_tot/(x_quantized.size()[0]*x_quantized.size()[1])\n",
    "    \n",
    "    if mean_entropy.isnan():\n",
    "        print(\"entropy is nan\")\n",
    "    if mean_entropy < 0:\n",
    "        print(\"entropy negative : \", mean_entropy)\n",
    "    return mean_entropy\n",
    "\n",
    "\n",
    "def distortion_pc(x, x_reconstructed, focal_length, skew, scaling_factor, image_size, principal_point):\n",
    "    # compute intrinsic matrix and its inverse\n",
    "    K = torch.tensor([[focal_length, skew, principal_point[0]], [ 0.0, focal_length, principal_point[1]], [0.0, 0.0, 1.0]])\n",
    "    K_ext = torch.eye(4)\n",
    "    K_ext[0:3:1, 0:3:1] = K\n",
    "    K_ext_inv = torch.inverse(K_ext).cuda()\n",
    "    \n",
    "    size_batch = x.size()[0]\n",
    "    total_mse = 0.0\n",
    "    for example in range(size_batch):\n",
    "        # compute x,y,z coords from u,v coords and gray-level value\n",
    "        z_reconstructed = x_reconstructed[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        z = x[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        bool_matrix = torch.logical_and(z_reconstructed != 0 , z != 0)\n",
    "            # for reconstructed depthmap\n",
    "        v_reconstructed = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v_reconstructed = v_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        u_reconstructed = (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u_reconstructed = u_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        z_reconstructed = z_reconstructed[bool_matrix].view(1, -1)\n",
    "        homogeneous_reconstructed_camera_points = torch.cat((u_reconstructed, v_reconstructed, torch.ones(1, u_reconstructed.size()[1]).cuda(), 1.0/z_reconstructed), 0)\n",
    "        homogeneous_reconstructed_coords = z_reconstructed*(torch.mm(K_ext_inv,homogeneous_reconstructed_camera_points))\n",
    "        coords_reconstructed = homogeneous_reconstructed_coords[0:3:1, :]\n",
    "            # for original depthmap\n",
    "        v = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v = v[bool_matrix].view(1, -1).cuda()\n",
    "        u =  (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u = u[bool_matrix].view(1, -1).cuda()    \n",
    "        z = z[bool_matrix].view(1, -1)\n",
    "        homogeneous_camera_points = torch.cat((u, v, torch.ones(1, u.size()[1]).cuda(), 1.0/z))\n",
    "        homogeneous_coords = z*(torch.mm(K_ext_inv,homogeneous_camera_points))\n",
    "        coords = homogeneous_coords[0:3:1, :]\n",
    "        \n",
    "        print(\" x original : \", torch.squeeze(z)[10]*scaling_factor)\n",
    "        print(\" original coords : \", coords[:, 10])\n",
    "        \n",
    "        # compute MSE on points\n",
    "        total_mse += torch.sum(torch.sqrt(torch.sum((coords_reconstructed - coords)**2, 0)))\n",
    "    return total_mse\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.05, 0.05).cuda()   \n",
    "    sum_log_gsm = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        sum_log_gsm[i] = (-torch.log2(gsm)).sum()\n",
    "    \n",
    "    entropy = torch.trapz(sum_log_gsm, u)\n",
    "    if entropy < 0:\n",
    "        print(\"negative entropy\")\n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def clip(x):\n",
    "    x_n = (x - torch.min(x))/(torch.max(x) - torch.min(x))\n",
    "    x_clipped = torch.round(255*x_n).float()\n",
    "    return x_clipped\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MyQuantization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input*10**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "        \n",
    "        \n",
    "class MyClipping(torch.autograd.Function):\n",
    "  \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input).clamp(min=0, max=2**16-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "    \n",
    "class MyConv2d_par(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d_par, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernel_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_channels, 1, self.n_channels, self.kernel_size_number))\n",
    "        #self.weight.data.uniform_(0, 1)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.out_channels, 1))\n",
    "        #self.bias.data.uniform_(0, 1)\n",
    "        \n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, x_segmented):\n",
    "        if x.is_cuda:\n",
    "            self.weight = self.weight.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        result_seg = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        \n",
    "        windows_depth = self.calculateWindows(x)\n",
    "        windows_seg = self.calculateWindows(x_segmented)\n",
    "        windows_seg[windows_seg < 1] = -1\n",
    "        windows_seg_centers = windows_seg[0, :, :, windows_seg.size()[3]//2].view(1, windows_seg.size()[1], windows_seg.size()[2], 1)\n",
    "        windows_seg = windows_seg * windows_seg_centers\n",
    "        windows_seg[windows_seg < 1] = 0 \n",
    "        windows_depth_seg = windows_depth * windows_seg\n",
    "        \n",
    "        \n",
    "        # compute result\n",
    "        ponderation = torch.sum(windows_seg, (2,3))\n",
    "        result = torch.sum(windows_depth_seg * self.weight , (2, 3))/ponderation + self.bias\n",
    "        result = result.reshape(x.shape[0], self.out_channels, width, height)\n",
    "        \n",
    "        # compute result_seg\n",
    "        shift_l = self.kernel_size[0]//2 - self.padding[0]\n",
    "        shift_c = self.kernel_size[1]//2 - self.padding[1]\n",
    "        if shift_l * shift_c > 0:\n",
    "            result_seg = x_segmented[:, :, shift_l:-shift_l, shift_c:-shift_c]\n",
    "        else:\n",
    "            if shift_l > 0:\n",
    "                result_seg = x_segmented[:, :, shift_l:-shift_l, :]\n",
    "            else:\n",
    "                if shift_c > 0:\n",
    "                    result_seg = x_segmented[:, :, :, shift_c:-shift_c]\n",
    "                else:\n",
    "                    result_seg = x_segmented\n",
    "\n",
    "        result_seg = result_seg[:, :, ::self.stride[0], ::self.stride[1]]\n",
    "        return result, result_seg\n",
    "        \n",
    "        \"\"\"\n",
    "        # compute result_seg\n",
    "        windows_seg_seg = self.calculateWindows(x_segmented) * windows_seg\n",
    "        result_seg = torch.sum(windows_seg_seg * self.weight , (2, 3)) + self.bias\n",
    "        result_seg = result_seg.reshape(x_segmented.shape[0], self.out_channels, width, height)\n",
    "        #result_seg = torch.clamp(result_seg, min=0, max=1)\n",
    "        thresh = (torch.max(result_seg) + torch.min(result_seg))/2.0\n",
    "        result_seg = (result_seg > thresh).float()\n",
    "        \n",
    "        return result, result_seg\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(1, -1, x.shape[1], self.kernel_size_number)\n",
    "        #windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1\n",
    "    \n",
    "\n",
    "class MyConv2d_inc(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d_inc, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernel_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernel_size_number))\n",
    "        #self.weight.data.uniform_(0, 1)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.out_channels))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x, x_segmented):\n",
    "        if x.is_cuda:\n",
    "            self.weight = self.weight.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        result_seg = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        \n",
    "        windows_depth = self.calculateWindows(x)\n",
    "        windows_seg = self.calculateWindows(x_segmented)\n",
    "        windows_seg[windows_seg < 1] = -1\n",
    "        windows_seg_centers = windows_seg[:, :, windows_seg.size()[2]//2].view(windows_seg.size()[0], windows_seg.size()[1], 1)\n",
    "        windows_seg = windows_seg * windows_seg_centers\n",
    "        windows_seg[windows_seg < 1] = 0 \n",
    "        windows_depth_seg = windows_depth * windows_seg\n",
    "        \n",
    "        \n",
    "        # compute result\n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows_depth_seg[channel], self.weight[i_convNumber][channel].view(-1, 1))\n",
    "                xx = xx.view(-1, width, height)/torch.sum(windows_seg[channel], 1).view(-1, width, height)\n",
    "                result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "            result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] /= x.shape[1]\n",
    "\n",
    "        result = result.view(x.shape[0], self.out_channels, width, height)\n",
    " \n",
    "        # compute result_seg\n",
    "        windows_seg_seg = self.calculateWindows(x_segmented) * windows_seg\n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows_seg_seg[channel], self.weight[i_convNumber][channel].view(-1, 1))\n",
    "                xx = xx.view(-1, width, height)\n",
    "                result_seg[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "\n",
    "        result_seg = result_seg.view(x_segmented.shape[0], self.out_channels, width, height)\n",
    "        result_seg = torch.clamp(result_seg, min=0, max=1)\n",
    "        \n",
    "        return result, result_seg\n",
    "        \n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernel_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(CustomConv2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernel_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernel_size_number))\n",
    "        self.weight.data.uniform_(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.is_cuda:\n",
    "            self.weight = self.weight.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        windows = self.calculateWindows(x)\n",
    "        \n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows[channel], self.weight[i_convNumber][channel])\n",
    "                xx = xx.view(-1, width, height)/self.kernel_size_number\n",
    "                result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "            result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] /= x.shape[1]\n",
    "\n",
    "        result = result.view(x.shape[0], self.out_channels, width, height)\n",
    "        return result\n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernel_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entre : 128*128\n",
    "class LossyCompAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = MyConv2d_par(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = MyConv2d_par(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = MyConv2d_par(128, 32, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = MyConv2d_par(32, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = MyConv2d_par(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = MyConv2d_par(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = MyConv2d_par(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = MyConv2d_par(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = MyConv2d_par(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 32))\n",
    "        #self.var.data.uniform_(0, 1)\n",
    "        #self.phi.data.uniform_(0, 1)\n",
    "        nn.init.kaiming_uniform_(self.var, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.phi, a=math.sqrt(5))\n",
    "    \n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(32).view(1, 32, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchNormed = nn.BatchNorm2d(32, momentum = False, affine=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, x_seg, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # get zero mask\n",
    "            #dilatation_mask = (x>0)\n",
    "                # removing black pixels\n",
    "            #x = black_pixels_removal_by_dilatation(x)\n",
    "            #if torch.any(x.isnan()):\n",
    "                #print(\"x after dilatation is nan\", x)\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            #x, radius, mean_channels = normalize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            x_seg = mirror_padding(x_seg, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x, x_seg = self.conv1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.conv2(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x_c1 = x.clone()\n",
    "            x_seg_c1 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"mean channels : \", mean_channels)\n",
    "                print(\"standard deviation : \", torch.sqrt(var))\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x, x_seg = self.resConv1_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv1_2(x, x_seg)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c1, min=0, max=1)\n",
    "            x_seg_c2 = x_seg\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weight gradients: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x, x_seg = self.resConv2_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv2_2(x, x_seg)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c2, min=0, max=1)\n",
    "            x_seg_c3 = x_seg\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "            \n",
    "                # residual block 3\n",
    "            x, x_seg = self.resConv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv3_2(x, x_seg)\n",
    "            x += x_c3\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c3, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "            \n",
    "                # output block\n",
    "            x, x_seg = self.conv3(x, x_seg)\n",
    "            x_latent_before_quantization = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x, x_seg = self.subpix1(x, x_seg)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_seg = upsample(x_seg, 2)\n",
    "            #x_seg = periodic_shuffling(x_seg, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            x_seg_c4 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                # residual block 1\n",
    "            x, x_seg = self.deconv1_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv1_2(x, x_seg)\n",
    "            x += x_c4\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c4, min=0, max=1)\n",
    "            x_c5 = x.clone()\n",
    "            x_seg_c5 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                   # residual block 2\n",
    "            x, x_seg = self.deconv2_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv2_2(x, x_seg)\n",
    "            x += x_c5\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c5, min=0, max=1)\n",
    "            x_c6 = x.clone()\n",
    "            x_seg_c6 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "            \n",
    "                   # residual block 3\n",
    "            x, x_seg = self.deconv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv3_2(x, x_seg)\n",
    "            x += x_c6\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c6, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "            \n",
    "                    # subpixel 2\n",
    "            x, x_seg = self.subpix2(x, x_seg)\n",
    "            x = periodic_shuffling(x, 256//4)\n",
    "            x_seg = upsample(x_seg, 2)\n",
    "            #x_seg = periodic_shuffling(x_seg, 256//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                    # subpixel 3\n",
    "            x, x_seg = self.subpix3(x, x_seg)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            #x = denormalize_output(x, radius, mean_channels)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "                    # replace black pixels\n",
    "           # x = x*dilatation_mask\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized, x_latent_before_quantization\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                x_seg = mirror_padding(x_seg, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x, x_seg))\n",
    "                x_seg = F.relu(self.conv1(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x = F.relu(self.conv2(x, x_seg))\n",
    "                x_seg = F.relu(self.conv2(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x, x_seg))\n",
    "                x = self.resConv1_2(x, x_seg)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x, x_seg))\n",
    "                x = self.resConv2_2(x, x_seg)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x, x_seg))\n",
    "                x = self.resConv3_2(x, x_seg)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x, x_seg)\n",
    "                x_seg = F.relu(self.conv3(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "            \n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            # batch normalization\n",
    "            x = self.batchNormed(x)\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x, x_seg)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_seg = self.subpix1(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                    print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x, x_seg))\n",
    "                x = self.deconv1_2(x, x_seg)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x, x_seg))\n",
    "                x = self.deconv2_2(x, x_seg)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x, x_seg))\n",
    "                x = self.deconv3_2(x, x_seg)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x, x_seg)\n",
    "                x = periodic_shuffling(x, 256//4)\n",
    "                x_seg = self.subpix2(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 256//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                    print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                    print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x, x_seg)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                x_seg = self.subpix3(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                    print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                    print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entre : 128*128\n",
    "class LossyCompAutoencoder_bis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder_bis, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = nn.Conv2d(128, 32, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = nn.Conv2d(32, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = nn.Conv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = nn.Conv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(32).view(1, 32, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.resConv1_1(x))\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x = F.relu(self.resConv2_1(x))\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "                # residual block 3\n",
    "            x = F.relu(self.resConv3_1(x))\n",
    "            x = self.resConv3_2(x)\n",
    "            x += x_c3\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.deconv1_1(x))\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                   # residual block 2\n",
    "            x = F.relu(self.deconv2_1(x))\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                   # residual block 3\n",
    "            x = F.relu(self.deconv3_1(x))\n",
    "            x = self.deconv3_2(x)\n",
    "            x += x_c6\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = F.relu(periodic_shuffling(x, 256//4))\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "\n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = F.relu(periodic_shuffling(x, 256//4))\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entre : 128*128\n",
    "class LossyCompAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = CustomConv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = CustomConv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = CustomConv2d(128, 16, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = CustomConv2d(16, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = CustomConv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = CustomConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = MyConv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = CustomConv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = CustomConv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 16))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 16))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(16).view(1, 16, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchNormed = nn.BatchNorm2d(16, momentum = False, affine=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # get zero mask\n",
    "            #dilatation_mask = (x>0)\n",
    "                # removing black pixels\n",
    "            #x = black_pixels_removal_by_dilatation(x)\n",
    "            #if torch.any(x.isnan()):\n",
    "                #print(\"x after dilatation is nan\", x)\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            #x, radius, mean_channels = normalize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after normalization is inf\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after mirror padding is inf\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"mean channels : \", mean_channels)\n",
    "                print(\"standard deviation : \", torch.sqrt(var))\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after input block is inf\", x)\n",
    "                # residual block 1\n",
    "            x = self.resConv1_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weight gradients: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 1 is inf\", x)\n",
    "                # residual block 2\n",
    "            x = self.resConv2_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 2 is inf\", x)\n",
    "            \"\"\"\n",
    "                # residual block 3\n",
    "            x, x_seg = self.resConv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv3_2(x, x_seg)\n",
    "            x += x_c3\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c3, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "            \"\"\"\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after output block is inf\", x)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after quantization block is inf\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after mask is inf\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after subpix 1 is inf\", x)\n",
    "                # residual block 1\n",
    "            x = self.deconv1_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 1 is inf\", x)\n",
    "                   # residual block 2\n",
    "            x = self.deconv2_1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after residual block 2 is inf\", x)\n",
    "            \"\"\"\n",
    "                   # residual block 3\n",
    "            x, x_seg = self.deconv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv3_2(x, x_seg)\n",
    "            x += x_c6\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c6, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "            \"\"\"\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = periodic_shuffling(x, 256//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after subpix 2 is inf\", x)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after subpix 3 is inf\", x)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            #x = denormalize_output(x, radius, mean_channels)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after denormalization is inf\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "            if torch.any(torch.isinf(x)):\n",
    "                print(\"x after clipping is inf\", x)\n",
    "                    # replace black pixels\n",
    "           # x = x*dilatation_mask\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "            \n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            # batch normalization\n",
    "            x = self.batchNormed(x)\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                    print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = periodic_shuffling(x, 256//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                    print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                    print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                    print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                    print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossyCompAutoencoder(\n",
      "  (conv1): MyConv2d_par()\n",
      "  (conv2): MyConv2d_par()\n",
      "  (resConv1_1): MyConv2d_par()\n",
      "  (resConv1_2): MyConv2d_par()\n",
      "  (resConv2_1): MyConv2d_par()\n",
      "  (resConv2_2): MyConv2d_par()\n",
      "  (resConv3_1): MyConv2d_par()\n",
      "  (resConv3_2): MyConv2d_par()\n",
      "  (conv3): MyConv2d_par()\n",
      "  (subpix1): MyConv2d_par()\n",
      "  (deconv1_1): MyConv2d_par()\n",
      "  (deconv1_2): MyConv2d_par()\n",
      "  (deconv2_1): MyConv2d_par()\n",
      "  (deconv2_2): MyConv2d_par()\n",
      "  (deconv3_1): MyConv2d_par()\n",
      "  (deconv3_2): MyConv2d_par()\n",
      "  (subpix2): MyConv2d_par()\n",
      "  (subpix3): MyConv2d_par()\n",
      "  (batchNormed): BatchNorm2d(32, eps=1e-05, momentum=False, affine=False, track_running_stats=True)\n",
      ")\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# instanciate the model\n",
    "model = LossyCompAutoencoder()\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      " loss distortion :  tensor(754069.437500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.298322, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1313139.375000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.280205, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(577493.125000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.258788, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1369293.250000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.270539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1559042.875000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.279220, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1419116.125000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.289610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1361164.625000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.270026, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(757238.375000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.273818, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(306829.187500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.260011, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(2793445., device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.257414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(550889.375000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.272658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(460011.875000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.233686, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(676599.312500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.256189, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1673621.250000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.263305, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(462493.281250, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.242177, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(2629903.250000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.235083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1592736.375000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.227265, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(2354036.750000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.232031, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(3370711.250000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.227168, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(2383318.750000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.231026, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(826213.312500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.234560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(471433.031250, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.192101, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(713015.187500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.252140, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(790170.062500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.202628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(439718.812500, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.192567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(977945.375000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.199263, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(901565.750000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.215236, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(770277.750000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.226771, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " loss distortion :  tensor(1868936.125000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "loss bit :  tensor(6.213652, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b25463138ebb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mbatch_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mbatch_segs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mdecoded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_quantized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_latent_before_quantization\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_segs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m#[decoded_images, x_quantized] = model(batch_images, 1, True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-38cec1694568>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_seg, mask, return_xq, is_lambda)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeconv2_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeconv2_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mx_c5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_seg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx_seg_c5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7e9c8b7fb8a0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_segmented)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0mwindows_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculateWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mwindows_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculateWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_segmented\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[0mwindows_seg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindows_seg\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[0mwindows_seg_centers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindows_seg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindows_seg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindows_seg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindows_seg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mwindows_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindows_seg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mwindows_seg_centers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transfert du model au gpu\n",
    "model.to(device)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "        \n",
    "      \n",
    "\n",
    "# define beta\n",
    "beta = 0.0001\n",
    "#beta = 1000\n",
    "\n",
    "# incremental update of coefficients  \n",
    "# define threshold and loss_init\n",
    "threshold = 0.95\n",
    "loss_init = float(\"Inf\")\n",
    "nb_ones = 1\n",
    "iteration = 0\n",
    "mask=(compute_mask(1, (32, 16, 16)).unsqueeze(0)).cuda()\n",
    "dim_latent = 16*16*32\n",
    "output_flag = False\n",
    "\n",
    "mask=1\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch : \", epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    if epoch==100:\n",
    "        # define a new learning rate and so a new optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \"\"\"\n",
    "        \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        #print(\"ibatch : \", i_batch)\n",
    "        batch_images = data[0].to(device).float()\n",
    "        batch_masks = data[1].to(device).float()\n",
    "        batch_segs = data[2].to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = model(batch_images, batch_segs.detach(), mask, True)\n",
    "        #[decoded_images, x_quantized] = model(batch_images, 1, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        #loss_dist = distortion_pc(batch_images, decoded_images, 525.0, 0, 1000.0, (128, 128), (319.5, 239.5))\n",
    "        #loss_bit = entropy_dist(x_quantized, model.phi, model.var)\n",
    "        loss_bit = mean_bit_per_px(x_quantized, model.phi, model.var)\n",
    "        print(\" loss distortion : \", loss_dist)\n",
    "        print(\"loss bit : \", loss_bit)\n",
    "        #loss = loss_dist + beta*loss_bit\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(\"loss : \", loss)\n",
    "        loss.backward()\n",
    "        #print(\"input weights conv 1 gradient: \", model.conv1.weight.grad)\n",
    "        #print(\"input bias conv 1 gradient: \",  model.conv1.bias.grad)\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        \n",
    "        \"\"\"\n",
    "        # check the value of the loss to see if another coefficient can be enabled\n",
    "        if (nb_ones<dim_latent):\n",
    "            if (loss.item() < loss_init*threshold or iteration > 0):\n",
    "                nb_ones +=1\n",
    "                loss_init = loss.item()\n",
    "                iteration = 0\n",
    "                mask = (compute_mask(nb_ones, tuple(x_quantized.size()[1:])).unsqueeze(0)).cuda()\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        iteration += 1\n",
    "        \n",
    "        x_latent_before_quantization_copy = torch.clone(x_latent_before_quantization).detach().view(1, -1).cpu().numpy()\n",
    "        np.savetxt(\"C:\\\\Users\\\\Flora\\\\autoencoder\\\\training\\\\latent\\\\\" + str(i_batch)+\"_latent.txt\", x_latent_before_quantization_copy)\n",
    "        x_quantized_copy = torch.clone(x_quantized).detach().view(1, -1).cpu().numpy()\n",
    "        np.savetxt(\"C:\\\\Users\\\\Flora\\\\autoencoder\\\\training\\\\quantized\\\\\" + str(i_batch)+\"_quantized.txt\", x_quantized_copy) \n",
    "        \n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d63e7dc7829>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.data)\n",
    "\n",
    "\n",
    "print(\"input weights conv 1 gradient: \", model.conv1.weight.grad)\n",
    "print(\"conv1.weights grad: \", params[0].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e45d58f1fa4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# transfert du model au gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLossyCompAutoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# transfert du model au gpu\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_segmented_Incremental.pth'))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "        \n",
    "      \n",
    "\n",
    "# define beta\n",
    "beta = 0.00001\n",
    "#beta = 1000\n",
    "\n",
    "# incremental update of coefficients  \n",
    "# define threshold and loss_init\n",
    "threshold = 0.95\n",
    "loss_init = float(\"Inf\")\n",
    "nb_ones = 1\n",
    "iteration = 0\n",
    "mask=(compute_mask(1, (32, 16, 16)).unsqueeze(0)).cuda()\n",
    "dim_latent = 16*16*32\n",
    "output_flag = False\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 2\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch : \", epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    if epoch==100:\n",
    "        # define a new learning rate and so a new optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \"\"\"\n",
    "        \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data[0].to(device).float()\n",
    "        batch_masks = data[1].to(device).float()\n",
    "        batch_segs = data[2].to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = model(batch_images, batch_segs.detach(), mask, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        loss_bit = entropy_dist(x_quantized, model.phi, model.var)\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        loss.backward()\n",
    "        \n",
    "        # check the value of the loss to see if another coefficient can be enabled\n",
    "        if (nb_ones<dim_latent):\n",
    "            if (loss.item() < loss_init*threshold or iteration > 0):\n",
    "                nb_ones +=1\n",
    "                loss_init = loss.item()\n",
    "                iteration = 0\n",
    "                mask = (compute_mask(nb_ones, tuple(x_quantized.size()[1:])).unsqueeze(0)).cuda()\n",
    "    \n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        iteration += 1\n",
    "        x_quantized_copy = torch.clone(x_quantized).detach().view(1, -1).cpu().numpy()\n",
    "        np.savetxt(\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\latent\\\\\" + str(i_batch)+\"_latent.txt\", x_quantized_copy) \n",
    "        #torch.save(x_quantized_copy,\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\latent\\\\\" + str(i_batch)+\"_latent.txt\")\n",
    "        #decoded_images_copy = torch.clone(decoded_images).detach()\n",
    "        #cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps\\\\training\\\\reconstructed\\\\\" + \"img\" + str(i_batch)+\".png\", np.squeeze(decoded_images_copy.cpu()).numpy().astype(np.uint16))\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save(model.state_dict(), './model_parameters/lossy_comp_depthmap3_beta000001_segmented_nonIncremental_distOnly.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "    \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "# transfert du model au gpu\n",
    "model_bis.to(device)\n",
    "\n",
    "# general update of coefficients    \n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(model_bis.parameters(), lr=0.00001)\n",
    "    # define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "    #Epochs\n",
    "n_epochs = 420\n",
    "beta = 0.00001\n",
    "nb_updates = 0\n",
    "learning_rate = 0.0001\n",
    "tau = 10000\n",
    "kappa = 0.8\n",
    "\n",
    "    # Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "          \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        learning_rate *= tau**kappa/(tau + nb_updates)**kappa\n",
    "        print(\"learning_ rate : \", learning_rate)\n",
    "        optimizer = torch.optim.Adam(model_bis.parameters(), lr=learning_rate)\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = model_bis(batch_images, 1, True, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        loss_bit = mean_bit_per_px(x_quantized, model_bis.phi, model_bis.var)\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(loss)\n",
    "            \n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        nb_updates += 1\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "\n",
    "model_bis = LossyCompAutoencoder_bis()\n",
    "model_bis.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model_bis.train()\n",
    "model_bis.to(device)\n",
    "\n",
    "\n",
    "weights_model_bis = list(model_bis.parameters())\n",
    "print(\"nb parameters of the model : \", len(weights_model_bis))\n",
    "for (name_bis, parameter_bis), (name, parameter) in zip(model_bis.named_parameters(), model.named_parameters()) :\n",
    "    \"\"\"\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter_bis)\n",
    "        parameter_bis.data = torch.FloatTensor(1, 96, 1, 1).uniform_(0.9, 1.1)\n",
    "        print(\"lamb new parameter : \", parameter_bis)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name_bis)\n",
    "    print(\"size of user-defined parameters : \", parameter_bis.size())\n",
    "    parameter.data = parameter_bis.data\n",
    "\n",
    "for name_bis, parameter_bis in model_bis.named_parameters():\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"check lamb parameter : \", parameter_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model = list(model.parameters())\n",
    "for i in range(0, 39, 1):\n",
    "    weights_model[i] = weights_model_bis[i]\n",
    "    print(\"size of user-defined parameters : \", weights_model_bis[i].size())\n",
    "    print(\"weights model: \" ,weights_model_bis[i])\n",
    "model.train()    \n",
    "#weights_model[39] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "#weights_model[40] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    \"\"\"\n",
    "    if name == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter)\n",
    "        parameter.data = torch.FloatTensor(1, 96, 1, 1).uniform_(1, 1)\n",
    "        print(\"lamb new parameter : \", parameter)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name)\n",
    "    print(\"size of user-defined parameters : \", parameter.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min vec latent :  tensor(-1478., device='cuda:0')\n",
      "max vec latent :  tensor(1679., device='cuda:0')\n",
      "min vec latent :  tensor(-1603., device='cuda:0')\n",
      "max vec latent :  tensor(1803., device='cuda:0')\n",
      "min vec latent :  tensor(-1602., device='cuda:0')\n",
      "max vec latent :  tensor(1767., device='cuda:0')\n",
      "min vec latent :  tensor(-1478., device='cuda:0')\n",
      "max vec latent :  tensor(1680., device='cuda:0')\n",
      "min vec latent :  tensor(-1553., device='cuda:0')\n",
      "max vec latent :  tensor(1778., device='cuda:0')\n",
      "min vec latent :  tensor(-1604., device='cuda:0')\n",
      "max vec latent :  tensor(1771., device='cuda:0')\n",
      "min vec latent :  tensor(-1570., device='cuda:0')\n",
      "max vec latent :  tensor(1739., device='cuda:0')\n",
      "min vec latent :  tensor(-1603., device='cuda:0')\n",
      "max vec latent :  tensor(1741., device='cuda:0')\n",
      "min vec latent :  tensor(-1510., device='cuda:0')\n",
      "max vec latent :  tensor(1680., device='cuda:0')\n",
      "min vec latent :  tensor(-1478., device='cuda:0')\n",
      "max vec latent :  tensor(1679., device='cuda:0')\n",
      "min vec latent :  tensor(-1602., device='cuda:0')\n",
      "max vec latent :  tensor(1737., device='cuda:0')\n",
      "min vec latent :  tensor(-1569., device='cuda:0')\n",
      "max vec latent :  tensor(1783., device='cuda:0')\n",
      "min vec latent :  tensor(-1600., device='cuda:0')\n",
      "max vec latent :  tensor(1769., device='cuda:0')\n",
      "min vec latent :  tensor(-1477., device='cuda:0')\n",
      "max vec latent :  tensor(1680., device='cuda:0')\n",
      "min vec latent :  tensor(-1585., device='cuda:0')\n",
      "max vec latent :  tensor(1741., device='cuda:0')\n",
      "min vec latent :  tensor(-1553., device='cuda:0')\n",
      "max vec latent :  tensor(1767., device='cuda:0')\n",
      "min vec latent :  tensor(-1478., device='cuda:0')\n",
      "max vec latent :  tensor(1679., device='cuda:0')\n",
      "min vec latent :  tensor(-1478., device='cuda:0')\n",
      "max vec latent :  tensor(1680., device='cuda:0')\n",
      "min vec latent :  tensor(-1505., device='cuda:0')\n",
      "max vec latent :  tensor(1785., device='cuda:0')\n",
      "min vec latent :  tensor(-1572., device='cuda:0')\n",
      "max vec latent :  tensor(1768., device='cuda:0')\n",
      "min vec latent :  tensor(-1543., device='cuda:0')\n",
      "max vec latent :  tensor(1785., device='cuda:0')\n",
      "min vec latent :  tensor(-1480., device='cuda:0')\n",
      "max vec latent :  tensor(1680., device='cuda:0')\n",
      "min vec latent :  tensor(-1603., device='cuda:0')\n",
      "max vec latent :  tensor(1753., device='cuda:0')\n",
      "min vec latent :  tensor(-1478., device='cuda:0')\n",
      "max vec latent :  tensor(1680., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 48 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test set \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_segmented_Incremental.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "#test_dataset = ImageDataset(root_dir='./data/kodac/', transform=transforms.Compose([RandomCrop(128), ToTensor()]))\n",
    "#test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "test_dataset = ImageDataset(depthmap_dir='C:/Users/Flora/autoencoder/test/dilated', \n",
    "                                mask_dir='C:/Users/Flora/autoencoder/test/mask',\n",
    "                                segmentation_dir='C:/Users/Flora/autoencoder/test/segmentation_roipoly',\n",
    "                                transform=transforms.Compose([RandomCrop((128, 128)), ToTensor()])\n",
    "                                 )\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():            \n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i][0].unsqueeze(0).to(device).float()\n",
    "        test_mask = test_dataset[i][1].unsqueeze(0).to(device).float()\n",
    "        test_seg = test_dataset[i][2].unsqueeze(0).to(device).float()\n",
    "        #test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "               \n",
    "        if test_image.size(2)>test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "            test_seg = test_seg.permute(0, 1, 3, 2)\n",
    "\n",
    "        #[reconstructed_image, vec_latent] = model(test_image, 1, True)\n",
    "        [reconstructed_image, vec_latent, vec_latent_before_quantization] =  model(test_image, test_seg, 1, True)\n",
    "        print(\"min vec latent : \", torch.min(vec_latent))\n",
    "        print(\"max vec latent : \", torch.max(vec_latent))\n",
    "\n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(torch.squeeze(reconstructed_image.int().cpu()))\n",
    "\n",
    "        \"\"\"\n",
    "        # We can set the number of bins with the `bins` kwarg\n",
    "        bins_list = [-6.5, -5.5, -4.5, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n",
    "        for k in range(96):\n",
    "            # plot histograms\n",
    "            fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "            ax.hist((vec_latent[:, k, :, :]).view(-1).cpu(), bins=bins_list)\n",
    "            plt.savefig(\"D:\\\\autoencoder_data\\\\histograms\\\\depthmap\\\\\" + \"img\" + str(i)+ \"hist\" + str(k) + \".png\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = model(test_image)\n",
    "        reconstructed_depthmap = np.squeeze(reconstructed_image.cpu()).numpy()\n",
    "        cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\reconstructed\\\\beta_1\\\\\" + \"img\" + str(i)+\".png\", reconstructed_depthmap.astype(np.uint16))\n",
    "        #save_image(reconstructed_depthmap, \"D:\\\\autoencoder_data\\\\depthmaps\\\\reconstructed\\\\beta_0001\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PSNR for each image of the test set and its reconstruction\n",
    "\n",
    "def write_data(filepath , tensor_data):\n",
    "    batch, channel, h, w = tensor_data.size()\n",
    "    matrix = tensor_data.cpu().numpy()\n",
    "    file = open(filepath, \"w\")\n",
    "    for image in range(batch):\n",
    "        np.savetxt(file, matrix[image, :, :, :].reshape(channel*h, w), fmt ='%.0f')\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "psnr_sum = 0.0\n",
    "bit_rate_sum = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model(test_image, 1, True)\n",
    "        #write_data('.\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2\\\\latent_vect\\\\' + 'vec' + str(i) +'.txt', im_quantized)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(2)*test_image.size(3))\n",
    "        psnr_sum+= psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0)\n",
    "        bit_rate_sum += nbpp\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0))\n",
    "print( \"mean nb bits per pixel : \", bit_rate_sum/len(test_dataset))\n",
    "print(\"psnr mean : \", psnr_sum/len(test_dataset) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "       \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "# Load previous model\n",
    "model_prev = LossyCompAutoencoder()\n",
    "model_prev.load_state_dict(torch.load('./model_parameters/lossy_comp_params_with_rate_beta2_incremental_2.pth'))\n",
    "model_prev.eval()\n",
    "model_prev.to(device)\n",
    "\n",
    "\n",
    "# And run test \n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model_prev(test_image,1, True)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(1)*test_image.size(2)*test_image.size(3))\n",
    "        print(\"nb_symbols : \", nb_symbols)\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 255.0))\n",
    "    \n",
    "# And print figures\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        if test_image.size(2)<test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "        \n",
    "        reconstructed_image = model_prev(test_image, 1,  False)\n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.squeeze(reconstructed_image.int().cpu()).permute(1, 2, 0))\n",
    "        \n",
    "# And save figures\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = np.squeeze(model_prev(test_image).cpu())\n",
    "        print(reconstructed_image.type())\n",
    "        save_image(reconstructed_image, \".\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2_incremental_bis\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.01, 0.01).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    entropy = torch.trapz(gsm_sum, u)\n",
    "    \n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "# Load incremental model\n",
    "incremental_model = LossyCompAutoencoder()\n",
    "incremental_model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta1_without_bitrate.pth'))\n",
    "incremental_model.train()\n",
    "incremental_model.to(device)\n",
    "\n",
    "# train again the model starting form incremental-learned weights\n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(incremental_model.parameters(), lr=0.00001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "\n",
    "\n",
    "# define beta\n",
    "beta = 1.0\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 800\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "           \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized, x_latent_before_quantization] = incremental_model(batch_images, 1, True, False)\n",
    "        optimizer.zero_grad()\n",
    "        #entropy = entropy_rate(x_quantized, incremental_model.phi, incremental_model.var)\n",
    "        #print(\"entropy : \", entropy)\n",
    "        dist = distortion(decoded_images, batch_images)\n",
    "        #print(\"distortion : \", dist)\n",
    "        #loss = beta * dist + entropy\n",
    "        loss = beta*dist\n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from range_coder import RangeEncoder, RangeDecoder, prob_to_cum_freq\n",
    "import os\n",
    "\n",
    "# Load previous model\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_params_with_rate_beta0005_incremental.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "nb_bits = 0.0\n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():  \n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "            # compute symbol probabilities\n",
    "        min_val = data_comp.min()\n",
    "        if min_val <0:\n",
    "            data_comp -= min_val\n",
    "            min_val = 0\n",
    "        max_val = data_comp.max()\n",
    "        nb_bins = max_val - min_val + 1\n",
    "        hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "        prob = hist/hist.sum()\n",
    "        #print(\"data comp : \", data_comp)\n",
    "        #print(prob)\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(torch.nonzero(prob)) != len(prob):\n",
    "            zero_indices = ((prob == 0).nonzero())\n",
    "            for j in reversed(range(0, len(zero_indices), 1)):\n",
    "                data_comp[data_comp > int(zero_indices[j])+min_val] -=1\n",
    "            min_val = data_comp.min()\n",
    "            max_val = data_comp.max()\n",
    "            nb_bins = max_val - min_val + 1\n",
    "            hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "            prob = hist/hist.sum()\n",
    "            print(min_val)\n",
    "            print(max_val)\n",
    "            print(\"data comp : \", data_comp)\n",
    "            print(prob)\n",
    "         \"\"\" \n",
    "            \n",
    "            # convert probabilities to cumulative integer frequency table\n",
    "        #cumFreq = prob_to_cum_freq(torch.clamp(prob, min=np.finfo(np.float32).eps).cpu(), resolution=128)\n",
    "        cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "        #print(cumFreq)\n",
    "        \n",
    "        # encode data\n",
    "        filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(i) + \".bin\"\n",
    "        encoder = RangeEncoder(filepath_to_write)\n",
    "        #print(torch.flatten(data_comp).cpu().tolist())\n",
    "        encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "        encoder.close()\n",
    "        \n",
    "        \n",
    "        file_size = os.path.getsize(filepath_to_write)*8 #number of bits in the file\n",
    "        print(file_size)\n",
    "        nb_bits += file_size\n",
    "        \n",
    "    nb_bits_per_image = nb_bits/len(test_dataset)\n",
    "    print(nb_bits_per_image)\n",
    "    nb_bits_per_pixel = nb_bits_per_image/(512*768)\n",
    "    print(nb_bits_per_pixel)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "    test_image = test_dataset[1].unsqueeze(0).to(device).float()\n",
    "    [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "        # compute symbol probabilities\n",
    "    min_val = data_comp.min()\n",
    "    print(min_val)\n",
    "    print(data_comp.max())\n",
    "    if min_val <0:\n",
    "        data_comp -= min_val\n",
    "        min_val = 0\n",
    "    max_val = data_comp.max()\n",
    "    \n",
    "    print(max_val)\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    print(nb_bins)\n",
    "    hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    prob = hist/hist.sum()\n",
    "    print(prob)\n",
    "         # convert probabilities to cumulative integer frequency table\n",
    "    cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "    #print(cumFreq)\n",
    "\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    \n",
    "    \n",
    "        # encode data\n",
    "    filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(1) + \".bin\"\n",
    "    encoder = RangeEncoder(filepath_to_write)\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "    encoder.close()   \n",
    "\"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "#fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_image = test_dataset[i].unsqueeze(0).float().cuda()\n",
    "\n",
    "    #test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "    test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "    cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\dilated\\\\\" + \"img\" + str(i)+\".png\", np.squeeze(test_image_without_black_px.cpu().numpy()).astype(np.uint16))\n",
    "    #ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "    #plt.imshow(torch.squeeze(test_image_without_black_px).cpu())\n",
    "\n",
    "\"\"\"    \n",
    "i = 0\n",
    "test_image = test_dataset[i].unsqueeze(0).float()\n",
    "\n",
    "#test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "plt.imshow(torch.squeeze(test_image_without_black_px))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
