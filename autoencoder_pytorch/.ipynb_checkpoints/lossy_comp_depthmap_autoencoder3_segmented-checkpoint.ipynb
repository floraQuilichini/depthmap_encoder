{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import pandas as pd\n",
    "from skimage import io, transform\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy import signal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "torch.set_printoptions(precision=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([14, 1, 128, 128])\n",
      "1 torch.Size([14, 1, 128, 128])\n",
      "2 torch.Size([14, 1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntransform = transforms.Compose([\\n    transforms.Pad(12, padding_mode='reflect'),\\n    transforms.ToTensor()])\\n\\ntrainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, depthmap_dir, mask_dir, segmentation_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            depthmap_dir (string): Directory with all the depthmaps.\n",
    "            mask_dir (string): Directory with all the masks.\n",
    "            segmentation_dir (string): Directory with all the segmentation of the depthmaps.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.depthmap_dir = depthmap_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        path, dirs, files = next(os.walk(self.depthmap_dir))\n",
    "        file_count = len(files)\n",
    "        return file_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path_d, dirs_d, files_d = next(os.walk(self.depthmap_dir))\n",
    "        img_d_name = os.path.join(self.depthmap_dir, files_d[idx])\n",
    "        image_d = io.imread(img_d_name)\n",
    "        \n",
    "        path_m, dirs_m, files_m = next(os.walk(self.mask_dir))\n",
    "        img_m_name = os.path.join(self.mask_dir, files_m[idx])\n",
    "        image_m = io.imread(img_m_name)\n",
    "        \n",
    "        path_s, dirs_s, files_s = next(os.walk(self.segmentation_dir))\n",
    "        img_s_name = os.path.join(self.segmentation_dir, files_s[idx])\n",
    "        image_s = io.imread(img_s_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image_d = self.transform(image_d)\n",
    "            image_m = self.transform(image_m)\n",
    "            image_s = self.transform(image_s)\n",
    "\n",
    "        return image_d, image_m, image_s\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        if h!= new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top = 0\n",
    "        if w!= new_w:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0\n",
    "     \n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        \n",
    "        image = np.expand_dims(image, axis=2)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class Downsample(object):\n",
    "    \"\"\"Downsample the image\n",
    "\n",
    "    Args:\n",
    "        downsampling_factor (int or tuple): Desired downsampling factor for rows and columns.\n",
    "        If the downsampling factor is an int, then both rows and columns are sampled by the same factor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampling_factor):\n",
    "        assert isinstance(downsampling_factor, (int, tuple))\n",
    "        if isinstance(downsampling_factor, int):\n",
    "            self.downsampling_factor = (downsampling_factor, downsampling_factor)\n",
    "        else:\n",
    "            assert len(downsampling_factor) == 2\n",
    "            self.downsampling_factor = downsampling_factor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        down_fact_h, down_fact_w = self.downsampling_factor\n",
    "        image = image[::down_fact_h,\n",
    "                      ::down_fact_w]\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class ConvertDepthToColor(object):\n",
    "    \"\"\" convert a 1xmxn 16-bits depthmap to a 2xmxn 8-bits colormap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if len(image.shape[:]) <3:\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            \n",
    "        h, w = image.shape[:2]\n",
    "        image_r_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color[(image > 2**8 - 1)] = image[(image > 2**8 - 1)] >> 8\n",
    "        image_r_color = image - (image_g_color <<8)\n",
    "\n",
    "        return np.concatenate((image_r_color, image_g_color), axis=2)\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in images to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, images):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        images = images.transpose((2, 0, 1))\n",
    "        images = images.astype(float)\n",
    "        return torch.from_numpy(images)\n",
    "    \n",
    "\n",
    "    \n",
    "class ConvertColorToDepth(object):\n",
    "    \"\"\" convert a 2xmxn 8-bits colormap to a 1xmxn 16-bits depthmap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, color_image):\n",
    "            \n",
    "        depth_image = color_image[:, 0, :, :]\n",
    "        depth_image += color_image[:, 1, :, :] << 8\n",
    "\n",
    "        return depth_image\n",
    "\n",
    "    \n",
    "def show_image_batch(images_batch):\n",
    "    \"\"\"Show image for a batch of samples.\"\"\"\n",
    "    if images_batch.size(1) == 1:\n",
    "        images_batch_normed = images_batch/torch.max(images_batch)\n",
    "        grid = utils.make_grid(images_batch_normed)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    else:\n",
    "        print(images_batch.size())\n",
    "        images_b_batch = torch.zeros(images_batch.size(0), 1, images_batch.size(2) , images_batch.size(3))\n",
    "        images_color_batch = torch.cat((images_batch, images_b_batch), 1) \n",
    "        print(images_color_batch.size())\n",
    "        grid = utils.make_grid(images_color_batch)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Load training dataset\n",
    "\n",
    "#mirror_padding_transform = transforms.Compose([transforms.ToPILImage(), transforms.Pad(padding=12, padding_mode='reflect'), transforms.ToTensor()])\n",
    "    # the  datasets contain depthmaps (first composant), masks (second composant) and segmentations (third composant)\n",
    "transformed_datasets = ImageDataset(depthmap_dir='D:/autoencoder_data/depthmaps/training/dilated', \n",
    "                                mask_dir='D:/autoencoder_data/depthmaps/training/mask',\n",
    "                                segmentation_dir='D:/autoencoder_data/depthmaps/training/segmented',\n",
    "                                transform=transforms.Compose([RandomCrop((384, 640)), Downsample(( 3, 5)), ToTensor()])\n",
    "                                 )\n",
    "dataloader = DataLoader(transformed_datasets, batch_size=14, shuffle=True, num_workers=0)\n",
    "\n",
    "for i_batch, batch_images in enumerate(dataloader):\n",
    "    print(i_batch, batch_images[2].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 2:\n",
    "        plt.figure()\n",
    "        show_image_batch(batch_images[2])\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(12, padding_mode='reflect'),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define additionnnal functions\n",
    "def periodic_shuffling(T, C):\n",
    "    T_copy = T.clone()\n",
    "    batch_size = T.size()[0]\n",
    "    H = T.size()[2]\n",
    "    W = T.size()[3]\n",
    "    T = T.view(batch_size, C, H*2, W*2)\n",
    "    \"\"\"\n",
    "    for k in range(C):\n",
    "        for i in range(2*H):\n",
    "            for j in range(2*W):\n",
    "                T[:, k, i, j] = T_copy[:, C*((j&1)<<1)+C*(i&1)+k, i>>1, j>>1]\n",
    "    \"\"\"\n",
    "                \n",
    "    T[:, :, ::2, ::2] = T_copy[:, 0:C, :, :]\n",
    "    T[:, :, 1::2, ::2] = T_copy[:, C:2*C, :, :]\n",
    "    T[:, :, ::2, 1::2] = T_copy[:, 2*C:3*C, :, :]\n",
    "    T[:, :, 1::2, 1::2] = T_copy[:, 3*C:4*C, :, :]\n",
    "\n",
    "    return T\n",
    "    \n",
    "    \n",
    "def mirror_padding(x, padding_size):\n",
    "    up_line = x[:, :, 0:padding_size, :].flip(2)\n",
    "    left_col = x[:, :, :, 0:padding_size].flip(3)\n",
    "    right_col = x[:, :, :, -padding_size:].flip(3)\n",
    "    bottom_line = x[:, :, -padding_size:, :].flip(2)\n",
    "    left_up_corner = left_col[:, :, 0:padding_size, :].flip(2)\n",
    "    right_up_corner = right_col[:, :, 0:padding_size, :].flip(2)\n",
    "    left_bottom_corner = left_col[:, :, -padding_size:, :].flip(2)\n",
    "    right_bottom_corner = right_col[:, :, -padding_size:, :].flip(2)\n",
    "\n",
    "    x_mirror_pad = torch.cat((torch.cat((left_up_corner, up_line, right_up_corner), 3), torch.cat((left_col, x, right_col), 3), torch.cat((left_bottom_corner, bottom_line, right_bottom_corner), 3)), 2)\n",
    "    return x_mirror_pad\n",
    "\n",
    "\n",
    "\n",
    "def normalize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    max_value = torch.max(x)\n",
    "    min_value = torch.min(x)\n",
    "    radius = max(max_value, abs(min_value))\n",
    "    x_centered_normalized = x_centered/radius\n",
    "    return x_centered_normalized, radius, mean_channels\n",
    "\n",
    "def standardize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    var = torch.sum(x_centered**2, (2, 3))/(x.size()[2]*x.size()[3])\n",
    "    x_standardized = x_centered / torch.sqrt(var.view(x_centered.size()[0], x_centered.size()[1], 1, 1))\n",
    "    return x_standardized, mean_channels, var\n",
    "    \n",
    "def denormalize_output(x, radius, mean_channels):\n",
    "    x_denormalized = x*radius\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_denormalized_centered = x_denormalized + mean_channels_images\n",
    "    return x_denormalized_centered\n",
    "\n",
    "def destandardize_output(x, mean_channels, var):\n",
    "    x_rescaled = x*torch.sqrt(var.view(x.size()[0], x.size()[1], 1, 1))\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_destandardized = x_rescaled + mean_channels_images\n",
    "    return x_destandardized\n",
    "\n",
    "\n",
    "\n",
    "def compute_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    return gsm\n",
    "\n",
    "\n",
    "def sum_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    #gsm_sum = (torch.log2(gsm)).sum()\n",
    "    gsm_sum = gsm.sum()\n",
    "    return gsm_sum\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    h = x.size(-1)\n",
    "    w = x.size(-2)\n",
    "    #print(\"input : \", input)\n",
    "    coeff = torch.sqrt(1.0 / (2 * np.pi * var))\n",
    "    #print(\"coeff : \", coeff)\n",
    "    x_resized = x.repeat((nScale, 1, 1, 1, 1))\n",
    "    #print(\"input : \", input_resized)\n",
    "    exponent = (-0.5*(x_resized ** 2)/var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w))\n",
    "    #print(\"exponent : \", exponent)\n",
    "    coeffs_resized = coeff.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    #print(\"coeffs : \", coeffs_resized)\n",
    "    gaussian = coeffs_resized * torch.exp(exponent)\n",
    "    #print(\"gaussian : \", gaussian)\n",
    "    phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    phi_gaussian = phi_resized*gaussian\n",
    "    sum_phi_gaussian = phi_gaussian.sum(dim=0)\n",
    "    #print(\"sum over scales : \", sum_phi_gaussian)\n",
    "    result = -torch.log2(sum_phi_gaussian).sum()\n",
    "\n",
    "    return result\n",
    "    \"\"\"\n",
    "    \n",
    "def compute_mask(nb_ones, dims):\n",
    "    mask = torch.zeros(dims)\n",
    "    indices = np.arange(nb_ones)\n",
    "    mask_flatten = mask.view(-1, 1, 1, 1)\n",
    "    mask_flatten[indices] = 1\n",
    "    mask_reshaped = mask_flatten.view(dims)\n",
    "    return mask_reshaped\n",
    "\n",
    "\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    integral_u = torch.trapz(gsm_sum, u)\n",
    "    #print(\"gsm sum : \", gsm_sum)\n",
    "    #print(\"integral over u : \", integral_u)\n",
    "    entropy = -torch.log2(integral_u)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def mean_bit_per_px(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = []\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    nb_bits = (-torch.log2(torch.clamp(integral_u, min=np.exp(-10**2), max=1))).sum()\n",
    "    if nb_bits.isnan():\n",
    "        print(\"nb bits is nan\")\n",
    "    if nb_bits < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"nb_bits negative : \", nb_bits)\n",
    "    return nb_bits/reduce(lambda x, y: x*y, list(x_quantized.size()))\n",
    "\n",
    "\n",
    "def entropy_dist(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = []\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    prob = torch.clamp(integral_u, min=np.exp(-10**2), max=1)\n",
    "    entropy = (-prob*torch.log2(prob)).sum()\n",
    "    if entropy.isnan():\n",
    "        print(\"entropy is nan\")\n",
    "    if entropy < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"entropy negative : \", entropy)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def distortion_pc(x, x_reconstructed, focal_length, skew, scaling_factor, image_size, principal_point):\n",
    "    # compute intrinsic matrix and its inverse\n",
    "    K = torch.tensor([[focal_length, skew, principal_point[0]], [ 0.0, focal_length, principal_point[1]], [0.0, 0.0, 1.0]])\n",
    "    K_ext = torch.eye(4)\n",
    "    K_ext[0:3:1, 0:3:1] = K\n",
    "    K_ext_inv = torch.inverse(K_ext).cuda()\n",
    "    \n",
    "    size_batch = x.size()[0]\n",
    "    total_mse = 0.0\n",
    "    for example in range(size_batch):\n",
    "        # compute x,y,z coords from u,v coords and gray-level value\n",
    "        z_reconstructed = x_reconstructed[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        z = x[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        bool_matrix = torch.logical_and(z_reconstructed != 0 , z != 0)\n",
    "            # for reconstructed depthmap\n",
    "        v_reconstructed = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v_reconstructed = v_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        u_reconstructed = (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u_reconstructed = u_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        z_reconstructed = z_reconstructed[bool_matrix].view(1, -1)\n",
    "        homogeneous_reconstructed_camera_points = torch.cat((u_reconstructed, v_reconstructed, torch.ones(1, u_reconstructed.size()[1]).cuda(), 1.0/z_reconstructed), 0)\n",
    "        homogeneous_reconstructed_coords = z_reconstructed*(torch.mm(K_ext_inv,homogeneous_reconstructed_camera_points))\n",
    "        coords_reconstructed = homogeneous_reconstructed_coords[0:3:1, :]\n",
    "            # for original depthmap\n",
    "        v = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v = v[bool_matrix].view(1, -1).cuda()\n",
    "        u =  (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u = u[bool_matrix].view(1, -1).cuda()    \n",
    "        z = z[bool_matrix].view(1, -1)\n",
    "        homogeneous_camera_points = torch.cat((u, v, torch.ones(1, u.size()[1]).cuda(), 1.0/z))\n",
    "        homogeneous_coords = z*(torch.mm(K_ext_inv,homogeneous_camera_points))\n",
    "        coords = homogeneous_coords[0:3:1, :]\n",
    "        \n",
    "        print(\" x original : \", torch.squeeze(z)[10]*scaling_factor)\n",
    "        print(\" original coords : \", coords[:, 10])\n",
    "        \n",
    "        # compute MSE on points\n",
    "        total_mse += torch.sum(torch.sqrt(torch.sum((coords_reconstructed - coords)**2, 0)))\n",
    "    return total_mse\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.05, 0.05).cuda()   \n",
    "    sum_log_gsm = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        sum_log_gsm[i] = (-torch.log2(gsm)).sum()\n",
    "    \n",
    "    entropy = torch.trapz(sum_log_gsm, u)\n",
    "    if entropy < 0:\n",
    "        print(\"negative entropy\")\n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def clip(x):\n",
    "    x_n = (x - torch.min(x))/(torch.max(x) - torch.min(x))\n",
    "    x_clipped = torch.round(255*x_n).float()\n",
    "    return x_clipped\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MyQuantization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "        \n",
    "        \n",
    "class MyClipping(torch.autograd.Function):\n",
    "  \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input).clamp(min=0, max=2**16-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "    \n",
    "class MyConv2d(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernal_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weights = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernal_size_number)).data.uniform_(0, 1)\n",
    "\n",
    "    def forward(self, x, x_segmented):\n",
    "        if x.is_cuda:\n",
    "            self.weights = self.weights.cuda()\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        result_seg = torch.zeros(\n",
    "            [x.shape[0] * self.out_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        \n",
    "        windows_depth = self.calculateWindows(x)\n",
    "        windows_seg = self.calculateWindows(x_segmented)\n",
    "        windows_seg[windows_seg < 1] = -1\n",
    "        windows_seg_centers = windows_seg[:, :, windows_seg.size()[2]//2].view(windows_seg.size()[0], windows_seg.size()[1], 1)\n",
    "        windows_seg = windows_seg * windows_seg_centers\n",
    "        windows_seg[windows_seg < 1] = 0 \n",
    "        windows_depth_seg = windows_depth * windows_seg\n",
    "        \n",
    "        # compute result\n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows_depth_seg[channel], self.weights[i_convNumber][channel].view(-1, 1))\n",
    "                xx = xx.view(-1, width, height)/torch.sum(windows_seg[channel], 1).view(-1, width, height)\n",
    "                result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "            result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] /= x.shape[1]\n",
    "\n",
    "        result = result.view(x.shape[0], self.out_channels, width, height)\n",
    " \n",
    "        # compute result_seg\n",
    "        windows_seg_seg = self.calculateWindows(x_segmented) * windows_seg\n",
    "        for i_convNumber in range(self.out_channels):\n",
    "            for channel in range(x.shape[1]):\n",
    "                xx = torch.matmul(windows_seg_seg[channel], self.weights[i_convNumber][channel].view(-1, 1))\n",
    "                xx = xx.view(-1, width, height)\n",
    "                result_seg[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "\n",
    "        result_seg = result_seg.view(x_segmented.shape[0], self.out_channels, width, height)\n",
    "        result_seg = torch.clamp(result_seg, min=0, max=1)\n",
    "        \n",
    "        return result, result_seg\n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernal_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entre : 128*128\n",
    "class LossyCompAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = MyConv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = MyConv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = MyConv2d(128, 96, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = MyConv2d(96, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = MyConv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = MyConv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = MyConv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = MyConv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = MyConv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = MyConv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 96))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 96))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(96).view(1, 96, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchNormed = nn.BatchNorm2d(96, momentum = False, affine=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, x_seg, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # get zero mask\n",
    "            #dilatation_mask = (x>0)\n",
    "                # removing black pixels\n",
    "            #x = black_pixels_removal_by_dilatation(x)\n",
    "            #if torch.any(x.isnan()):\n",
    "                #print(\"x after dilatation is nan\", x)\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            #x, radius, mean_channels = normalize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            x_seg = mirror_padding(x_seg, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x, x_seg = self.conv1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.conv2(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x_c1 = x.clone()\n",
    "            x_seg_c1 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"mean channels : \", mean_channels)\n",
    "                print(\"standard deviation : \", torch.sqrt(var))\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x, x_seg = self.resConv1_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv1_2(x, x_seg)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c1, min=0, max=1)\n",
    "            x_seg_c2 = x_seg\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weight gradients: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x, x_seg = self.resConv2_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv2_2(x, x_seg)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c2, min=0, max=1)\n",
    "            x_seg_c3 = x_seg\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "                # residual block 3\n",
    "            x, x_seg = self.resConv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.resConv3_2(x, x_seg)\n",
    "            x += x_c3\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c3, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "                # output block\n",
    "            x, x_seg = self.conv3(x, x_seg)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x, x_seg = self.subpix1(x, x_seg)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_seg = periodic_shuffling(x_seg, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            x_seg_c4 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                # residual block 1\n",
    "            x, x_seg = self.deconv1_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv1_2(x, x_seg)\n",
    "            x += x_c4\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c4, min=0, max=1)\n",
    "            x_c5 = x.clone()\n",
    "            x_seg_c5 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                   # residual block 2\n",
    "            x, x_seg = self.deconv2_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv2_2(x, x_seg)\n",
    "            x += x_c5\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c5, min=0, max=1)\n",
    "            x_c6 = x.clone()\n",
    "            x_seg_c6 = x_seg.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                   # residual block 3\n",
    "            x, x_seg = self.deconv3_1(x, x_seg)\n",
    "            x = F.relu(x)\n",
    "            x, x_seg = self.deconv3_2(x, x_seg)\n",
    "            x += x_c6\n",
    "            x_seg = torch.clamp(x_seg + x_seg_c6, min=0, max=1)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                    # subpixel 2\n",
    "            x, x_seg = self.subpix2(x, x_seg)\n",
    "            x = periodic_shuffling(x, 256//4)\n",
    "            x_seg = periodic_shuffling(x_seg, 256//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                    # subpixel 3\n",
    "            x, x_seg = self.subpix3(x, x_seg)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            #x = denormalize_output(x, radius, mean_channels)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "                    # replace black pixels\n",
    "           # x = x*dilatation_mask\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                x_seg = mirror_padding(x_seg, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x, x_seg))\n",
    "                x_seg = F.relu(self.conv1(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x = F.relu(self.conv2(x, x_seg))\n",
    "                x_seg = F.relu(self.conv2(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x, x_seg))\n",
    "                x = self.resConv1_2(x, x_seg)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x, x_seg))\n",
    "                x = self.resConv2_2(x, x_seg)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x, x_seg))\n",
    "                x = self.resConv3_2(x, x_seg)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x, x_seg)\n",
    "                x_seg = F.relu(self.conv3(x_seg.clone(), x_seg))\n",
    "                x_seg[x_seg > 0] = 1\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "            \n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            # batch normalization\n",
    "            x = self.batchNormed(x)\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x, x_seg)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_seg = self.subpix1(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                    print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x, x_seg))\n",
    "                x = self.deconv1_2(x, x_seg)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x, x_seg))\n",
    "                x = self.deconv2_2(x, x_seg)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x, x_seg))\n",
    "                x = self.deconv3_2(x, x_seg)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x, x_seg)\n",
    "                x = periodic_shuffling(x, 256//4)\n",
    "                x_seg = self.subpix2(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 256//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                    print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                    print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x, x_seg)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                x_seg = self.subpix3(x_seg.clone(), x_seg)\n",
    "                x_seg[x_seg > 0] = 1\n",
    "                x_seg = periodic_shuffling(x_seg, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                    print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                    print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entre : 128*128\n",
    "class LossyCompAutoencoder_bis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder_bis, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = nn.Conv2d(128, 32, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = nn.Conv2d(32, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = nn.Conv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = nn.Conv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 32))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(32).view(1, 32, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.resConv1_1(x))\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x = F.relu(self.resConv2_1(x))\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "                # residual block 3\n",
    "            x = F.relu(self.resConv3_1(x))\n",
    "            x = self.resConv3_2(x)\n",
    "            x += x_c3\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.deconv1_1(x))\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                   # residual block 2\n",
    "            x = F.relu(self.deconv2_1(x))\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                   # residual block 3\n",
    "            x = F.relu(self.deconv3_1(x))\n",
    "            x = self.deconv3_2(x)\n",
    "            x += x_c6\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = F.relu(periodic_shuffling(x, 256//4))\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "\n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = F.relu(periodic_shuffling(x, 256//4))\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossyCompAutoencoder(\n",
      "  (conv1): MyConv2d()\n",
      "  (conv2): MyConv2d()\n",
      "  (resConv1_1): MyConv2d()\n",
      "  (resConv1_2): MyConv2d()\n",
      "  (resConv2_1): MyConv2d()\n",
      "  (resConv2_2): MyConv2d()\n",
      "  (resConv3_1): MyConv2d()\n",
      "  (resConv3_2): MyConv2d()\n",
      "  (conv3): MyConv2d()\n",
      "  (subpix1): MyConv2d()\n",
      "  (deconv1_1): MyConv2d()\n",
      "  (deconv1_2): MyConv2d()\n",
      "  (deconv2_1): MyConv2d()\n",
      "  (deconv2_2): MyConv2d()\n",
      "  (deconv3_1): MyConv2d()\n",
      "  (deconv3_2): MyConv2d()\n",
      "  (subpix2): MyConv2d()\n",
      "  (subpix3): MyConv2d()\n",
      "  (batchNormed): BatchNorm2d(96, eps=1e-05, momentum=False, affine=False, track_running_stats=True)\n",
      ")\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# instanciate the model\n",
    "model = LossyCompAutoencoder()\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "ibatch :  0\n",
      "loss :  tensor(155559.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155569.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155565.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155570.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155566.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155568.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155562.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155567.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155562.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155570.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155563.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155563.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155559.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155569.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155562.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155563.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155560.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155558.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155567.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155560.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155558.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155561.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155558.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155562.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155561.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155560.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155564.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155556.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155557.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155555.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155562.884896\n",
      "epoch :  2\n",
      "ibatch :  0\n",
      "loss :  tensor(155564.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155552.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155565.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155556.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155551.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155561.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155555.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155557.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155554.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155564.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155553.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155564.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155555.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155561.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155552.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155554.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155552.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155550.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155555.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155550.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155554.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155553.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155552.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155550.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155561.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155571.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155551.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155553.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155544.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155564.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155556.444271\n",
      "epoch :  3\n",
      "ibatch :  0\n",
      "loss :  tensor(155555.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155551.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155544.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155552.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155555.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155554.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155559.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155557.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155551.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155543.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155550.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155550.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155544.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155549.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155545.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155547.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155547.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155545.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155543.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155546.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155550.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155546.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155544.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155556.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155552.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155544.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155543.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155543.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155560.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155553.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155549.684375\n",
      "epoch :  4\n",
      "ibatch :  0\n",
      "loss :  tensor(155539.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155545.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155539.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155552.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155545.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155549.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155541.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155551.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155553.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155541.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155554.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155548.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155545.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155545.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155542.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155542.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155540.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155538.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155543.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155539.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155534.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155545.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155538.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155539.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155541.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155538.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155532.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155541.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155537.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155539.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155543.026562\n",
      "epoch :  5\n",
      "ibatch :  0\n",
      "loss :  tensor(155539.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155534.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155541.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155535.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155535.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155532.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155536.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155539.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155539.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155544.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155536.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155535.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155539.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155535.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155539.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155543.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155545.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155538.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155539.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155536.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155543.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155529.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155532.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155536.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155534.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155534.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155532.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155527.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155536.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155534.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155536.965625\n",
      "epoch :  6\n",
      "ibatch :  0\n",
      "loss :  tensor(155548.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155533.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155532.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155527.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155538.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155536.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155530.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155534.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155529.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155528.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155526.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155532.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155530.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155521.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155532.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155531.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155531.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155527.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155524.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155525.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155521.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155532.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155524.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155523.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155523.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155529.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155526.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155529.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155543.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155521.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155530.077083\n",
      "epoch :  7\n",
      "ibatch :  0\n",
      "loss :  tensor(155521.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155531., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155524.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155519.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155529.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155529.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155525.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155524.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155521.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155523.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155527.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155517.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155518.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155525.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155522.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155524.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155522.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155517.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155518.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155531.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155515.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155518.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155531.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155524.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155530.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155524.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155519.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155528.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155525.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155518.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155523.873958\n",
      "epoch :  8\n",
      "ibatch :  0\n",
      "loss :  tensor(155523.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155513.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155517.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155519.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155516.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155527.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155514.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155531.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155519.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155511.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155519.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155516.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155517.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155512.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155518.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155517.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155513.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155520.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155518.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155513.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155517.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155520.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155514.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155518.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155514.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155520.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155512.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155512.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155514.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155511.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155517.326563\n",
      "epoch :  9\n",
      "ibatch :  0\n",
      "loss :  tensor(155509.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155530.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155518.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155508.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155515.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155508.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155514.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155508.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155508.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155513.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155510.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155508.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155513.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155509.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155506.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155507.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155508.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155511.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155509.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155510.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155508.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155511.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155510.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155507.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155520.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155506.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155504.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155511.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155506.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155504.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155510.702083\n",
      "epoch :  10\n",
      "ibatch :  0\n",
      "loss :  tensor(155503.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155511.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155506.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155511.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155509.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155510.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155504.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155511.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155502.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155508.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155514.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155500.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155508.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155504.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155501.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155510.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155501.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155500.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155504.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155504.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155503.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155500.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155501.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155499.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155507.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155496.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155502.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155500.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155494.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155494.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155504.353125\n",
      "epoch :  11\n",
      "ibatch :  0\n",
      "loss :  tensor(155499.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155502.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155496.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155499.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155500.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155499.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155506.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155494.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155496.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155495.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155495.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155495.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155489.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155498.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155500.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155496.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155494.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155501.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155494.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155504.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155493.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155502.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155497.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155491.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155495.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155500.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155491.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155493.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155495.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155496.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155497.244792\n",
      "epoch :  12\n",
      "ibatch :  0\n",
      "loss :  tensor(155489.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155491.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155492.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155494.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155492.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155491.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155501.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155488.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155491.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155499.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155488.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155488.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155489.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155486.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155488.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155502.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155496.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155490.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155483.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155491.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155486.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155490.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155491.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155483., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155497.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155499.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155488.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155485.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155491.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155482.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155491.184896\n",
      "epoch :  13\n",
      "ibatch :  0\n",
      "loss :  tensor(155487.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155493.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155482.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155482.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155488.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155485.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155481.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155493.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155481.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155481.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155478.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155497.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155495.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155482.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155488.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155481.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155484.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155491.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155481.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155483.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155489.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155474.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155491.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155481.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155486.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155482.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155477.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155489.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155477.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155472.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155484.838542\n",
      "epoch :  14\n",
      "ibatch :  0\n",
      "loss :  tensor(155489.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155478.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155477.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155475.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155475.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155482.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155490.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155488.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155473.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155484.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155478.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155477.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155475.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155475.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155480.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155473.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155478.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155480.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155478.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155476.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155474.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155473.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155474.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155476.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155483.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155480.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155472.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155479.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155467.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155475.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155478.317188\n",
      "epoch :  15\n",
      "ibatch :  0\n",
      "loss :  tensor(155480.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155470.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155480.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155474.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155469.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155471.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155471.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155477.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155474.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155480.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155477.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155474.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155473.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155470.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155471.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155473.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155466.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155471.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155472.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155471.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155466.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155468.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155470.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155476.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155465.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155466.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155462.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155468.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155468.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155459.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155471.538021\n",
      "epoch :  16\n",
      "ibatch :  0\n",
      "loss :  tensor(155461.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155476.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155467.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155469.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155474.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155465.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155465.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155471.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155467.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155461.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155458.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155464.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155462.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155464.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155468.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155460.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155466.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155463.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155463.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155465.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155456.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155473.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155460.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155465.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155463.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155465.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155463.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155460.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155465.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155463.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155465.288542\n",
      "epoch :  17\n",
      "ibatch :  0\n",
      "loss :  tensor(155468.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155456.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155455.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155466.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155466.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155460.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155461.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155464.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155458.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155458.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155458.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155472.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155453.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155453.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155458.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155457.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155461.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155459.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155467.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155455.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155453.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155461.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155450.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155452.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155457.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155453.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155455.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155455.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155451.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155455.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155458.743750\n",
      "epoch :  18\n",
      "ibatch :  0\n",
      "loss :  tensor(155450.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155451.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155453.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155451.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155451.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155449.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155451.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155447.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155446.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155454.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155452.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155463.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155457.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155465.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155454.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155456.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155454.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155449.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155449.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155455.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155448.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155453.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155445.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155451.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155444.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155451.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155449.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155453.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155442.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155448.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155451.865625\n",
      "epoch :  19\n",
      "ibatch :  0\n",
      "loss :  tensor(155444.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155442.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155452., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155445.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155443.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155445.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155449.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155454.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155438.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155440.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155460.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155444.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155444.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155449.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155452.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155449.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155443.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155445.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155441.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155455.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155442.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155443.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155447.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155438.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155451.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155437.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155441.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155440.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155443.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155441.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155445.738021\n",
      "epoch :  20\n",
      "ibatch :  0\n",
      "loss :  tensor(155439.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155439.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155451.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155439.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155442.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155440.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155438.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155446.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155446.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155435.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155442.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155442.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155442.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155439.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155439.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155432.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155439.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155445.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155430.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155443.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155436.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155436.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155435.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155438.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155443.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155442.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155437.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155432.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155431.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155428.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155439.319271\n",
      "epoch :  21\n",
      "ibatch :  0\n",
      "loss :  tensor(155432.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155439.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155439.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155436.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155430.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155434.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155440.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155432.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155432.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155435.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155428.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155433.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155432.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155430.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155439.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155426.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155428.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155436.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155431.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155430.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155436.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155436.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155425.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155429.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155429.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155431.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155439.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155426.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155424.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155435.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155432.876562\n",
      "epoch :  22\n",
      "ibatch :  0\n",
      "loss :  tensor(155429.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155434.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155425.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155429.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155424.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155423.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155428.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155431.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155423.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155423.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155430.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155419.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155435.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155424.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155431.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155421.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155420.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155426.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155421.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155418.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155423.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155430.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155429.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155423.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155421.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155427.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155425.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155430.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155430.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155417.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155426.136979\n",
      "epoch :  23\n",
      "ibatch :  0\n",
      "loss :  tensor(155414.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155419.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155419.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155419.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155424.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155430.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155414.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155415.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155418.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155413.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155421.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155432.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155424., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155417.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155426.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155414.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155420.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155417.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155420.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155414.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155415.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155412.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155415.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155414.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155412.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155435.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155414.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155412.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155419.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155418.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155419.128646\n",
      "epoch :  24\n",
      "ibatch :  0\n",
      "loss :  tensor(155423.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155416.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155416.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155418.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155415.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155418.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155413.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155408.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155413.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155415.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155414.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155411.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155410.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155412.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155412.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155408.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155412.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155410.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155410.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155408.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155410.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155410.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155412.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155410.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155419.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155405.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155409.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155408.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155419.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155414.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155413.114583\n",
      "epoch :  25\n",
      "ibatch :  0\n",
      "loss :  tensor(155409.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155404.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155411.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155408.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155405.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155419.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155412.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155415.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155409.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155419.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155410.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155413.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155402.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155408.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155400.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155403.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155410.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155406.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155404.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155409.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155407.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155401.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155401.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155399.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155398.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155403.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155404.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155404.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155401.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155402.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155406.991667\n",
      "epoch :  26\n",
      "ibatch :  0\n",
      "loss :  tensor(155401.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155397.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155398.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155396.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155412.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155396.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155401.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155410.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155403.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155401.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155402.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155394.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155396.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155394.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155405.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155406.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155400.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155403.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155391.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155395.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155400.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155402.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155407.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155400.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155396.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155396.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155401.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155393.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155398.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155391.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155400.026042\n",
      "epoch :  27\n",
      "ibatch :  0\n",
      "loss :  tensor(155391.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155397.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155394.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155401.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155393.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155398.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155390.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155394.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155399.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155402.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155390.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155398.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155394.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155389.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155399.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155387.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155395.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155390.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155391.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155392.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155388.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155385.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155390.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155387.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155384.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155396.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155391.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155400.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155391.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155399.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155393.613021\n",
      "epoch :  28\n",
      "ibatch :  0\n",
      "loss :  tensor(155395.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155389.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155399.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155389.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155384.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155384.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155387.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155383.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155381.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155384.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155390.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155388.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155387.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155385.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155385.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155386.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155379.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155386.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155385.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155387.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155388.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155381.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155384.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155386.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155382.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155381.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155382.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155395.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155386.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155383.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155386.509375\n",
      "epoch :  29\n",
      "ibatch :  0\n",
      "loss :  tensor(155383.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155380.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155384.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155386.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155374.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155379.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155379.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155388.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155381.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155384.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155384.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155382.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155375.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155381.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155383.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155376.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155377.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155377.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155381.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155376.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155385.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155381.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155377.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155379.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155369.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155377.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155374.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155373.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155387.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155373.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155379.967708\n",
      "epoch :  30\n",
      "ibatch :  0\n",
      "loss :  tensor(155381.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155373.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155379.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155378.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155376.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155369.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155381.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155379.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155374.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155377.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155369.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155375.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155374.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155373.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155377.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155366.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155370.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155368.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155374.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155370.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155369., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155370.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155367.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155384.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155374.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155366.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155375.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155372.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155370.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155367.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155373.842708\n",
      "epoch :  31\n",
      "ibatch :  0\n",
      "loss :  tensor(155366.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155375.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155365.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155371.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155368.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155368.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155365.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155366.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155374.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155374.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155361.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155370.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155368.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155363.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155367.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155370.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155367.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155368.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155367.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155363.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155366.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155357.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155365.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155366.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155363.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155372.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155362.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155364., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155370.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155366.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155367.371875\n",
      "epoch :  32\n",
      "ibatch :  0\n",
      "loss :  tensor(155366.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155364.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155356.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155360.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155364.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155355.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155362.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155353.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155366.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155370., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155359.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155361.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155364.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155365.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155361.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155365.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155357.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155355.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155354.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155358.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155360.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155358.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155354.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155356.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155355.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155360.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155355.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155362.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155365.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155358.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155360.390104\n",
      "epoch :  33\n",
      "ibatch :  0\n",
      "loss :  tensor(155352.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155362.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155365.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155356.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155359.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155352.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155355.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155354.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155355.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155356.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155359.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155350.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155354.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155349.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155350.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155354.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155351.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155358.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155355.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155351.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155351.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155358.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155355.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155349.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155349.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155346.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155356.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155353.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155348.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155360.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155354.537500\n",
      "epoch :  34\n",
      "ibatch :  0\n",
      "loss :  tensor(155352.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155344.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155350.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155351.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155344.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155359.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155345.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155347.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155346.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155350.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155354.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155347.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155354.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155344.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155341.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155347.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155346.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155345.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155353.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155345.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155346.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155353.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155344.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155346.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155345.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155345.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155341.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155345.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155338.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155345.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155347.579167\n",
      "epoch :  35\n",
      "ibatch :  0\n",
      "loss :  tensor(155346.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155340.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155344.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155340.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155343.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155351.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155342.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155341.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155339.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155352.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155340.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155337.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155343.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155344.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155336.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155347.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155344.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155335., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155344.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155335.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155335.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155349.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155340.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155336.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155345.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155341.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155335.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155339.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155338.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155332.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155341.522917\n",
      "epoch :  36\n",
      "ibatch :  0\n",
      "loss :  tensor(155333.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155332.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155332.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155335.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155338.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155337.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155334.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155329.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155344.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155326.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155340.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155333.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155336.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155326., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155333.218750, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibatch :  15\n",
      "loss :  tensor(155339.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155329.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155332.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155334.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155341.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155333.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155343.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155332.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155331.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155345.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155331.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155329.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155337.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155328.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155340.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155334.820833\n",
      "epoch :  37\n",
      "ibatch :  0\n",
      "loss :  tensor(155336.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155332.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155330.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155327.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155330.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155326.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155329.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155327.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155334.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155335.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155330.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155325.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155338.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155325.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155325.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155333.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155324.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155337.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155330.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155326.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155328.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155323.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155329.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155323.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155326.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155320.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155323.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155319.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155322.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155318.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155328.105208\n",
      "epoch :  38\n",
      "ibatch :  0\n",
      "loss :  tensor(155321.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155331.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155320.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155317.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155321.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155320.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155325.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155323.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155328.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155324.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155323.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155322.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155319.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155318., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155316.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155318.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155325.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155325.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155317.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155318.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155315.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155316.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155325.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155317.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155322.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155322.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155318.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155319.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155322.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155324.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155321.537500\n",
      "epoch :  39\n",
      "ibatch :  0\n",
      "loss :  tensor(155316.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155329.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155321.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155317.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155311.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155323.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155317.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155320.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155315.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155325.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155311., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155311.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155310.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155308.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155316.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155321.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155314.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155315.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155316.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155316.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155317., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155309.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155312.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155315.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155313.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155315.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155311.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155312.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155309.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155305.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155315.407812\n",
      "epoch :  40\n",
      "ibatch :  0\n",
      "loss :  tensor(155320.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155306.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155308.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155310.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155311.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155310.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155307.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155309., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155304.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155310.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155306.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155307.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155307.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155303.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155311.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155320.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155317.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155309.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155300.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155312.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155305.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155307.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155301.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155300.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155313.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155311.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155300.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155309.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155307.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155302.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155308.581771\n",
      "epoch :  41\n",
      "ibatch :  0\n",
      "loss :  tensor(155303.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155305.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155311.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155299.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155301.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155303.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155305.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155297.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155308.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155298.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155296.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155298.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155307.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155300.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155299.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155297.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155309.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155302.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155295.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155303.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155293.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155308.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155293.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155293.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155305.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155295.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155302.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155305.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155304.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155297.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155301.521875\n",
      "epoch :  42\n",
      "ibatch :  0\n",
      "loss :  tensor(155293.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155301.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155293.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155293.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155297.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155296.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155294.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155295.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155293.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155303.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155306.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155297.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155290.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155294., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155293.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155295.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155299.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155310.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155294.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155298.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155294.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155299.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155290.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155289.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155292.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155290.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155299.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155289.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155295.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155288.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155295.807292\n",
      "epoch :  43\n",
      "ibatch :  0\n",
      "loss :  tensor(155283.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155297.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155293.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155293.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155286.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155288.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155287.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155297.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155295.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155293.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155297.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155284.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155289.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155290.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155290.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155289.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155281.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155293.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155288.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155290.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155286.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155291.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155281.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155284.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155292.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155285.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155288.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155284.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155280.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155281.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155288.989063\n",
      "epoch :  44\n",
      "ibatch :  0\n",
      "loss :  tensor(155286.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155280.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155281.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155286.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155283.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155285.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155291.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155277.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155295.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155282.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155286.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155280.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155279.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155284.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155282.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155287.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155278.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155280.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155275.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155277.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155283.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155276.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155276.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155284.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155273.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155277.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155295.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155280.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155273.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155286.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155282.413542\n",
      "epoch :  45\n",
      "ibatch :  0\n",
      "loss :  tensor(155275.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155282.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155277., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155277.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155276.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155278.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155292.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155274.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155278.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155270.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155274.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155272.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155283.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155273.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155278., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155273.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155279.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155275.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155272.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155281.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155268.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155268.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155271.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155279.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155268.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155268.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155273.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155274.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155292.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155269.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155276.095833\n",
      "epoch :  46\n",
      "ibatch :  0\n",
      "loss :  tensor(155283.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155272.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155270.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155269.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155280.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155264.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155269.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155266.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155267.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155272.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155263.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155270.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155262.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155265.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155266.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155265.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155265.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155265.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155267.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155273.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155273., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155267.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155270.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155270.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155275.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155263.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155266., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155271.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155270.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155265.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155269.191146\n",
      "epoch :  47\n",
      "ibatch :  0\n",
      "loss :  tensor(155262.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155269.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155265.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155272.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155262.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155262.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155258.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155265.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155261.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155264.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155260.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155264.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155269.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155257., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155264.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155260.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155268.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155265.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155257., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155265.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155267.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155262.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155260.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155262.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155259.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155261.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155256.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155259.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155262.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155254.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155262.870833\n",
      "epoch :  48\n",
      "ibatch :  0\n",
      "loss :  tensor(155256.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155261.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155255.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155250.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155254.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155257.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155256.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155255.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155253.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155257.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155260.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155255.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155257.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155260.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155252.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155255.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155253.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155256.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155250.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155263.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155253., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155251.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155253.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155249.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155256.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155265.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155256.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155253.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155252.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155259.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155255.945312\n",
      "epoch :  49\n",
      "ibatch :  0\n",
      "loss :  tensor(155248.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155253.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155247.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155252.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155249.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155250.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155254.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155251.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155246.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155255.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155259.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155250., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155246.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155260.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155244.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155247.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155250.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155246.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155245.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155241.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155243.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155256., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155246.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155247.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155250.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155251.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155245.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155249.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155247.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155245.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155249.513021\n",
      "epoch :  50\n",
      "ibatch :  0\n",
      "loss :  tensor(155248.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155242.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155243.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155238.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155251.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155252.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155238.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155241.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155240.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155245.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155260.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155245.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155239.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155244.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155250.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155241.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155244.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155249.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155238.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155235.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155248.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155239.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155241.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155242.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155240.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155247.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155236.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155235.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155237.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155235.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155243.254167\n",
      "epoch :  51\n",
      "ibatch :  0\n",
      "loss :  tensor(155237.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155232.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155239.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155237.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155242.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155240.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155233.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155241.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155231.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155232.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155232.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155231.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155234.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155245.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155236.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155239.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155245.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155237.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155238.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155241.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155232.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155235.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155229.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155238.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155242.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155230.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155235.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155232.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155232.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155225.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155236.279688\n",
      "epoch :  52\n",
      "ibatch :  0\n",
      "loss :  tensor(155228.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155226.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155236.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155236.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155228.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155236.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155227.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155237.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155232.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155227.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155233.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155235.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155226.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155226.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155227.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155226.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155232.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155232.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155225.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155229.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155232.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155228.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155226.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155238.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155224.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155231.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155219.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155234.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155221.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155227.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155229.931250\n",
      "epoch :  53\n",
      "ibatch :  0\n",
      "loss :  tensor(155231.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155233.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155235.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155226.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155225.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155225.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155223.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155217.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155225.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155221.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155222.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155227.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155222.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155222.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155219.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155220.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155226.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155228.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155223.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155226.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155231.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155217.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155219.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155221.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155217.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155219.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155219.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155216.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155215.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155214.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155223.239583\n",
      "epoch :  54\n",
      "ibatch :  0\n",
      "loss :  tensor(155230.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155219.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155217.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155216.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155218.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155210.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155215.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155227.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155215.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155217.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155218., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155212.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155219.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155216.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155222.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155216.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155214.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155214.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155211.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155212.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155232.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155212.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155217.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155206.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155218.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155206.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155208.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155223.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155213.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155211.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155216.647917\n",
      "epoch :  55\n",
      "ibatch :  0\n",
      "loss :  tensor(155215.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155209.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155221.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155218.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155216.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155207.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155212.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155208.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155210.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155209.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155213.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155211.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155208.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155207.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155220.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155205.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155210.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155227.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155212.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155201.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155206.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155204.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155214.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155206.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155203.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155202.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155210.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155201., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155205.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155208., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155210.450521\n",
      "epoch :  56\n",
      "ibatch :  0\n",
      "loss :  tensor(155205.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155202., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155204.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155214.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155203.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155200.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155209., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155201.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155205.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155202.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155200.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155202.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155205.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155200.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155203.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155202.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155208.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155199.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155198.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155213.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155199.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155201.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155207.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155196.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155199.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155207.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155206.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155195.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155193.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155203.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155203.140104\n",
      "epoch :  57\n",
      "ibatch :  0\n",
      "loss :  tensor(155205.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155204.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155197.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155205.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155199.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155197.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155193.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155204.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155194.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155204.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155192.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155191.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155193.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155201.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155198.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155198.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155191.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155195.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155198.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155208.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155203.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155197.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155188.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155192.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155194.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155188.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155192.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155192.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155194.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155193.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155197.143229\n",
      "epoch :  58\n",
      "ibatch :  0\n",
      "loss :  tensor(155191.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155189.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155193.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155192.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155192.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155190.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155200.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155193.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155192.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155187.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155184.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155185.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155194.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155188.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155189.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155200.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155183.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155194.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155193.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155186.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155191.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155200.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155193.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155189.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155183.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155188.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155184.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155185.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155193.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155192.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155190.910938\n",
      "epoch :  59\n",
      "ibatch :  0\n",
      "loss :  tensor(155184.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155197.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155190.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155193.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155186.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155183.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155181.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155181.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155178.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155190.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155177.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155182.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155187.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155188.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155187., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155180.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155194.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155184.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155181.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155181.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155179.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155178.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155178.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155180.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155177.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155174.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155179.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155184.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155174.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155178.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155183.338542\n",
      "epoch :  60\n",
      "ibatch :  0\n",
      "loss :  tensor(155188.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155174.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155185.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155180.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155178., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155196.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155188.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155179.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155177.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155175.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155179.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155178.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155171.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155176.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155174.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155169.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155179.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155175.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155174.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155170.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155171.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155172.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155187.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155175.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155172.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155172.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155167.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155177.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155168.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155171.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155176.993229\n",
      "epoch :  61\n",
      "ibatch :  0\n",
      "loss :  tensor(155170.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155166.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155181.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155177.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155169.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155180.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155179.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155173.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155169.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155169.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155172.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155168.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155169.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155168.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155164.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155171.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155174.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155168.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155167.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155166.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155163.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155165.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155169.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155172.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155162.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155175.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155164.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155165.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155169.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155165.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155170.173958\n",
      "epoch :  62\n",
      "ibatch :  0\n",
      "loss :  tensor(155163.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155169.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155166.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155160.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155163.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155161.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155163.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155167.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155156.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155163., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155158.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155169.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155170.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155172.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155171.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155167.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155167.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155170.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155170.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155172.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155157.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155156.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155163.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155161.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155160.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155161.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155158.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155151.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155157.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155156.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155163.680208\n",
      "epoch :  63\n",
      "ibatch :  0\n",
      "loss :  tensor(155171.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155161.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155162.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155156.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155163.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155153.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155156.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155155.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155163.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155160.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155158.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155155.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155150.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155160.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155156.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155162.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155156.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155162.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155152.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155154.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155158.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155160.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155147.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155152.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155152.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155159.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155151.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155150.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155160.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155146.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155157.159896\n",
      "epoch :  64\n",
      "ibatch :  0\n",
      "loss :  tensor(155164.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155147.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155153.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155152.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155150.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155153., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155153.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155155.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155155.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155148.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155145.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155152.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155150.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155149.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155153.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155145.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155143.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155144.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155155.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155146.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155143.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155149.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155144.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155151.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155151.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155158.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155148.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155144.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155146.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155155.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155150.463021\n",
      "epoch :  65\n",
      "ibatch :  0\n",
      "loss :  tensor(155150.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155144.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155144.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155146.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155149.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155138.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155140.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155144.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155142.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155146.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155145.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155147.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155142.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155137.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155144.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155140.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155139.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155153.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155146.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155146.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155140.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155143.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155138.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155141.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155140.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155144.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155138.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155137.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155137.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155143.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155143.287500\n",
      "epoch :  66\n",
      "ibatch :  0\n",
      "loss :  tensor(155142.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155148.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155136.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155144.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155132., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155141.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155135.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155137.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155131.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155142.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155140.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155139.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155132.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155135.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155136.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155139.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155138.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155136.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155130.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155130.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155130.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155136.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155137.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155129.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155138.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155134.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155137.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155144.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155133.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155129.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155136.780729\n",
      "epoch :  67\n",
      "ibatch :  0\n",
      "loss :  tensor(155135.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155131.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155139.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155135.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155130.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155129.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155131.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155130.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155129.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155131.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155128.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155125.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155133.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155129.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155125.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155134.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155129.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155130.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155123.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155126.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155127.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155123.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155135.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155123.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155126.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155121.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155133.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155131.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155122.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155135.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155129.728125\n",
      "epoch :  68\n",
      "ibatch :  0\n",
      "loss :  tensor(155119.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155126.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155125.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155126.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155124.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155120.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155137.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155126.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155121.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155120.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155125.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155124.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155123.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155117.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155120.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155122.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155129.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155120.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155119.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155124.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155116.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155128.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155125.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155124.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155122.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155117.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155115.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155118.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155115.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155122.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155122.768750\n",
      "epoch :  69\n",
      "ibatch :  0\n",
      "loss :  tensor(155129.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155120.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155111.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155115.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155113.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155131.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155113.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155120., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155127.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155121.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155111.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155111.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155114.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155116.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155125.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155114.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155117.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155110.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155117.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155118., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155109.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155114.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155109.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155115.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155109.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155108.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155116.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155115.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155107.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155105.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155115.823437\n",
      "epoch :  70\n",
      "ibatch :  0\n",
      "loss :  tensor(155111.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155119.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155125.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155106.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155106.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155109.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155108.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155107.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155105.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155111.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155107.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155108.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155109.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155106.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155109.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155103.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155113.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155111.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155101.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155110.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155109.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155103.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155109.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155106.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155111.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155118.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155101.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155105.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155104.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155101.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155108.803646\n",
      "epoch :  71\n",
      "ibatch :  0\n",
      "loss :  tensor(155102.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155106.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155103.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155104.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155105.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155101.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155112.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155103.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155100.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155098.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155100.203125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155095.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155100.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155096.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155100.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155100.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155103.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155102.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155105.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155102.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155100.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155101.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155096.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155105.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155098., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155097.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155101.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155093.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155094.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155098.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155101.144271\n",
      "epoch :  72\n",
      "ibatch :  0\n",
      "loss :  tensor(155092.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155095.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155098.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155100.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155091.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155094.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155097.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155100.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155094., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155108.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155093.578125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155089.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155092.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155090.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155100.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155092.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155100.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155085.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155098.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155097.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155092.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155089.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155084.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155096.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155086.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155085.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155088.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155091.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155090.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155089.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155093.651042\n",
      "epoch :  73\n",
      "ibatch :  0\n",
      "loss :  tensor(155085.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155108.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155088., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155091.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155088.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155084.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155080.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155098.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155081.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155084.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155091.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155093.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155089.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155080.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155079.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155083.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155085.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155083.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155080.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155082.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155083.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155093.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155084.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155083.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155082.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155080.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155091.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155085.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155078.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155084.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155086.333854\n",
      "epoch :  74\n",
      "ibatch :  0\n",
      "loss :  tensor(155079.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155078.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155083.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155077.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155081.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155077.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155078.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155080.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155085.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155080.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155072.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155077.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155071.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155078.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155071.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155088.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155074.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155078.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155073.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155082.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155077.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155081.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155078.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155079.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155074.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155074.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155084.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155079.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155067.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155074.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155078.081250\n",
      "epoch :  75\n",
      "ibatch :  0\n",
      "loss :  tensor(155075.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155068.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155083.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155085.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155077.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155065.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155070.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155079.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155077.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155071.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155068.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155073.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155065.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155068.734375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155069., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155071.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155077.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155067.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155073.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155065.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155072.640625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155068.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155060.250000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155062.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155060.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155067.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155062.015625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155061.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155062.781250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155064.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155069.969792\n",
      "epoch :  76\n",
      "ibatch :  0\n",
      "loss :  tensor(155068.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155067.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155068.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155072.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155066.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155056.515625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155059.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155061.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155063.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155065.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155065.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155064.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155060.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155061.937500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155053.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155061.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155060.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155058.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155066.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155063.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155054.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155055.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155066.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155053.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155057.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155058.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155058.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155063.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155051.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155059.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155061.441146\n",
      "epoch :  77\n",
      "ibatch :  0\n",
      "loss :  tensor(155052.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155050.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155053.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155060.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155059.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155055.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155048.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155053.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155052.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155057.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155053.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155048.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155047.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155045.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155051.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155058.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155063.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155047.625000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155054.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155048.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155049.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155053.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155051.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155052.218750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155052.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155046.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155041.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155046.484375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155050.921875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155055.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155052.116667\n",
      "epoch :  78\n",
      "ibatch :  0\n",
      "loss :  tensor(155043.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155053., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155042.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155044.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155049.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155039.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155044.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155042.437500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155042.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155039.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155049.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155034.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155052.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155045.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155040.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155042.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(155039.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155035.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155042.796875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155044.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155039.828125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155045.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155034.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155033.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155041.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155043.875000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155039.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155037.718750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155059.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155039.296875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155042.799479\n",
      "epoch :  79\n",
      "ibatch :  0\n",
      "loss :  tensor(155039.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155042.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155034.531250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155037.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155037.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155030.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155036., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155041.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155041.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155037.906250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155028.968750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155029.187500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155040.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155026.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155032.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155026.421875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155027.593750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155038.468750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155026.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155030.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155031.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155032.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155026.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155030.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155025.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155026.546875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155021.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155032.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155026.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155033.062500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155032.379688\n",
      "epoch :  80\n",
      "ibatch :  0\n",
      "loss :  tensor(155022.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155030.234375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155040.406250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155029.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155030.312500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155025.265625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155019.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155020.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155019.375000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155016.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155017.140625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155029.562500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155034.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155015.812500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155021., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155022.656250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155016.109375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155027.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(155025.359375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n",
      "loss :  tensor(155023.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  20\n",
      "loss :  tensor(155021.890625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  21\n",
      "loss :  tensor(155017.687500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  22\n",
      "loss :  tensor(155014.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  23\n",
      "loss :  tensor(155028.843750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  24\n",
      "loss :  tensor(155017.171875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  25\n",
      "loss :  tensor(155009.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  26\n",
      "loss :  tensor(155010.859375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  27\n",
      "loss :  tensor(155011.328125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  28\n",
      "loss :  tensor(155009.078125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  29\n",
      "loss :  tensor(155021.953125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "running loss : 155021.640104\n",
      "epoch :  81\n",
      "ibatch :  0\n",
      "loss :  tensor(155009.609375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  1\n",
      "loss :  tensor(155011.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  2\n",
      "loss :  tensor(155007.500000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  3\n",
      "loss :  tensor(155019.281250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  4\n",
      "loss :  tensor(155014.046875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  5\n",
      "loss :  tensor(155015.390625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  6\n",
      "loss :  tensor(155019.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  7\n",
      "loss :  tensor(155009.750000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  8\n",
      "loss :  tensor(155010.703125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  9\n",
      "loss :  tensor(155015.343750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  10\n",
      "loss :  tensor(155005.125000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  11\n",
      "loss :  tensor(155016.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  12\n",
      "loss :  tensor(155006.093750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  13\n",
      "loss :  tensor(155004.765625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  14\n",
      "loss :  tensor(155006.671875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  15\n",
      "loss :  tensor(155010.156250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  16\n",
      "loss :  tensor(155007.984375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  17\n",
      "loss :  tensor(155002.031250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  18\n",
      "loss :  tensor(154999.453125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ibatch :  19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7be1ae0fa320>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mbatch_segs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m#batch_segs[batch_segs == 0] = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mdecoded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_quantized\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_segs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mloss_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistortion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\root\\anaconda3\\envs\\torch1.8.1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ee5a081184ca>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_seg, mask, return_xq, is_lambda)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeconv3_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeconv3_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_seg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mx_c6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mx_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_seg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx_seg_c6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\root\\anaconda3\\envs\\torch1.8.1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ded445c94da1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, x_segmented)\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindows_depth_seg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_convNumber\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindows_seg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_convNumber\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi_convNumber\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_convNumber\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi_convNumber\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transfert du model au gpu\n",
    "model.to(device)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "        \n",
    "# incremental update of coefficients        \n",
    "\n",
    "# define beta\n",
    "beta = 0.00001\n",
    "#beta = 1000\n",
    "# define threshold and loss_init\n",
    "threshold = 0.95\n",
    "loss_init = float(\"Inf\")\n",
    "nb_ones = 1\n",
    "iteration = 0\n",
    "mask=(compute_mask(1, (96, 16, 16)).unsqueeze(0)).cuda()\n",
    "dim_latent = 16*16*96\n",
    "output_flag = False\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 6150\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch : \", epoch)\n",
    "    \n",
    "    \"\"\"\n",
    "    if epoch==100:\n",
    "        # define a new learning rate and so a new optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \"\"\"\n",
    "        \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        print(\"ibatch : \", i_batch)\n",
    "        batch_images = data[0].to(device).float()\n",
    "        batch_masks = data[1].to(device).float()\n",
    "        batch_segs = data[2].to(device).float()\n",
    "        #batch_segs[batch_segs == 0] = 1\n",
    "        [decoded_images, x_quantized] = model(batch_images, batch_segs, mask, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        #loss_dist = distortion_pc(batch_images, decoded_images, 525.0, 0, 1000.0, (128, 128), (319.5, 239.5))\n",
    "        loss_bit = entropy_dist(x_quantized, model.phi, model.var)\n",
    "        #print(\" loss distortion : \", loss_dist)\n",
    "        #print(\"loss bit : \", loss_bit)\n",
    "        #loss = loss_dist + beta*loss_bit\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        print(\"loss : \", loss)\n",
    "        \n",
    "        # check the value of the loss to see if another coefficient can be enabled\n",
    "        if (loss.item() < loss_init*threshold or iteration > 5):\n",
    "            if (nb_ones<dim_latent):\n",
    "                nb_ones +=1\n",
    "                loss_init = loss.item()\n",
    "                iteration = 0\n",
    "                mask = (compute_mask(nb_ones, tuple(x_quantized.size()[1:])).unsqueeze(0)).cuda()\n",
    "            else:\n",
    "                output_flag = True\n",
    "                break\n",
    "            \n",
    "        loss.backward()\n",
    "        #print(\"input weights conv 1 gradient: \", model.conv1.weight.grad)\n",
    "        #print(\"input bias conv 1 gradient: \",  model.conv1.bias.grad)\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        iteration += 1\n",
    "\n",
    "    if output_flag:\n",
    "        break\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save(model.state_dict(), './model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_without_black_px.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_bis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-94555e69bfd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# transfert du model au gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel_bis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# general update of coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_bis' is not defined"
     ]
    }
   ],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "    \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "# transfert du model au gpu\n",
    "model_bis.to(device)\n",
    "\n",
    "# general update of coefficients    \n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(model_bis.parameters(), lr=0.00001)\n",
    "    # define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "    #Epochs\n",
    "n_epochs = 420\n",
    "beta = 0.00001\n",
    "nb_updates = 0\n",
    "learning_rate = 0.0001\n",
    "tau = 10000\n",
    "kappa = 0.8\n",
    "\n",
    "    # Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "          \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        learning_rate *= tau**kappa/(tau + nb_updates)**kappa\n",
    "        print(\"learning_ rate : \", learning_rate)\n",
    "        optimizer = torch.optim.Adam(model_bis.parameters(), lr=learning_rate)\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized] = model_bis(batch_images, 1, True, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        loss_bit = mean_bit_per_px(x_quantized, model_bis.phi, model_bis.var)\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(loss)\n",
    "            \n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        nb_updates += 1\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LossyCompAutoencoder_bis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c27b05dcba91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# The new network is trained each time we want to change the rate-distortion tradeoff beta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_bis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLossyCompAutoencoder_bis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel_bis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel_bis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LossyCompAutoencoder_bis' is not defined"
     ]
    }
   ],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "\n",
    "model_bis = LossyCompAutoencoder_bis()\n",
    "model_bis.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model_bis.train()\n",
    "model_bis.to(device)\n",
    "\n",
    "\n",
    "weights_model_bis = list(model_bis.parameters())\n",
    "print(\"nb parameters of the model : \", len(weights_model_bis))\n",
    "for (name_bis, parameter_bis), (name, parameter) in zip(model_bis.named_parameters(), model.named_parameters()) :\n",
    "    \"\"\"\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter_bis)\n",
    "        parameter_bis.data = torch.FloatTensor(1, 96, 1, 1).uniform_(0.9, 1.1)\n",
    "        print(\"lamb new parameter : \", parameter_bis)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name_bis)\n",
    "    print(\"size of user-defined parameters : \", parameter_bis.size())\n",
    "    parameter.data = parameter_bis.data\n",
    "\n",
    "for name_bis, parameter_bis in model_bis.named_parameters():\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"check lamb parameter : \", parameter_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9d7010c95117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweights_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m39\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mweights_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights_model_bis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"size of user-defined parameters : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_model_bis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights model: \"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mweights_model_bis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "weights_model = list(model.parameters())\n",
    "for i in range(0, 39, 1):\n",
    "    weights_model[i] = weights_model_bis[i]\n",
    "    print(\"size of user-defined parameters : \", weights_model_bis[i].size())\n",
    "    print(\"weights model: \" ,weights_model_bis[i])\n",
    "model.train()    \n",
    "#weights_model[39] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "#weights_model[40] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    \"\"\"\n",
    "    if name == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter)\n",
    "        parameter.data = torch.FloatTensor(1, 96, 1, 1).uniform_(1, 1)\n",
    "        print(\"lamb new parameter : \", parameter)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name)\n",
    "    print(\"size of user-defined parameters : \", parameter.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(41., device='cuda:0')\n",
      "min vec latent :  tensor(-20., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-15., device='cuda:0')\n",
      "max vec latent :  tensor(41., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(45., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(44., device='cuda:0')\n",
      "min vec latent :  tensor(-17., device='cuda:0')\n",
      "max vec latent :  tensor(44., device='cuda:0')\n",
      "min vec latent :  tensor(-15., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-20., device='cuda:0')\n",
      "max vec latent :  tensor(40., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-18., device='cuda:0')\n",
      "max vec latent :  tensor(44., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(40., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-15., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(43., device='cuda:0')\n",
      "min vec latent :  tensor(-17., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-17., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-20., device='cuda:0')\n",
      "max vec latent :  tensor(45., device='cuda:0')\n",
      "min vec latent :  tensor(-22., device='cuda:0')\n",
      "max vec latent :  tensor(45., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 48 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test set \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "#test_dataset = ImageDataset(root_dir='./data/kodac/', transform=transforms.Compose([RandomCrop(128), ToTensor()]))\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        if test_image.size(2)>test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "        \n",
    "        [reconstructed_image, vec_latent] = model(test_image, 1, True)\n",
    "        print(\"min vec latent : \", torch.min(vec_latent))\n",
    "        print(\"max vec latent : \", torch.max(vec_latent))\n",
    "        \n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(torch.squeeze(reconstructed_image.int().cpu()))\n",
    "        \n",
    "        \"\"\"\n",
    "        # We can set the number of bins with the `bins` kwarg\n",
    "        bins_list = [-6.5, -5.5, -4.5, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n",
    "        for k in range(96):\n",
    "            # plot histograms\n",
    "            fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "            ax.hist((vec_latent[:, k, :, :]).view(-1).cpu(), bins=bins_list)\n",
    "            plt.savefig(\"D:\\\\autoencoder_data\\\\histograms\\\\depthmap\\\\\" + \"img\" + str(i)+ \"hist\" + str(k) + \".png\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = model(test_image)\n",
    "        reconstructed_depthmap = np.squeeze(reconstructed_image.cpu()).numpy()\n",
    "        cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\reconstructed\\\\beta_1\\\\\" + \"img\" + str(i)+\".png\", reconstructed_depthmap.astype(np.uint16))\n",
    "        #save_image(reconstructed_depthmap, \"D:\\\\autoencoder_data\\\\depthmaps\\\\reconstructed\\\\beta_0001\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy :  tensor(2.709279, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.063918, device='cuda:0')\n",
      "psnr :  tensor(60.218197)\n",
      "entropy :  tensor(2.813324, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.219986, device='cuda:0')\n",
      "psnr :  tensor(55.619770)\n",
      "entropy :  tensor(2.705266, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.057899, device='cuda:0')\n",
      "psnr :  tensor(62.501282)\n",
      "entropy :  tensor(2.712790, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.069184, device='cuda:0')\n",
      "psnr :  tensor(57.227882)\n",
      "entropy :  tensor(2.755549, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.133324, device='cuda:0')\n",
      "psnr :  tensor(55.564781)\n",
      "entropy :  tensor(2.704483, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.056725, device='cuda:0')\n",
      "psnr :  tensor(62.117290)\n",
      "entropy :  tensor(2.801929, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.202894, device='cuda:0')\n",
      "psnr :  tensor(54.938484)\n",
      "entropy :  tensor(2.827868, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.241803, device='cuda:0')\n",
      "psnr :  tensor(53.801464)\n",
      "entropy :  tensor(2.731619, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.097428, device='cuda:0')\n",
      "psnr :  tensor(61.901928)\n",
      "entropy :  tensor(2.747969, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.121954, device='cuda:0')\n",
      "psnr :  tensor(59.501442)\n",
      "entropy :  tensor(2.756656, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.134984, device='cuda:0')\n",
      "psnr :  tensor(58.213383)\n",
      "entropy :  tensor(2.792858, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.189287, device='cuda:0')\n",
      "psnr :  tensor(55.602898)\n",
      "entropy :  tensor(2.668185, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.002278, device='cuda:0')\n",
      "psnr :  tensor(61.186325)\n",
      "entropy :  tensor(2.728021, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.092031, device='cuda:0')\n",
      "psnr :  tensor(62.043625)\n",
      "entropy :  tensor(2.684215, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.026322, device='cuda:0')\n",
      "psnr :  tensor(62.661434)\n",
      "entropy :  tensor(2.736803, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.105205, device='cuda:0')\n",
      "psnr :  tensor(60.333271)\n",
      "entropy :  tensor(2.844352, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.266529, device='cuda:0')\n",
      "psnr :  tensor(58.247154)\n",
      "entropy :  tensor(2.919651, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.379477, device='cuda:0')\n",
      "psnr :  tensor(51.732670)\n",
      "entropy :  tensor(2.778053, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.167079, device='cuda:0')\n",
      "psnr :  tensor(49.837807)\n",
      "entropy :  tensor(2.875844, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.313766, device='cuda:0')\n",
      "psnr :  tensor(46.581654)\n",
      "entropy :  tensor(2.833498, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.250247, device='cuda:0')\n",
      "psnr :  tensor(49.168198)\n",
      "entropy :  tensor(2.940254, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.410381, device='cuda:0')\n",
      "psnr :  tensor(56.956802)\n",
      "entropy :  tensor(2.899744, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.349617, device='cuda:0')\n",
      "psnr :  tensor(57.619205)\n",
      "entropy :  tensor(2.882065, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.323098, device='cuda:0')\n",
      "psnr :  tensor(51.335922)\n",
      "mean nb bits per pixel :  tensor(4.178142, device='cuda:0')\n",
      "psnr mean :  tensor(56.871372)\n"
     ]
    }
   ],
   "source": [
    "# compute PSNR for each image of the test set and its reconstruction\n",
    "\n",
    "def write_data(filepath , tensor_data):\n",
    "    batch, channel, h, w = tensor_data.size()\n",
    "    matrix = tensor_data.cpu().numpy()\n",
    "    file = open(filepath, \"w\")\n",
    "    for image in range(batch):\n",
    "        np.savetxt(file, matrix[image, :, :, :].reshape(channel*h, w), fmt ='%.0f')\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "psnr_sum = 0.0\n",
    "bit_rate_sum = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model(test_image, 1, True)\n",
    "        #write_data('.\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2\\\\latent_vect\\\\' + 'vec' + str(i) +'.txt', im_quantized)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(2)*test_image.size(3))\n",
    "        psnr_sum+= psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0)\n",
    "        bit_rate_sum += nbpp\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0))\n",
    "print( \"mean nb bits per pixel : \", bit_rate_sum/len(test_dataset))\n",
    "print(\"psnr mean : \", psnr_sum/len(test_dataset) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "       \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "# Load previous model\n",
    "model_prev = LossyCompAutoencoder()\n",
    "model_prev.load_state_dict(torch.load('./model_parameters/lossy_comp_params_with_rate_beta2_incremental_2.pth'))\n",
    "model_prev.eval()\n",
    "model_prev.to(device)\n",
    "\n",
    "\n",
    "# And run test \n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model_prev(test_image,1, True)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(1)*test_image.size(2)*test_image.size(3))\n",
    "        print(\"nb_symbols : \", nb_symbols)\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 255.0))\n",
    "    \n",
    "# And print figures\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        if test_image.size(2)<test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "        \n",
    "        reconstructed_image = model_prev(test_image, 1,  False)\n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.squeeze(reconstructed_image.int().cpu()).permute(1, 2, 0))\n",
    "        \n",
    "# And save figures\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = np.squeeze(model_prev(test_image).cpu())\n",
    "        print(reconstructed_image.type())\n",
    "        save_image(reconstructed_image, \".\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2_incremental_bis\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 90881.718424\n",
      "running loss : 80767.381022\n",
      "running loss : 81141.024251\n",
      "running loss : 82880.777669\n",
      "running loss : 82961.080566\n",
      "running loss : 80102.377279\n",
      "running loss : 78891.135254\n",
      "running loss : 81277.776530\n",
      "running loss : 85981.076497\n",
      "running loss : 79330.397786\n",
      "running loss : 81722.521647\n",
      "running loss : 81752.704427\n",
      "running loss : 80582.047445\n",
      "running loss : 79124.916667\n",
      "running loss : 77118.216146\n",
      "running loss : 80098.077311\n",
      "running loss : 80323.040690\n",
      "running loss : 77555.702474\n",
      "running loss : 81157.973796\n",
      "running loss : 83483.012695\n",
      "running loss : 81039.977051\n",
      "running loss : 80674.319661\n",
      "running loss : 78851.218424\n",
      "running loss : 79257.480469\n",
      "running loss : 81489.141927\n",
      "running loss : 78403.699056\n",
      "running loss : 77935.420654\n",
      "running loss : 75854.299316\n",
      "running loss : 77985.073893\n",
      "running loss : 79333.242025\n",
      "running loss : 78887.763672\n",
      "running loss : 76951.318522\n",
      "running loss : 74481.261230\n",
      "running loss : 76596.198730\n",
      "running loss : 80668.370605\n",
      "running loss : 78486.251790\n",
      "running loss : 79721.607747\n",
      "running loss : 81922.940918\n",
      "running loss : 74783.033040\n",
      "running loss : 76995.726074\n",
      "running loss : 75828.042806\n",
      "running loss : 75566.468831\n",
      "running loss : 77649.646891\n",
      "running loss : 75448.314860\n",
      "running loss : 80816.064128\n",
      "running loss : 78636.360352\n",
      "running loss : 79527.605794\n",
      "running loss : 79126.900879\n",
      "running loss : 78015.188965\n",
      "running loss : 74594.386637\n",
      "running loss : 77053.531820\n",
      "running loss : 75672.388346\n",
      "running loss : 76516.456706\n",
      "running loss : 74747.424805\n",
      "running loss : 78134.740234\n",
      "running loss : 76639.831543\n",
      "running loss : 78095.943848\n",
      "running loss : 76729.708659\n",
      "running loss : 75958.247721\n",
      "running loss : 76754.540853\n",
      "running loss : 74499.404785\n",
      "running loss : 76064.408447\n",
      "running loss : 74957.913411\n",
      "running loss : 74683.286133\n",
      "running loss : 76095.526449\n",
      "running loss : 74837.771810\n",
      "running loss : 78134.862467\n",
      "running loss : 73558.099772\n",
      "running loss : 73238.178223\n",
      "running loss : 73542.956380\n",
      "running loss : 71541.253743\n",
      "running loss : 76337.736003\n",
      "running loss : 77145.011230\n",
      "running loss : 82591.630697\n",
      "running loss : 74320.608724\n",
      "running loss : 75400.365723\n",
      "running loss : 76297.754720\n",
      "running loss : 73032.168457\n",
      "running loss : 72223.773275\n",
      "running loss : 77493.298340\n",
      "running loss : 74394.108236\n",
      "running loss : 73023.217041\n",
      "running loss : 76107.315267\n",
      "running loss : 74056.746582\n",
      "running loss : 74171.511719\n",
      "running loss : 74653.592773\n",
      "running loss : 73351.820312\n",
      "running loss : 78821.387044\n",
      "running loss : 75447.558757\n",
      "running loss : 72967.428385\n",
      "running loss : 72232.532227\n",
      "running loss : 71519.714762\n",
      "running loss : 72271.118571\n",
      "running loss : 75045.665039\n",
      "running loss : 71788.911540\n",
      "running loss : 71587.612956\n",
      "running loss : 75326.343424\n",
      "running loss : 72893.089844\n",
      "running loss : 72551.791504\n",
      "running loss : 72402.118164\n",
      "running loss : 72482.847819\n",
      "running loss : 70326.353027\n",
      "running loss : 72497.148438\n",
      "running loss : 71030.833252\n",
      "running loss : 72834.551188\n",
      "running loss : 69049.243978\n",
      "running loss : 72041.862793\n",
      "running loss : 73895.796224\n",
      "running loss : 71108.629639\n",
      "running loss : 70673.523600\n",
      "running loss : 70563.469645\n",
      "running loss : 71669.611491\n",
      "running loss : 71572.809570\n",
      "running loss : 73207.610026\n",
      "running loss : 70289.937988\n",
      "running loss : 73608.363932\n",
      "running loss : 69644.988770\n",
      "running loss : 70632.329346\n",
      "running loss : 72060.336344\n",
      "running loss : 74013.411377\n",
      "running loss : 72037.394206\n",
      "running loss : 70665.166341\n",
      "running loss : 68599.423014\n",
      "running loss : 69972.512858\n",
      "running loss : 71346.927246\n",
      "running loss : 71016.819010\n",
      "running loss : 66559.967692\n",
      "running loss : 71759.971680\n",
      "running loss : 68208.039225\n",
      "running loss : 68988.021159\n",
      "running loss : 72319.526855\n",
      "running loss : 66640.239339\n",
      "running loss : 71162.450195\n",
      "running loss : 73307.329427\n",
      "running loss : 72013.509603\n",
      "running loss : 67888.399577\n",
      "running loss : 67838.267253\n",
      "running loss : 67403.818359\n",
      "running loss : 66424.451579\n",
      "running loss : 68155.833171\n",
      "running loss : 66552.755371\n",
      "running loss : 69753.739746\n",
      "running loss : 68499.088460\n",
      "running loss : 67558.430339\n",
      "running loss : 68600.804281\n",
      "running loss : 69922.785156\n",
      "running loss : 66757.889730\n",
      "running loss : 68707.454753\n",
      "running loss : 68183.697591\n",
      "running loss : 65835.472493\n",
      "running loss : 67034.937663\n",
      "running loss : 68011.588867\n",
      "running loss : 66190.742350\n",
      "running loss : 72201.026693\n",
      "running loss : 67558.470052\n",
      "running loss : 67204.678711\n",
      "running loss : 66466.527100\n",
      "running loss : 67778.593913\n",
      "running loss : 66149.158854\n",
      "running loss : 66644.091553\n",
      "running loss : 66531.372070\n",
      "running loss : 68879.133464\n",
      "running loss : 65883.524577\n",
      "running loss : 65989.712565\n",
      "running loss : 64777.686930\n",
      "running loss : 66744.919027\n",
      "running loss : 64951.270345\n",
      "running loss : 69847.909180\n",
      "running loss : 68170.651530\n",
      "running loss : 63616.576986\n",
      "running loss : 65671.874756\n",
      "running loss : 64878.045410\n",
      "running loss : 65433.097087\n",
      "running loss : 65687.117350\n",
      "running loss : 64973.419434\n",
      "running loss : 65977.335938\n",
      "running loss : 64862.483317\n",
      "running loss : 65343.246257\n",
      "running loss : 67058.325602\n",
      "running loss : 67467.834310\n",
      "running loss : 64426.727458\n",
      "running loss : 68849.332194\n",
      "running loss : 66362.670898\n",
      "running loss : 63499.920329\n",
      "running loss : 66428.607096\n",
      "running loss : 67124.953776\n",
      "running loss : 65019.662923\n",
      "running loss : 65982.241211\n",
      "running loss : 63719.047689\n",
      "running loss : 64204.582194\n",
      "running loss : 63478.442708\n",
      "running loss : 64878.372762\n",
      "running loss : 65296.317139\n",
      "running loss : 63436.056803\n",
      "running loss : 63702.435954\n",
      "running loss : 64820.618083\n",
      "running loss : 65014.929769\n",
      "running loss : 64328.333496\n",
      "running loss : 64518.002686\n",
      "running loss : 64876.278076\n",
      "running loss : 64833.586426\n",
      "running loss : 64939.841471\n",
      "running loss : 64190.499349\n",
      "running loss : 64012.215088\n",
      "running loss : 61040.502197\n",
      "running loss : 62624.248454\n",
      "running loss : 62617.131836\n",
      "running loss : 62997.559082\n",
      "running loss : 62796.240641\n",
      "running loss : 65386.404297\n",
      "running loss : 68609.843750\n",
      "running loss : 63262.989665\n",
      "running loss : 62988.893962\n",
      "running loss : 63351.589111\n",
      "running loss : 63487.382894\n",
      "running loss : 66367.378581\n",
      "running loss : 63156.476888\n",
      "running loss : 62550.610352\n",
      "running loss : 65799.342611\n",
      "running loss : 63506.818115\n",
      "running loss : 62989.610189\n",
      "running loss : 61707.131022\n",
      "running loss : 62448.594889\n",
      "running loss : 62532.534831\n",
      "running loss : 62570.916667\n",
      "running loss : 63322.117188\n",
      "running loss : 63918.275391\n",
      "running loss : 66068.375326\n",
      "running loss : 62895.030762\n",
      "running loss : 62117.057861\n",
      "running loss : 62043.133626\n",
      "running loss : 61570.696777\n",
      "running loss : 64902.429688\n",
      "running loss : 63346.717041\n",
      "running loss : 62140.927083\n",
      "running loss : 61786.596354\n",
      "running loss : 63687.006266\n",
      "running loss : 60424.630615\n",
      "running loss : 61077.136312\n",
      "running loss : 60420.428792\n",
      "running loss : 61147.024740\n",
      "running loss : 61320.482666\n",
      "running loss : 61779.270508\n",
      "running loss : 62446.612386\n",
      "running loss : 60744.679688\n",
      "running loss : 64975.457031\n",
      "running loss : 60987.298096\n",
      "running loss : 62643.756917\n",
      "running loss : 61548.540039\n",
      "running loss : 59826.493164\n",
      "running loss : 61498.732747\n",
      "running loss : 60646.932699\n",
      "running loss : 59577.069255\n",
      "running loss : 60038.006673\n",
      "running loss : 62109.148600\n",
      "running loss : 59153.445231\n",
      "running loss : 58757.138184\n",
      "running loss : 61034.308431\n",
      "running loss : 59519.460612\n",
      "running loss : 59256.192546\n",
      "running loss : 58871.865234\n",
      "running loss : 61170.366292\n",
      "running loss : 61147.690348\n",
      "running loss : 59543.365397\n",
      "running loss : 60917.224854\n",
      "running loss : 60962.057292\n",
      "running loss : 59510.192546\n",
      "running loss : 59751.641846\n",
      "running loss : 60623.890299\n",
      "running loss : 57572.010905\n",
      "running loss : 59270.081380\n",
      "running loss : 57685.983358\n",
      "running loss : 58103.847575\n",
      "running loss : 59411.888184\n",
      "running loss : 60618.824870\n",
      "running loss : 57591.052327\n",
      "running loss : 60389.428467\n",
      "running loss : 59656.430827\n",
      "running loss : 58223.061849\n",
      "running loss : 56892.443359\n",
      "running loss : 59637.031576\n",
      "running loss : 56824.380127\n",
      "running loss : 57407.539795\n",
      "running loss : 59503.296224\n",
      "running loss : 59686.411865\n",
      "running loss : 56967.001302\n",
      "running loss : 57625.842855\n",
      "running loss : 58739.717041\n",
      "running loss : 59640.992676\n",
      "running loss : 56245.007812\n",
      "running loss : 56935.122396\n",
      "running loss : 56973.031494\n",
      "running loss : 57224.989176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 56330.414144\n",
      "running loss : 59217.237142\n",
      "running loss : 56974.153239\n",
      "running loss : 57221.444499\n",
      "running loss : 59008.677165\n",
      "running loss : 57119.617513\n",
      "running loss : 55305.609049\n",
      "running loss : 57673.525716\n",
      "running loss : 56229.853027\n",
      "running loss : 59675.997640\n",
      "running loss : 55790.359294\n",
      "running loss : 57322.470052\n",
      "running loss : 56106.875407\n",
      "running loss : 60872.270020\n",
      "running loss : 56010.167318\n",
      "running loss : 57382.687419\n",
      "running loss : 58415.753255\n",
      "running loss : 56892.956543\n",
      "running loss : 55117.039795\n",
      "running loss : 55409.036784\n",
      "running loss : 56686.735514\n",
      "running loss : 53838.631673\n",
      "running loss : 56824.481038\n",
      "running loss : 55640.784505\n",
      "running loss : 58160.728841\n",
      "running loss : 57626.534668\n",
      "running loss : 58323.094727\n",
      "running loss : 53920.407064\n",
      "running loss : 55168.505452\n",
      "running loss : 55127.270671\n",
      "running loss : 54881.633057\n",
      "running loss : 54211.694743\n",
      "running loss : 54501.730794\n",
      "running loss : 56947.436442\n",
      "running loss : 56152.468913\n",
      "running loss : 55613.786621\n",
      "running loss : 55751.203613\n",
      "running loss : 56320.617106\n",
      "running loss : 55469.636556\n",
      "running loss : 58944.215576\n",
      "running loss : 55151.181234\n",
      "running loss : 54784.307699\n",
      "running loss : 53933.224284\n",
      "running loss : 53722.103516\n",
      "running loss : 55022.672526\n",
      "running loss : 55112.404785\n",
      "running loss : 56338.940511\n",
      "running loss : 54212.332845\n",
      "running loss : 58285.784668\n",
      "running loss : 53848.245280\n",
      "running loss : 53618.175374\n",
      "running loss : 60219.200602\n",
      "running loss : 56293.832357\n",
      "running loss : 53725.534993\n",
      "running loss : 53165.360514\n",
      "running loss : 55134.238932\n",
      "running loss : 52998.208496\n",
      "running loss : 56099.515462\n",
      "running loss : 53734.485921\n",
      "running loss : 53228.031657\n",
      "running loss : 55388.830322\n",
      "running loss : 52846.800618\n",
      "running loss : 53111.263590\n",
      "running loss : 52534.631673\n",
      "running loss : 52801.039388\n",
      "running loss : 53641.130859\n",
      "running loss : 53582.857992\n",
      "running loss : 52399.858398\n",
      "running loss : 56638.229818\n",
      "running loss : 52979.531006\n",
      "running loss : 54271.182373\n",
      "running loss : 52913.174805\n",
      "running loss : 55493.278646\n",
      "running loss : 55342.254557\n",
      "running loss : 57968.763753\n",
      "running loss : 52760.720785\n",
      "running loss : 52599.817790\n",
      "running loss : 52585.163493\n",
      "running loss : 53373.625977\n",
      "running loss : 52442.100586\n",
      "running loss : 53891.315267\n",
      "running loss : 50547.944987\n",
      "running loss : 51661.942057\n",
      "running loss : 53970.030111\n",
      "running loss : 53986.946533\n",
      "running loss : 52550.642578\n",
      "running loss : 52293.479980\n",
      "running loss : 51929.335938\n",
      "running loss : 52587.779541\n",
      "running loss : 53295.073893\n",
      "running loss : 53681.895915\n",
      "running loss : 52964.613241\n",
      "running loss : 52228.323975\n",
      "running loss : 52873.979980\n",
      "running loss : 53320.415853\n",
      "running loss : 53423.901449\n",
      "running loss : 50707.938558\n",
      "running loss : 54250.009359\n",
      "running loss : 52043.060221\n",
      "running loss : 52570.794678\n",
      "running loss : 50841.515544\n",
      "running loss : 52653.602702\n",
      "running loss : 50633.415283\n",
      "running loss : 53026.918864\n",
      "running loss : 53763.368083\n",
      "running loss : 52898.638509\n",
      "running loss : 51744.891317\n",
      "running loss : 49307.681885\n",
      "running loss : 51092.994548\n",
      "running loss : 52226.794352\n",
      "running loss : 50100.759684\n",
      "running loss : 51270.523682\n",
      "running loss : 52272.166585\n",
      "running loss : 52876.253988\n",
      "running loss : 53089.156820\n",
      "running loss : 54536.442790\n",
      "running loss : 51735.277018\n",
      "running loss : 51833.324219\n",
      "running loss : 49773.806722\n",
      "running loss : 51504.332845\n",
      "running loss : 51773.265869\n",
      "running loss : 49764.508464\n",
      "running loss : 49225.610514\n",
      "running loss : 49575.354899\n",
      "running loss : 49936.283854\n",
      "running loss : 48218.027507\n",
      "running loss : 50748.913330\n",
      "running loss : 49324.360840\n",
      "running loss : 50701.173747\n",
      "running loss : 49849.854736\n",
      "running loss : 51438.284261\n",
      "running loss : 50187.976644\n",
      "running loss : 50680.334391\n",
      "running loss : 50702.315674\n",
      "running loss : 53446.914551\n",
      "running loss : 50960.229045\n",
      "running loss : 50184.661214\n",
      "running loss : 50461.203532\n",
      "running loss : 50077.770833\n",
      "running loss : 50244.267741\n",
      "running loss : 49254.449544\n",
      "running loss : 48433.246501\n",
      "running loss : 49656.638509\n",
      "running loss : 48824.016520\n",
      "running loss : 49076.353923\n",
      "running loss : 50104.597453\n",
      "running loss : 50265.500488\n",
      "running loss : 50032.427246\n",
      "running loss : 49624.999512\n",
      "running loss : 50925.185954\n",
      "running loss : 48534.235921\n",
      "running loss : 50662.632568\n",
      "running loss : 48193.469889\n",
      "running loss : 48474.606852\n",
      "running loss : 49389.584473\n",
      "running loss : 49591.266439\n",
      "running loss : 51075.706055\n",
      "running loss : 47004.641195\n",
      "running loss : 49832.155762\n",
      "running loss : 48841.529948\n",
      "running loss : 48364.793050\n",
      "running loss : 49087.891032\n",
      "running loss : 49919.277507\n",
      "running loss : 50339.088623\n",
      "running loss : 48497.556234\n",
      "running loss : 48887.618652\n",
      "running loss : 49891.339193\n",
      "running loss : 50234.233073\n",
      "running loss : 49169.051351\n",
      "running loss : 47015.986491\n",
      "running loss : 48219.990153\n",
      "running loss : 47937.049316\n",
      "running loss : 47478.959554\n",
      "running loss : 47636.843750\n",
      "running loss : 48866.298258\n",
      "running loss : 47756.621826\n",
      "running loss : 48210.740072\n",
      "running loss : 47783.646322\n",
      "running loss : 48204.722738\n",
      "running loss : 47461.079508\n",
      "running loss : 49181.149658\n",
      "running loss : 49097.230957\n",
      "running loss : 50193.783366\n",
      "running loss : 49531.762288\n",
      "running loss : 50378.979818\n",
      "running loss : 48356.587484\n",
      "running loss : 48787.566243\n",
      "running loss : 48377.106934\n",
      "running loss : 48516.516927\n",
      "running loss : 46364.435872\n",
      "running loss : 47041.051025\n",
      "running loss : 47239.184001\n",
      "running loss : 46649.845540\n",
      "running loss : 45625.941976\n",
      "running loss : 48156.247965\n",
      "running loss : 46817.323812\n",
      "running loss : 46268.982544\n",
      "running loss : 45899.153646\n",
      "running loss : 46552.740560\n",
      "running loss : 47220.430013\n",
      "running loss : 47627.274089\n",
      "running loss : 47222.320475\n",
      "running loss : 45934.450358\n",
      "running loss : 47653.203451\n",
      "running loss : 47547.944092\n",
      "running loss : 47703.819173\n",
      "running loss : 46692.880778\n",
      "running loss : 45628.091960\n",
      "running loss : 45902.782715\n",
      "running loss : 46425.142415\n",
      "running loss : 45972.709961\n",
      "running loss : 46971.066650\n",
      "running loss : 46530.842041\n",
      "running loss : 44901.349935\n",
      "running loss : 45541.446289\n",
      "running loss : 44867.612305\n",
      "running loss : 47857.843750\n",
      "running loss : 46523.240641\n",
      "running loss : 47989.493245\n",
      "running loss : 48226.034831\n",
      "running loss : 49315.416585\n",
      "running loss : 46795.908040\n",
      "running loss : 46138.067424\n",
      "running loss : 47623.291016\n",
      "running loss : 46836.219076\n",
      "running loss : 45712.408285\n",
      "running loss : 46073.073893\n",
      "running loss : 47835.127360\n",
      "running loss : 44855.554850\n",
      "running loss : 44278.727946\n",
      "running loss : 44724.735758\n",
      "running loss : 44922.763590\n",
      "running loss : 47036.272542\n",
      "running loss : 45472.129232\n",
      "running loss : 44592.049967\n",
      "running loss : 46900.262044\n",
      "running loss : 45040.774577\n",
      "running loss : 44863.964193\n",
      "running loss : 44892.395020\n",
      "running loss : 50270.711344\n",
      "running loss : 47854.146647\n",
      "running loss : 45389.122803\n",
      "running loss : 44142.645589\n",
      "running loss : 46900.283854\n",
      "running loss : 45170.434408\n",
      "running loss : 44071.296387\n",
      "running loss : 47352.829834\n",
      "running loss : 44197.769775\n",
      "running loss : 47417.472331\n",
      "running loss : 47880.432129\n",
      "running loss : 45117.166829\n",
      "running loss : 43656.764567\n",
      "running loss : 44177.691569\n",
      "running loss : 44232.839600\n",
      "running loss : 48167.512044\n",
      "running loss : 46936.025635\n",
      "running loss : 46363.481201\n",
      "running loss : 45062.203451\n",
      "running loss : 44512.832357\n",
      "running loss : 44358.731445\n",
      "running loss : 44597.669637\n",
      "running loss : 44642.365641\n",
      "running loss : 44932.551676\n",
      "running loss : 46019.332682\n",
      "running loss : 46495.985189\n",
      "running loss : 44827.285238\n",
      "running loss : 45363.432048\n",
      "running loss : 44987.661865\n",
      "running loss : 44964.790446\n",
      "running loss : 47934.064046\n",
      "running loss : 44326.019531\n",
      "running loss : 44429.802327\n",
      "running loss : 43404.033936\n",
      "running loss : 42733.738118\n",
      "running loss : 42979.190999\n",
      "running loss : 43090.233480\n",
      "running loss : 42178.195231\n",
      "running loss : 44282.277262\n",
      "running loss : 43237.059408\n",
      "running loss : 43080.399984\n",
      "running loss : 44342.421956\n",
      "running loss : 45713.942220\n",
      "running loss : 45192.666016\n",
      "running loss : 44188.002686\n",
      "running loss : 43359.221354\n",
      "running loss : 42526.843750\n",
      "running loss : 42504.014079\n",
      "running loss : 44542.493408\n",
      "running loss : 43413.537598\n",
      "running loss : 42975.689697\n",
      "running loss : 41882.025798\n",
      "running loss : 45173.108561\n",
      "running loss : 43113.059163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 42049.697184\n",
      "running loss : 43533.153076\n",
      "running loss : 44885.608317\n",
      "running loss : 41994.284668\n",
      "running loss : 42707.935465\n",
      "running loss : 43617.561401\n",
      "running loss : 43501.439535\n",
      "running loss : 45306.684977\n",
      "running loss : 42371.569499\n",
      "running loss : 42649.004028\n",
      "running loss : 42970.214600\n",
      "running loss : 43877.583211\n",
      "running loss : 45055.379883\n",
      "running loss : 47416.100423\n",
      "running loss : 44013.671549\n",
      "running loss : 41930.735921\n",
      "running loss : 42926.273356\n",
      "running loss : 42976.764323\n",
      "running loss : 42344.595866\n",
      "running loss : 45141.950684\n",
      "running loss : 41748.728271\n",
      "running loss : 40846.569661\n",
      "running loss : 40846.616211\n",
      "running loss : 42071.263021\n",
      "running loss : 42872.596273\n",
      "running loss : 41079.228597\n",
      "running loss : 43009.363444\n",
      "running loss : 41749.715007\n",
      "running loss : 43407.815430\n",
      "running loss : 41383.585124\n",
      "running loss : 42660.203613\n",
      "running loss : 39532.865723\n",
      "running loss : 42471.631836\n",
      "running loss : 41117.482422\n",
      "running loss : 42307.607259\n",
      "running loss : 43086.586670\n",
      "running loss : 41912.858398\n",
      "running loss : 40667.849447\n",
      "running loss : 40378.923360\n",
      "running loss : 42101.007955\n",
      "running loss : 43457.886882\n",
      "running loss : 42638.267253\n",
      "running loss : 40191.221517\n",
      "running loss : 42054.164632\n",
      "running loss : 42710.465576\n",
      "running loss : 41776.853027\n",
      "running loss : 40840.724528\n",
      "running loss : 41981.012533\n",
      "running loss : 41827.223470\n",
      "running loss : 42626.150553\n",
      "running loss : 41102.608724\n",
      "running loss : 42965.730469\n",
      "running loss : 40813.096680\n",
      "running loss : 42967.378255\n",
      "running loss : 40754.357422\n",
      "running loss : 43311.801514\n",
      "running loss : 43746.010010\n",
      "running loss : 45314.289225\n",
      "running loss : 44185.326742\n",
      "running loss : 41368.289469\n",
      "running loss : 42592.260905\n",
      "running loss : 42177.947917\n",
      "running loss : 39440.786865\n",
      "running loss : 41175.631348\n",
      "running loss : 40673.101237\n",
      "running loss : 41260.341309\n",
      "running loss : 39842.173096\n",
      "running loss : 39610.854655\n",
      "running loss : 40011.491252\n",
      "running loss : 41294.312744\n",
      "running loss : 40743.686686\n",
      "running loss : 40808.694580\n",
      "running loss : 40922.118083\n",
      "running loss : 41300.705566\n",
      "running loss : 39272.987427\n",
      "running loss : 39944.322591\n",
      "running loss : 40927.354818\n",
      "running loss : 41713.333903\n",
      "running loss : 41205.771159\n",
      "running loss : 40223.961263\n",
      "running loss : 40055.026204\n",
      "running loss : 42504.890137\n",
      "running loss : 38472.325480\n",
      "running loss : 39715.286133\n",
      "running loss : 40104.106201\n",
      "running loss : 39906.485270\n",
      "running loss : 39945.331217\n",
      "running loss : 40454.727132\n",
      "running loss : 40127.358480\n",
      "running loss : 40601.440552\n",
      "running loss : 39349.564697\n",
      "running loss : 39965.462565\n",
      "running loss : 39061.080078\n",
      "running loss : 39247.757243\n",
      "running loss : 40323.256266\n",
      "running loss : 39330.027995\n",
      "running loss : 40946.892822\n",
      "running loss : 39579.459066\n",
      "running loss : 40141.127523\n",
      "running loss : 37958.996094\n",
      "running loss : 38565.373617\n",
      "running loss : 39442.689250\n",
      "running loss : 37960.278198\n",
      "running loss : 40197.711995\n",
      "running loss : 38318.272054\n",
      "running loss : 38624.005941\n",
      "running loss : 39791.647135\n",
      "running loss : 40296.829346\n",
      "running loss : 38651.548828\n",
      "running loss : 39517.404378\n",
      "running loss : 39225.887451\n",
      "running loss : 40284.003174\n",
      "running loss : 39523.394084\n",
      "running loss : 39864.496908\n",
      "running loss : 40773.883952\n",
      "running loss : 40279.895996\n",
      "running loss : 38176.749430\n",
      "running loss : 39640.435221\n",
      "running loss : 40695.060954\n",
      "running loss : 38564.190186\n",
      "running loss : 38240.065918\n",
      "running loss : 39776.977295\n",
      "running loss : 39240.485107\n",
      "running loss : 39634.348633\n",
      "running loss : 39527.217855\n",
      "running loss : 38991.980387\n",
      "running loss : 40660.052409\n",
      "running loss : 39328.340251\n",
      "running loss : 37052.416667\n",
      "running loss : 37462.157878\n",
      "running loss : 36517.768880\n",
      "running loss : 37817.378174\n",
      "running loss : 37819.425456\n",
      "running loss : 39793.552816\n",
      "running loss : 39935.727580\n",
      "running loss : 37920.915934\n",
      "running loss : 39468.699219\n",
      "running loss : 40378.190267\n",
      "running loss : 42313.413981\n",
      "running loss : 41549.432536\n",
      "running loss : 40742.347575\n",
      "running loss : 38147.835856\n",
      "running loss : 37041.001465\n",
      "running loss : 38509.685872\n",
      "running loss : 39745.657308\n",
      "running loss : 39248.717041\n",
      "running loss : 37695.092529\n",
      "running loss : 37377.616374\n",
      "running loss : 37953.819417\n",
      "running loss : 36682.688721\n",
      "running loss : 36722.831787\n",
      "running loss : 37717.070312\n",
      "running loss : 37457.493571\n",
      "running loss : 37760.280558\n",
      "running loss : 38268.555990\n",
      "running loss : 39532.212484\n",
      "running loss : 38873.700765\n",
      "running loss : 37996.628255\n",
      "running loss : 36304.407104\n",
      "running loss : 35136.636841\n",
      "running loss : 36975.792847\n",
      "running loss : 36151.441610\n",
      "running loss : 37569.448730\n",
      "running loss : 37302.001546\n",
      "running loss : 36688.678345\n",
      "running loss : 36615.930908\n",
      "running loss : 36540.654948\n",
      "running loss : 36370.960327\n",
      "running loss : 36948.654378\n",
      "running loss : 36589.127238\n",
      "running loss : 35811.934977\n",
      "running loss : 36479.235596\n",
      "running loss : 37382.861694\n",
      "running loss : 36323.645264\n",
      "running loss : 37198.983073\n",
      "running loss : 36736.359049\n",
      "running loss : 36496.441162\n",
      "running loss : 36162.318237\n",
      "running loss : 37526.738851\n",
      "running loss : 37927.038574\n",
      "running loss : 36195.593913\n",
      "running loss : 35435.948975\n",
      "running loss : 36975.531901\n",
      "running loss : 37372.103190\n",
      "running loss : 35218.944051\n",
      "running loss : 37129.270345\n",
      "running loss : 36393.042765\n",
      "running loss : 36152.630452\n",
      "running loss : 36788.343424\n",
      "running loss : 37108.506755\n",
      "running loss : 37676.685059\n",
      "running loss : 36886.631999\n",
      "running loss : 36423.447673\n",
      "running loss : 35939.713257\n",
      "running loss : 37843.852376\n",
      "running loss : 36102.163167\n",
      "running loss : 36574.401855\n",
      "running loss : 38166.099121\n",
      "running loss : 36151.078857\n",
      "running loss : 37152.074951\n",
      "running loss : 38201.656169\n",
      "running loss : 35700.944499\n",
      "running loss : 37860.992025\n",
      "running loss : 36248.929606\n",
      "running loss : 36788.438558\n",
      "running loss : 36701.525553\n",
      "running loss : 36676.227783\n",
      "running loss : 35208.483602\n",
      "running loss : 36152.297363\n",
      "running loss : 35524.565104\n",
      "running loss : 35788.688883\n",
      "running loss : 36025.366781\n",
      "running loss : 35683.189372\n",
      "running loss : 35550.561117\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.01, 0.01).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    entropy = torch.trapz(gsm_sum, u)\n",
    "    \n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "# Load incremental model\n",
    "incremental_model = LossyCompAutoencoder()\n",
    "incremental_model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta1_without_bitrate.pth'))\n",
    "incremental_model.train()\n",
    "incremental_model.to(device)\n",
    "\n",
    "# train again the model starting form incremental-learned weights\n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(incremental_model.parameters(), lr=0.00001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "\n",
    "\n",
    "# define beta\n",
    "beta = 1.0\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 800\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "           \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized] = incremental_model(batch_images, 1, True, False)\n",
    "        optimizer.zero_grad()\n",
    "        #entropy = entropy_rate(x_quantized, incremental_model.phi, incremental_model.var)\n",
    "        #print(\"entropy : \", entropy)\n",
    "        dist = distortion(decoded_images, batch_images)\n",
    "        #print(\"distortion : \", dist)\n",
    "        #loss = beta * dist + entropy\n",
    "        loss = beta*dist\n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from range_coder import RangeEncoder, RangeDecoder, prob_to_cum_freq\n",
    "import os\n",
    "\n",
    "# Load previous model\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_params_with_rate_beta0005_incremental.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "nb_bits = 0.0\n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():  \n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "            # compute symbol probabilities\n",
    "        min_val = data_comp.min()\n",
    "        if min_val <0:\n",
    "            data_comp -= min_val\n",
    "            min_val = 0\n",
    "        max_val = data_comp.max()\n",
    "        nb_bins = max_val - min_val + 1\n",
    "        hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "        prob = hist/hist.sum()\n",
    "        #print(\"data comp : \", data_comp)\n",
    "        #print(prob)\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(torch.nonzero(prob)) != len(prob):\n",
    "            zero_indices = ((prob == 0).nonzero())\n",
    "            for j in reversed(range(0, len(zero_indices), 1)):\n",
    "                data_comp[data_comp > int(zero_indices[j])+min_val] -=1\n",
    "            min_val = data_comp.min()\n",
    "            max_val = data_comp.max()\n",
    "            nb_bins = max_val - min_val + 1\n",
    "            hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "            prob = hist/hist.sum()\n",
    "            print(min_val)\n",
    "            print(max_val)\n",
    "            print(\"data comp : \", data_comp)\n",
    "            print(prob)\n",
    "         \"\"\" \n",
    "            \n",
    "            # convert probabilities to cumulative integer frequency table\n",
    "        #cumFreq = prob_to_cum_freq(torch.clamp(prob, min=np.finfo(np.float32).eps).cpu(), resolution=128)\n",
    "        cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "        #print(cumFreq)\n",
    "        \n",
    "        # encode data\n",
    "        filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(i) + \".bin\"\n",
    "        encoder = RangeEncoder(filepath_to_write)\n",
    "        #print(torch.flatten(data_comp).cpu().tolist())\n",
    "        encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "        encoder.close()\n",
    "        \n",
    "        \n",
    "        file_size = os.path.getsize(filepath_to_write)*8 #number of bits in the file\n",
    "        print(file_size)\n",
    "        nb_bits += file_size\n",
    "        \n",
    "    nb_bits_per_image = nb_bits/len(test_dataset)\n",
    "    print(nb_bits_per_image)\n",
    "    nb_bits_per_pixel = nb_bits_per_image/(512*768)\n",
    "    print(nb_bits_per_pixel)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "    test_image = test_dataset[1].unsqueeze(0).to(device).float()\n",
    "    [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "        # compute symbol probabilities\n",
    "    min_val = data_comp.min()\n",
    "    print(min_val)\n",
    "    print(data_comp.max())\n",
    "    if min_val <0:\n",
    "        data_comp -= min_val\n",
    "        min_val = 0\n",
    "    max_val = data_comp.max()\n",
    "    \n",
    "    print(max_val)\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    print(nb_bins)\n",
    "    hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    prob = hist/hist.sum()\n",
    "    print(prob)\n",
    "         # convert probabilities to cumulative integer frequency table\n",
    "    cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "    #print(cumFreq)\n",
    "\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    \n",
    "    \n",
    "        # encode data\n",
    "    filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(1) + \".bin\"\n",
    "    encoder = RangeEncoder(filepath_to_write)\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "    encoder.close()   \n",
    "\"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\ni = 0\\ntest_image = test_dataset[i].unsqueeze(0).float()\\n\\n#test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\\ntest_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\\nax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\\nplt.imshow(torch.squeeze(test_image_without_black_px))\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "#fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_image = test_dataset[i].unsqueeze(0).float().cuda()\n",
    "\n",
    "    #test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "    test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "    cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\dilated\\\\\" + \"img\" + str(i)+\".png\", np.squeeze(test_image_without_black_px.cpu().numpy()).astype(np.uint16))\n",
    "    #ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "    #plt.imshow(torch.squeeze(test_image_without_black_px).cpu())\n",
    "\n",
    "\"\"\"    \n",
    "i = 0\n",
    "test_image = test_dataset[i].unsqueeze(0).float()\n",
    "\n",
    "#test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "plt.imshow(torch.squeeze(test_image_without_black_px))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
