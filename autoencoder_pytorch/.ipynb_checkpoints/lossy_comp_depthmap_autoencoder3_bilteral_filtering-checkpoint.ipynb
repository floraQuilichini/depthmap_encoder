{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy import signal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "torch.set_printoptions(precision=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([18, 1, 128, 128])\n",
      "1 torch.Size([18, 1, 128, 128])\n",
      "2 torch.Size([18, 1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAACcCAYAAADcS3gSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29ebClyVUn9su737ev9V7t1VXdVd1T6kVStNTQRmxi2ESMEBYjOwgYhsGGsQeP8BBhiAmDHbZnTGDAMpIIBhwY5BGy0CBGngBphNBIvbhXdXVXdXV37durt293Xz//cd/JOt+5J7/lvlcl9ej7Rdy4935L5sntlydPnsw0nuchQYIECRLcG6S+2QIkSJAgwbcTEtJNkCBBgnuIhHQTJEiQ4B4iId0ECRIkuIdISDdBggQJ7iES0k2QIEGCe4iEdBM4YYy5aox5f4zn/ydjzKoxZvFuyhVRlu8xxty8S2EfM8Z4xpjMXQj7q8aYf7TX4Sb41kFCum8z7BBhzRhTNsZsGGP+nTHmcMR37yZZHAbw3wL4O57nze91+HcTxph/YIx56pstR4JvDySk+/bEj3meNwJgP4AlAP/HN1keADgKYM3zvGXt5t0g+gR3kOTv2wcJ6b6N4XleHcCfA/g7dM0Y86PGmG8YY7aNMTeMMb/BXvnazvfmjqb8HTvv/Lwx5rwxpmSMed0Y8y72zmPGmFeNMVvGmM8YYwpSjh0TxL8HcGAn3D9mWvXPGWOuA/iKMSZljPnnxphrxphlY8yfGGPGd8Kg5392R+4NY8wvGGMe34l/0xjze668MMYUd+LdMMa8DuBxcf+/M8ZcYmn88Z3rDwH4fQDfsSP7ZoR8lHEfMMb8W2PMujHmojHm59m99xhjnt2R/7Yx5veMMTl2/weMMW/s5O/vATAi7H+4UzYbxpgvGmOOsnueMea/MsZcAHDBJV+CbzF4npd83kYfAFcBvH/n9xCA/wvAn7D73wPgYfQ61EfQ04Q/uHPvGAAPQIY9/2EAt9AjKQPgfgBHWVzPAzgAYArAeQC/4JDrewDcZP8prj8BMAygCOAfArgI4DiAEQD/BsCfiud/H0ABwN8FUAfweQD7ABwEsAzgux3x/0sAX9+R8zCAs0KeD++kIwXg7wOoANi/c+8fAHhKSU+kfATwHwB8YkfuxwCsAPj+nXvvBvAEgMzOe+cB/NOdezMAtgH8pwCyAD4KoA3gH+3c/+BOfj208/4/B/AMk9FDr7ObAlD8ZtfN5BOxDX+zBUg+MQusR4RlAJs7DXQBwMMBz/8ugN/Z+a2R7hcB/DcBcf0U+/+bAH7f8ayLdI+za38D4B+z/6cAtBgheQAOsvtrAP4++/85Iiwl/ssAfoj9/y+4PMrzrwD4ezu/+0g3aj6iR/AdAKPs2X8B4I8d4fxTAH+x8/unAfx/7J4BcJOR7l8B+Dl2PwWgijudogfg+77ZdTL5xPsk5oW3Jz7oed4EgDyA/xrAfzDGzAOAMea9xpi/NcasGGO2APwCehqVC4cBXAq4zz0RquhpqHFwg/0+AOAa+38NPeKaY9eW2O+a8t8V/wERF48HxpifNsa8sjPM3wTwDgTkS4x8PABg3fO8koj74E44J40x/68xZtEYsw3gf2Hh+GT2ekzK03AUwP/OZF5Hj5gPsmf48wneBkhI920Mz/M6nuf9G/Q0rf9k5/K/BvBvARz2PG8cveE62Qm1LeVuADhxN8VkvxfQIxLCEfS09SXsHrfR60B42ACAHTvov0Kvg5re6bDOIjhfgvKRYwHAlDFmVMR9a+f3JwG8AeABz/PGAPwaC8cnszHGiDTcAPBfep43wT5Fz/OeYc8k2wS+zZCQ7tsYpoe/B2ASPVshAIyip3nVjTHvAfCfs1dWAHTRs6kS/hDAPzPGvHsnvPv5ZM0e49MAPmqMuc8YM4Ke1vcZz/PaexD2/wPgV40xk8aYQwD+Cbs3jB45rQCAMeZn0dN0CUsADvEJLgTno4XneTcAPAPgXxhjCsaYRwD8HID/m4WzDaBsjHkQwC+y1/8dgNPGmA+ZnvfBLwHg7na/v5Om0ztyjxtjPhwxPxJ8iyIh3bcnvmCMKaPXmP9nAD/jed65nXv/GMD/aIwpAfjv0SMjAIDnedWd55/eGbI+4XneZ3eu/WsAJfQmrqbuktz/J4A/Rc+L4gp6E2X/JPCN6Pgf0BvWXwHwpZ14AACe570O4H8D8Cx6BPswgKfZu18BcA7AojFmdeeaMx8V/Gfo2XkXAPwFgF/3PO/f79z7Z+gRdgk9bfszTK5V9Cb4/iV69usHuFye5/0FgP8VwJ/tmCbOAvjhKJmR4FsXpmdGSpAgQYIE9wKJppsgQYIE9xAJ6SZIkCDBPURCugkSJEhwD5GQboIECRLcQySkmyBBggT3EIE7E83MzHjHj99x6SRPh1TqDlf3/LmBoaEh3Lx5EwsLC/aafIaHAQDZbBZjY2M2zHQ6HUloY4wNp16vo1arYWxszMYjPTKMMb57Uj5XHFGfp/j4c+l02vd/Y2MDa2troWFxWYPk0jA2NoZcLmef40sPu92u778LlUoFc3NztiwmJyextbWFfD6Per2OdDqN0dFRe7/ZbGJoaMjWCZlfmrz0DJdRuwfcqWudTgedTgeNRgPdbheXLl3C8PAwjDHodrvodrtIpVJIpVK++qFBuxd2jcsp89H13e120Wq1MDIyEqncNbTbbXS7XeTzeWQyd5qrlC0qtGebzSaazSYKBf9eRrwc6LtQKGBiYgIAbF7zsLvdLoBeedVqNVueqVTK5pkxBp1Ox7Z5ukbvcjl5Pm9tbaFcLvv4h8v2zQKX1RiDZrOJdrvtFCqQdI8fP46f/umf9hUUZTT/eJ6HQ4cO4eMf/zharZbvHuAnMC5osVjEQw89ZAukUCjYgu90OmpF4I0bAJaWlrC4uIiTJ0/2kauUmWRxdQJSXn6dKoSMnxMuVa5MJuNrIADw2muv4fnnn++7roHkpXxxyS5l3rdvH44cOeJ7lxp+u91Gu91Gp9OxaZH5YIzBjRs38OSTT6JQKMDzPGQyGXS7XSt3JpNBNptFJpOxjea+++7D9PS0JXaZ51rDov+coHijNMYgnU5bIq3VaqjX69jY2ECr1cIf/dEf4ciRI0ilUmi1WjbfeXpkHPw6/y+/g0jU9ZEdW7fbRbvdxtbWFu677z4rh5SR1xtepp7nodlsolqtwhiDTCaDqakp26ly2ThZ8XuuTk0S7/r6OjY2NjA3N2ffI5nom37Pzs7i4Ycf7qvnnufZOkZ1bmlpCc1mE+l02nbS6XQa9Xod7XbbKkpUfp1Ox5d/lOftdhue5+H8+fO4evWqzQMXBiHhOO9wjtGUmc3NzcD3Axmg2WyiXq/7Istms0in02g0Gvjyl7+MTqeDBx98EPPz8+h0OoEJkA2gVqtheXkZGxsb2NrawsjICJ544gnbSLvdrkqQLk2GEs61HZ4ZsofkskrylNqYqwLzcHn+7Aay59Rk1u6tr69jcnISw8PD6HQ6gRpf2GiEPu12G6lUCt1u15YLD9sYg2q1ipmZGZ82I8PlJMobFBGxa2TBSY0TQbPZxNraGqanp21+c9KX+SjT5npGI1/tv8w7SfAyDdQ2MpmM7xmpEFBc1WoVzWbTXmu1Wtjc3MS+ffv68krmu0t54HkbVi+0MIwxqFQqaLfbyOfzvnvlchmdTgeFQkFVuuhDnTV9ZLo58VJ+8mtBMg+CuO/uhVYdSLq1Wg03b95Es9m0DS2XyyGXy2F1dRWvvPIKPM/DuXPn8Pzzz2NjYyOWcN1uFzdv3rTETo2PV0ZXY6RrPD6ukUjNwUW4PFxO1vwdSchcs5CNJp/P+3pADRopafellitll+h0OlheXsbhw4d970k5tDCjNMigRs01a62xaeUpyZzepXu84wPQN/yk+jY5OenUaoO+o14Lej8Ixhg7zKZOh6dJEhNwZ1hOI0aKj+pdqVTC6Oio2i6kYiAVB3lNashcbheIACmMN998E6+99hq2trbQ7XZx4sQJ3H///RgdHbVhuTR5KntevtpoiXfUUeV0Ya9JNkixcSGQdLvdLur1OsbGxqwG1263MTU1heHhYQwPD6NcLlvy5L1XVKHb7TaM6Q2NH374YTtc4RVIkh/d1wqTntMyglfyIPl4pZANQ6u8BD6MIhtklIbqImGp6WsakfxfLpfRaDSQzWbVZ13pdmm6mjlJysLLIJ1O++SWZEvXKI+kDFraeHnyxtfpdFCv133hyHSGabau31od0p6TjU7+73Q6aLfbVsPl+aURLrUnSap89KTVg6DOmz/Lr0vzlYTsPPlveq9YLGJhYQGNRgOZTAbnzp3DW2+9hYmJCRw6dAgjIyNWdq0j1eTSRg2aLHEQ550oRMvljYtA0s1mszh69KidCOANqVAo4L3vfS++9rWv+Sr9xMQEtre3sX//fqytraFQKKDVagEADh48iCtXrmBiYgKlUslnushmsxgaGrKJ0bRM+e0iTO0/rywu8wN/R9M2NZKToIpFPTjZowhBDVm7zk02QZ0M/W61WtjY2MDExITVrAhkb/O8nm3RVbmktqQNmbk89Xrd15jI1quRhyQebl8m7Ys/R89oeU+k22q1+tIaV6vlv+v1OtbX11Gv11EoFOyQnndEMixJEjI/W60WstmsSrie17Pf8okneo/bUwFYW7uLtGR+B8kF9E+GSWidPs+zI0eO4MCBA7h8+TKazSZarZadQ9jY2EA+n8fo6CiOHj2K6elp24FQWVP80j6uKW9RiXMQUqb3qIyDwnDdjxpvIOmOjIxgdHS0bxKpUqmgVqthfHwc6XTaRwxzc3Oo1Wr4xV/8RXz84x/HT/zET2BrawsXLlzAz/7sz+KXf/mXcejQIVy8eNFHupubm+h0OrZAjDG2gmkF4SJXnmmy8UqC1XpT3qC0SksVhKCZMniFotGBZt6QcGkrvDLISTAtHxYWFrCysoJisYharQYAvgmxRqOBiYkJzMzMOOPk+a2RLQeZnvhHGwlIjYXqDte8KE8lGRLJ8tEUkVmtVkOxWLTX+HuuawTu+QD0ZshXV1dtnSYiyefzVsbx8fHI2g5Pc6vV6stXz/PQaDSsCYKHy9sCLwdZr13EK69rRMzzXcqtpYEmB/P5vC27xx57DGtra9je3raTaJ1OB7lcDt1uF41GA+vr65iensb4+DgOHDiAVCqFZrNpO38+GUlx0uQar1euPA6DplwNAle7i4NA0vW83kQK0Bs21mo1LC0t2RlwY3pmgZs3b9rn6d61a9cAAJcvX8bw8DBu3LhhC5sIlqNYLPb1uplMBpVKxWZ+oVBQZy65RiTld/3mWjs3I3AC0AhN0xok8dL7VNC7nVzT4tQ0Lf6ftEC63mg0fOlYXe1tpjU7O+vstWUDdjVmqa3yMOT7kpRJQ5XEK+Pm5Czj5hq8zJ+g341GA6VSCd1uF0NDQ6hWqyiVSn11kzwnyG2LtNYgMpd5QG2DFAt6R06YUTiScGUcsnzoGs+73Wh8/FuCXMwo32dnZ3Hy5Em8/PLLPuWgVqshnU4jm80im81iZWXFdmhHjx61Gj4An2LnUjA4tBGolga6L0dvcfOCh0XXZScYFaE2XQDI5XIwxqBWq9lMMsZgdHQUP/zDP4w//uM/thX19OnTqNVqyGQymJiYQLPZxDvf+U585Stfwauvvoput4vt7W2r5ZLg+/fvV4cUZNqQ9k2ZEbKHlJlEIFe0YrGoasKchOkaPSdJnROEZv+lPJQ+u4MiaNgTNLSV16kCrq+vY3x83Ofby+9rDdf1X5Iqt+WS1sbzj94jbVdrQFzD5a45Mn2VSqXPtZCH5SJHIg+g5/lBHZMrj8mMQf7KMjz5n/Ihn8+j0WjAmJ6nB2nsRFyyrlHYWlm6NNYgrZffc03wukha60Q3Nzd97RKA9SLhnibAnXmhZrOJXC6HbDaLzc1NHD582GdOkB8ySVIYfBQkO0WXzDyPuKtiHMjy5Nej5JmGUKdRqhTG9Gbm0+m07e2JeLmJ4a//+q/R6XSs/+za2hrOnz+Pzc1NfPKTn7QZyhuEMcaSoCtRriFQWK/M7zebTbzxxhvwPA/veMc7rPsKz9ggn0cXeP7w8MieK22SuxnmBBFv0H0tHZ1OB6VSCZOTk77rcvJLkij/AD0zVNDCBM01iNu+SRY5sUOEG6ahNBoN3+ShfNZFjM1m0zcyCANpZul02pozXHFwUN5QeqrVKoDeSE4SldZBa2FL4pUjA/kc0G+20aCFIcml3W5bOz6Fm8vlMDIygs3NTZWMPK9nRqGOJp/PY2ZmBsPDw/a+/PDrpAzRJ0h+2QZ4uvey7ck8jxp2KOlSZtVqNUxPT9uGRbbBcrnse5569C984Qs+DUaGSQgiTUm0riGFfI73jPROrVbDSy+9hFKphFwuZx2yXUO0MBl4Zefx8vDIjkoud3uFQYlX3iMSkTb7KJ0ZPU/am+wguXsUxSVHEUAvz7LZrE8TIZMWkb2cuJINiEZPExMTfY1Cpp3/JkKnPIgyGqFVcdRJaOGS7Pw62fY1eehbUyyCGrJUTFx12TVKCIMrP8h0RXFSXpISw0edGlHV63VcuHABV65cwezsLGZmZlAsFn1KkCxvbv+Pkw6NyHeDIOKNilDSJVeXWq2G9fV1mwHZbBa1Wg1f+tKXUK/X+yoMacguoiRHeZro0eDqobXKxTUGANje3kaz2cTU1BTW19dx4cIFbG9vW6K4ceMGTpw4obrilEolXL16FSdPnsTw8HDfsE5bBADApoWWpwKwDTqqhhEVuyFeDqk5RK2YnJQ7nQ7OnDmD9fV1LCwsYHh4GD/4gz/ocxcCesPztbU1O9MN9DxaPM/Ds88+i9HRUdy6dQvVahXFYhH79++H53kYGxtDrVazkzLFYrFPTlq9FaaB8sZMQ9UgzUmDtgopKC4AtmOhciGiCnKx1MJrNBq+eQ1N2+LtbRDCdSkhRPDtdtuaYnjY4+PjVtPl7/COiMvWbrdx+/ZtLC0tIZVKYWRkBNPT076RF+9YByFdKeNeEK8Wvhz9BSGQdIlgKpUKqtWqtdXyisqHB3GQy+UCtT+t4GXGyWf4UsJut4tz587ZGXxuQ06lUlhcXMT+/fsxPj7uC6PT6eD8+fNYXFzEysoKHn/8cUxMTPgIV6aXy0GkIG2YMm2D9JB3A8bcsZfLzoE3Zk37pe+VlRV87Wtf82lyFy9etJ0PhdlqtbCysuIzL5F2Q3WK59nFixfVPMpkMtja2vIRLCkGNP/AodVNXpfilgNfCOKKh3fS9OH1k57hblNh8rrsmdpwmr8zCMLMCzSi5Z0PVzY4aOSTTqetSUeGSaOVcrncN2HOR0n0Pw72inQ15ZHHwRWyIIRqupVKBSsrK76VQ3w4OTs72+cjyQV0CalpWFqvzAuVKiiRhOabSYmnzmFra8unXVAcnU4Hq6urlnTp/uXLl7G8vAygt7Tx2rVrfT2vNuQF/B4PfJLGZSf7VgDliTQPuCqP1qPfunXL5x0BAMvLy31DbC3NUd3pOMgPVIKGvTSpFkS2QL+t2fWOhGa/C4uL6o1cni2f0Wbsw+yYPJxBTQmEoDrKy50muuQIMJvNWq8GQqFQsHlNWwjI9PA8abfbatsOS5tWLlxG/j9qHshwgt6J2skFkm61WvW5iElHf+q90um0b7YxCrgtkf5Xq1W1MkotjDKR4iUSdTVwWcmpkiwvL2Nubg5DQ0PwPA83b97EG2+84ZNrbW3NN8njCpNrr1RpyJfRZdveDcLCixMfkS6lUS5tlpCVj9zPwp6ja5omKn1Xg54PQr1et9pzmDwut8AoiEq61HZkO+H1hZureHqp3kdpyNTgOTGFLXyQiKMc8PbO005+t5Q+GnnIZ6RCQr9pRapsz1EITRsB83yNiqgdr+udXWm6lUoFW1tbSKVSvkymTEylUmi32xgeHu7bWcfVuDSQHx83ztO3SyMgOSqVSp93QFhFI2f3jY0NPPPMM9bUITsC4I6/JG8QvLEAdyoINR7afYnb8OJqc0HYDYHLSp7L5XxaEl3n6XSZGQjaKIeXfxhxavcGSSM1zEajEajt0nWtkYaBiJB8dXl4WhztdhvVahUjIyO+uCSx8jB4HkchAE64vM7xznQ30Mqbz9nw68ViEdVq1XY0rklQV57TRjpyJCM7lCBZtdEv/x8nrS5odT5K+EAI6XY6Hayvr1ubjDasbjQafTOZnJyCBKZMbLfb2NzcxPj4eOQMI3KTvp+UeNIstEksmoFut9totVq+lXESExMTfVou17QpDr5ghDdMOVkSlC9REKVSuJ5xkZvWoWg2Pa1i1Wo16wLF44hDZK4OWpJ3lPQA6BveavHRd1xNWiMQfl17lmu7GikExRNFHm4P10Zj0nykQesMNI1Xy1eeJ5rrZ9iogD+fzWbVvTS4TTcIknv2UssNyr84HXjoijQ+McU1IML29ra151HBa42UZ4ZW6dfX1zE0NBRYqPI/mTwosTzBrVYr1G5E1/h92iuCVg4dOHCgj4zk7HUul7N7iQ4NDcEYY4dPUuOVccapEHtNuHyrRW2YrREhfWgV1dramtppaZVfI21ep+JqJUFpjEJmg3R+vAzDyESr55lMxqfFDWreAPr393XlcdzwtWe0SUOXVikXxvD7UjYZl1SUZB5G1XQl1+wFguTWnnEhdCKN25QoQzmr0y5PQQJJbUKrnLQ7lsw07R26T+YNSRCVSgWvvfaaunSUy0juTMViEeVy2S78oEaRyWTsyRaa5kf5k0qlrKM75QnfI4B+a4sHomq+YY1lkOG7NgR3yUJ1gGv16XQahULB5l9YWEFEGKSFx0EY6Wj1VItHk4eW/lKZB6VPhtlut30b3sj444Lapdx/1hV/WNqCrge5PMryprzhXj5aPmlaNOBXUrTwwxA04pAYJN95mIMSb+gyYDlBJis17aMZFmmU3kkuiXSFQd/kMC1nOnO5HA4ePIhLly75NuOhTiObzdrNNsbHx3Hy5Ek8/fTTfRMPrVbLLjHlwxyXXMYYu3GJfE5OHGgNVCvEoMLl70e9TtdSqZRzkxiyV3LzD6WJNyLqnLhtMy7ksCwu0cat+Pw6dbK5XM63Os0F6lg1QgiLm65RvrrcJcM6DQA+c4Lm7cDD0kYPUSfYorRreZ2XpxxBRenYPM9TzQtE6HEQVp6y7t0rRDYvuECO69JlKAy8kY2OjmJ8fLyvIkYZvnHSpTAzmQwOHz6MRqOBa9eu9fWqfGs5Y4xvRpl/Hz9+3J4HpZlGgDsz4FzjpT1UOST5y8oYV/uIq9nK63LJMh9dkKbuMjHwsIwxGB4e9m1gHxeuhillixqGDMd13RhjO54oNkOt/IPi4XnFdxjL5/M+O7iWBhf5EvnwdhlWF6QsYRNsckWZK33aNarnMj/DCJCPHLX3ovi/hsn3rYJQTZeTB2+YlEm5XM7unVsul32kRloEhUHaFW2c02w2kUr1tsojsuIIG1pQIUv7Ecl2+PBhNJtNrKys2LDlUKzdbmN0dBRDQ0O245iZmcHa2hpOnDgROPPLtT+qZHyykSo32XvpnSi2x6A4d3svlUpZ7V3rADqdju/kApd2ThjUAT8OXNqUlEV7Pux6lBWD9O4g2i3FQfFQHXHZSTkk8XHCjaqhacTL6+eg6dOe4QtfXCYEKZsMQ5NDK6M4eeDCoO8P2g6BATVdrQfMZrOWaLPZrG/nKl7I5ItLM/+856Y4efxSHh43rzhyxUoqlcLw8DAeffRRLC8v49y5c9YljC/0oH0R+KkPFF7Qdoz0jNxkXFYOmpQLCoendRBbXNA9rRHTxkWueLim7uog+HVNa4uD3TQeV8MOIgnXCEraW3lZRNHaXPlDaLVa1m+1UCjYzV9kGEHEq3WUURBmtgsavkcdpgP9Gwdp72p7nsg2HcQ59O5uCHeQd6O8E+WZyPvphkHaluR2fURS5PzcaDQsabmWNvJvl3w8Xj4E4YQ5NzeH1dVVu6evhHT3Ip9jzc7IKwtVDllJqLPi/rscLmK9W6YE/p+Gt0HEwVeJ8crtIpUoJxzvNfiISmpxUYaiWucelg4aycj3XeHLZ7hHjTHGKibaQgAX8ZIP8qB5rtU9aR+W6QiD1MRlHFSHaIRFpity2eQ847IDBykjUdN6N7TaQZ4LJd2g3k+LhDJ0aGjIVlDpJ0jDEL7mPkgG1zU5RAsqrP3792NhYcGuqjPGWLKvVquoVqt2oQBpxLdu3bLbz3FtnQ+BpNO2vAf4jyB32R6lxqZd59CuR3lWanASvMOQYcQxL0RpILIzk++GVWKen7xTCdLq4sgpZdKINMr7XB5OKjTikx2HVCZ4ej2vtwx/eHh44I3xtXx3tfWoRMfrPK3EpLiGh4ftSJhGg9RpkgJQLpd9I9mgdh+HPO8m0boUmzBEMi9QgEEFIIfZshBlpeJhytU9/J2g/0D/sI93Bvx7bGwM+XzeOs7zmXjaHYxrrkBvw5WrV69iZmYGR44csTuucc1WI2HZQGk9OscgPTdPU9Tr8pmg5cwE0nSjhhl1llsiqjYahXyDwo0azyDaqyssTWaqH9zmL8PU2okMj8wSY2Nje6LxSiUhKF1B9+l9WpFJnMA3fNfSy+UJyuM47WVQ88sg4cSNK5KfbhSQSxR/T5oXeKZx0qO9CjiiaiC84gbZeVKpFHK5XN/+Dt1uF1evXoXn+X2Q6Xez2bQa8pEjR/q0WSmDbDQUR9Qd1YIabdyK4bJ3UjxSo5bae1RwW/puEKWxBeVLHFKMe99FokHhaOTPD9HUFAbtw2Xgo8ft7W0MDQ2pp2ZEgSTeOGXuUoAoTNpvW9t7wZU3rsNFSda9wCCmur3GwOYFDjkRRpWL7DbcFsbfoUxtNpuRNF0t3qBek1/3PA8zMzPY3Nz0pYtXAFkJua1tcXERQ0NDGB0d7SNWLSyp/cpVSHvRa0cd3nCNn5eDSwaXnd2FKGmJQnaatuOyD4aFHUemIA1MezYoLC4nl5fSp3Vosv646hH9JxdHWjLb7fZObhhE643SYUXtqKTN2xjjO0suSjnFeVaDHOWGPbdbDBLOnpEubzR8Zy1eIFQBpaBRNCstw3kj5ZNhspLS98jIiD1qSNPOKFCrAqIAACAASURBVL28gfD3b968iePHj6tDM1djofRpG1bzNLiwV4RLv4Pik+Wn3ePfBNeOU9p/fl1q13xZclBYUTqOqPf5M1HCitpZ0rPU0VGaSAnROpiw/81m07fsnkiNymL//v32dJe42AtNl0DtmeSKYysOM3EEjWaB6CTIO/VBMei7oaQbpdFwoiIPBZ7hvFHJnpWHEUUDlJlFWiTZZTVCpN/ZbBYjIyM+R35OhnzWnqeLwqCN3Ekr1xqJlm8UFrmhhU1S8euDXuPX+X1tZl/GH9e8wIe8HDw/JNHwTpme44tU+JJbCkPWGUn+e0HAcZ6L+jzv7DyvZ06j+QFXRy3vNRoNbGxs+OpOt9v1LUq6evUq1tbWcPTo0b4TNKKkISgdUTsk3nG73g3iFO3ZQSeswjBIOHsR965supqGoO2MT3C5n8meMSw++k0FTK4nmlzy9+joqDUxUNz8GW4m0cjRtQm1K07gziYe9FsLm8cRtaJpNlvZKfFn+FZ7LrKiMouicRDoeZ533KtDvhsGTjiSfCi+MO00roYblXTiEDJ/Vo4UwkxS/H+5XEa5XA71FfY8D6VSCRcvXsShQ4f6zowLk1UqPnGUIPmft68oHaNrFCbzYrekd7eJNsqzkc0LQXY0uWqN++hKEpG79dPztEBBCz+KjNp2floFlmTEnwkiGmPubBBD6XURr/bb5drmiivsWdczQb+5zS9I2+UEKhuMbBidTgflctm301hcTVEDLyuXBhSHWKM+E0a8cbVrAh8WS9unq8PudruWcKPISOHX63VcvnwZR48exfT0dCzijZMXUcKJGp4Wf9z3g/CtQLaESKcB828NvIHS0FtqXi7B+HNxhrRcvlarZb0ntEKSxCrttS6QucTzets3Tk9Pq1pWENkCUO2UmmaqEaEmU9gzrnRI+fh9qYXxzpbfk3Jub28H7kc8KHgn7NK4NM1nN0QclWTiKAQEaWLQ6hC/3ul0sLGx4TMfBMWrzZNcu3YNlUoFhw4dCvXp3atOjKcnaKexqDLstgOPS7Z7QbRhYQxs0+VwNVDXc7uJS3vH87zI2mc2m0WxWESlUnGGm81m7baFZC/mRB1Gsi6tLArJx2lYmtYun4vyjCavHGq68nOQjjIIRE60WkvWLf6MJrtEnMYehXSlBh4FvAwk+UoZgN6E2dbWls8NM0y71dDtdrG8vIxut4uDBw/6DnvU0rfbvHR1jHHe3U0YwDdfo921eQGI19Pw4dMg7wySYdTw5fltdE/+9jwPExMT9uQIoLdpD5EsLVUkki2VSr6dxMLCl3FRI9M0Xa2DGtSWG/QuJylNXi4P5edeNMKoINkymYxdyWiM/6hvQiaTQaFQwMbGRp+muxeamusd7Zy8qHG5OguNdJvNZt+E2SCEy7G6uopKpYL777/f6dP7zSbdqHFw7Ma+ezdMBzSi3JWmC8SvpINoPlrPHxVEEtr6de03xVcoFFCpVDA0NITx8fE+GzPX4sjjQJvICCN61wQBT7dLc4pDuK4wpXblCou/E9V7oVwuxzqQVMpLy66p86PzsUgefrosTUDSyRxcbhcGGTm5ELRBkKtsg+5r1yqVCkqlUuiEmQw/Cmq1Gi5cuIBDhw5hcnJSfWavSFcqFnsRthzpDIK7aTqgNhbFXW9PzAtBBBf13UFIl5OEthtakFw0qWSM8W1O43meSiSkTWvnpbnipP+yU+EkKCuTq2IFmQxc+RJWEaSGHacseOfEfUXb7bZdhUT7a5B7FHeTMsb0/dfiJs1Myy9tpBAFg3buceILy0utntRqtbtCuIR6vY4rV66g2+1icnKyz995L0Y2UdrDbsKPizhaqvwv29luNGuOPdV0d5tpu9V0g8LRhnJAj0xlZQd6GUzaDZ2GTHtExCF3Css11NQaclSTgSsOHn7YBjD8njQxhKFYLFoy5emnMINsiPRsHE0oqsYY5Z4WdhRC5ekLik+WOR3X45KN9jAOk3u3Db/T6eDKlSsol8s4fPjwni80AcKXhQ8aT1DeA9EnlcMUmN3kcZQ82nPzwiDax27NC5wkomoVdLAi2Vs9z3+6L3DHrEDyNZtNdZgZFmeQeYGe1cwC8tk4pgVpP3S95xoKRikLl0eE7IQG0Ua1cOW9vQw7TjhxND5awNDtdvuW6Wqd9V5rty7ZlpeX0el0cPDgQXttEE12N8/uNqwg85sc7YUR9qByhcmjYU/MCzJCFwmFVexBExuVdPk9IlM6LJCGwnyYzA+WpGthS1Rd4O/xChFWmWTlkc9p5KE965JTI+a4DfBulOteEeBu6tRegDaE10ZIPK6gPN9LwuWgk5zJxjuIghV2fa/KkRMnb6dRNdS7lYccUdN6T80LYYUUpTfTyI5IN+pwRv7XdgHj+0dQQWvD7iiatSQ2lxYsK1DQf+197bkgLZff5/e0ibS71WFq4USNS2ovd5PcB3mf5y3ZvV3Pk9cMLWcn3G2yqFQqKJfLvqXtcXEvyZefNBz33buNPdF0O51OpAMntYYbB7tR52nzGrlqJyyOWq0Gz7vjwE2aLpkPaMEFv8/PpooTlzHG56LGr9PzMg+Dhk5xEHcigfySo2yQzUlvNxVekmeUoX673e7bTjQo/L1G1JFAp9NBvV63+y243qW6KBem3AtQvZSEL+UbBHulPRtj+nzxvxVhjHEu8ScEku6pU6fwoQ99yGp81MsMDQ0hk8kglUrhi1/8Ir7+9a/HtrlEeebxxx/HO97xDgB3/HiJBMvlMhqNBlZWVvC3f/u3vuPbg+yh5M9LQ5RSqWSfizKspniCyFFepwozPj4eayMSY4xvkyAupyQml/bs0vLl3gj0TfsG5/P5UPkqlQpu3LhhV6SFaTYyT+bm5nDgwIG+k4d5OUgZichKpRKGhoZ8cfH3uEZE+UEHpQa5fwXJz9sBNwlIzYtGSe12G+vr65Zsw46+SqfTmJ2dRbFY7DsuiTpuPhfB89SY3mkt5IZXrVbtkVj0fKFQsN4gfNS2vb2NbDaLI0eOWNm1Dag022itVsPZs2dDJ17pncceewwPPvgggP6FLyQrKT/G9BbJVCoV1Ot1nDt3DlevXlW3gSXQSeDDw8N9cUsFgStCdLYjfw7oLVShk2XIvNhut20eSfdSAE5N3MoYdPPYsWP48Ic/jE6ng3w+bwUbHh62BTw/P48XX3zROYwN+x107bHHHsOHPvQhAHfWq1OCtra2UC6Xce3aNTz77LO2AXJipv+8kna7XashyV3PpInC1UHIQyvlEJeTAH92dnYW4+PjWlb7wImDbM4UD4UvGwQRAlUEqhjpdNq6cHH56BlpT+x0Ojh16hRGR0edlafT6eD69etYWFhALpez9kpJ4LKBEoaGhvDII4/g4MGDdkc6vhkPl5EqM9+trt1u49q1a5iamurrYIgsMplMX4eVzWZtveVyuvJfdiKVSgWFQgHpdBqNRsPKSYtp6Ll6vW7rWa1Ww8zMjKOk/aDFIWNjY1Z2/qlUKqhWq33Xqc4PDQ1hcnISxhg0m02sra3ZTdPT6TRmZmasL7TneZZElpeXUSgU8K53vQvNZtOeW0b1SXaIXIF56aWXUCwWnaTL27QxBg8//DC+//u/32qEstPiZZfNZpHNZrG+vo56vW6XRg8NDYV2mrOzszhy5AiA/gNHqYxozxBShGjrV3qOVgaur6+jXC7bjbXo5A5qY9zd1BgTelBrqHmBtBheUakxDLqKLCpef/11fOQjH7FxcWd5On5ncXHR+b4kXOAOYfJNdjRbLeAmDZ7B2nVJYlE0aCm3Rv6uvKbCL5fLqNVq9j/JSPk2Pj7u83uV4RO4jVsSb6PRwJkzZ7CwsBApHfyb5Dl9+jSOHj3quyd3p+P/iXCDfLG5ecaVNllfXaYRrtFpIwHyPW40Gs46wMPS4nGN8LRzBekjlQKSkeSs1WrI5XK2jCcnJ1GpVNBoNFAsFn2Eq8nM6w3JwOPh7zWbTbz88su4ePFirLr95ptv4v3vf7+P1GUnzbeK1c5QdI3iCNvb2zh79iw6nQ7uu+8+X/4SarUaarVa3xajZOIhTpCjG75aU7b1qFwYSLqcWHkC+TCA/sc5aYDDRWzAHUd7vmGMHDZqM5nakI+DNCveS/H3w2T1PM/n+M/vaxqfNlEXBk3L1hpzvV7HysqKbym0bJjUKwPwLQPVGiC/TivBKC+r1SpefPFF337EYfJLeVKpFGZnZ/vi5h2fVrEl4dLxS5IYSUuLIh8vd1cnykc3JBsRAcnc7XZ954Dx9GthBclEdV3TZmn+QlMmgDu+vjR6MMagWCxiaGjI+lS7wM0KUqmQdbrT6eC5556zx1xFhefdOfSV8pKXMeUjpYF3krKzDMvPbreLN998E+Pj49i3b1/f83JinI9I+ag4Cje42pELkfbTJVIlEpSC8N5hL5FKpax9i47/od6P/Gnjng/FK5DsLLQMloUlKx8vLEm68p040DQyWdlarRZWV1cjEw3XfnnYMt1ce+KaBg1voyBMHpccXLvipKZpslqDDFIA5LaiUgPX8kJ2oOl0um9Ch4dHnVSr1bJLepvNpj2Sii91luCNXSNdORqQ96m8aBEP19S4/VTTvDUtjoMTYKPRwO3bt9U0hIHIXdqsuQJEbZueG3RTJVoIMjc357tuTM++TfZarjxyE5dUyjiCRkdhCCVd3hh4peS9okuw3SKdTmNsbAzGGGvLymQyaDab6HQ6aDab1uzhGq6FpS1uLyUrpDw5V1ZkiidqHFFkprC73W6svQ/48fNR4uTDLmMMJicn8Z73vAevv/461tbWnBUvDJI4eHr4b03rAtA3uomijciNSFzvade73S6q1arVNLPZrJ3U4qRA5FGpVOwKxlarZU109Dy3scv805aZS1u3zDdOvCQPdcS0KIMvAOLp5J1bWBxS1iigd+n79u3b2NjYwNTUlG8kyMPj+RlmKw5DqVSyPME7JnmmYzqd9p14wzsDzazD340rU6RNzKnQeYWg4eygvVAUkGYNAMPDw+h0OnaGkZ9zxhGWcJl5Gllq70iNhzdIzfnd9duFKAXGy0AbXvKwXHHGIUUeFl0bGxvDu971Lty8eROXLl2yLoUkVz6fR71eD41HNjJerzjp8meJVKjeUd0MIl6tMWv3XBou/SetK5fLIZfLWe8ZbrPnac5kMpiZmfHthuZ5PZON5hnCFQCp6fI80WTm6HZ7h1SSNk7lQtqvPKuP12tOvFr4cRUr3gkQWq0W1tbWMDU1ZcN0kT5NJg7CMXw0WK1WsbCwgLm5OQwPD9td1wDYnQV5fhMonzTijRK3C5FIlzKPAqTZ0FQqhfHxcd/S2b3EpUuXcOnSJTz00EOWbIl4qSK7TkDViEnTynkv7BpGhA0vyI3MVVnDEFRIvPFTOQxqPw+Li8cp4+XXcrkcjh8/jpGREbzyyivWjYk6hTDSlx2mJFzNzs4bMN2jUQbVRW3SxZUHUkaNrOk+zaST1uh5vWOf8vm8HWnQsL7ZbCKfz/s2lOFp0WbspQyaYsDzXtru+Xe9XrdanWZ6kPVdtg9ZLjJfwuo2eb0UCgUMDw/jgx/8IL7whS9ge3sbKysraLfbePHFF3H06FGblxQu17Cpnkh/cVl2YfW53W7j5ZdfRiqVwsTEBEZGRjA1NYV2u43t7W1fGBRupVKxhE/xyw5pN4i0Io1rHlzl3t7exh/+4R/eNW23Xq/j137t1/A7v/M7OHbsWE9g5mJUr9d9PnthBeAyg2jEq2Wwq5HyChJ1iKGF49Ky5H35zdMQBHL70zodCdngZHqNMdi3bx8OHz6MS5cu+To4KTdpiKQB014EPL300QiXT2JpCNJgZZrkc0H1gcvGbbPUEEn5IG2JNHAivaCFRZqMmmmBj6goLrpHrkuSKOUIgd7nhCVJn78n3cSigBSgX/qlX8JLL72ES5cu4ad+6qdw8uRJ/Nmf/RmOHTuG1dVVGGPwwgsvYHx8HD/0Qz/ke5+nNWykGCYbr4uNRgMjIyMYHh7G5uYmNjc3sbS0hGq1ik6nY11gyQyzsbGBVCqF4eFh29lzbTyuLBKhpEsz9NKOlslk8Nprr+Eb3/hGrAjjwPM8vP766/joRz+KX/3VX8XIyAgA4NChQ/Y3t8MQtEzgw1Wt8dH1qA1WG47yWewokPajILKVpMafdZGodi8K0Whp5vHzDml+ft43i81HGGRqoAUJFA75VPOz9FwTZpr2p8lIw2YtzTQpUyqVsL29jampKaRSKbvyrtFoOM1V9XrdziHwCd1sNovp6WmMjo760pbJZPqG8Dxfgn5TPmhp52Y+IgIiYNKy5ay/DEvmERF2VEVDA8/vBx98EB/4wAfwjW98AxcvXkQ+n8eZM2ewuLiIxcVFX3qWl5f7ykrTesPqahTCM8ZYjXdhYcE3D2KMwerqKgDg8uXLSKVSmJ+fx/T0tH2G59leaLyBpEuVK5/P+7bpo8q5uroaukw4qIJFgTE9p/BPfOITNuGnTp3CRz7yEUxNTWFhYSE03CANViMy7Z0o6ZOmGH4vSGuVMmhpcWmdccEbodTwNe0mKK+MMRgeHvZpsTQKIi3QJa+mScq08wmkOOni1xqNBhYXF633DQAsLi46tXtX2BJkKywWizhx4oSvI5G70cl6oC2scXWIvHxc7mSkeMjTLXiZynoGwLpv1Wo161IWJa/JTCCxtraGUqlkw0+n0zh48KA1zRBvpNNpvPOd73TmLYF3CLITC2vv8vfm5iaWl5f7rvP00oTn5cuXsby8jFOnTvlGNfT8bkf2gaSbTqdRLBZtoZK7GLnNRHGQ3w1SqRSOHj2K/fv3A7hD9ufPn8ev//qvI5vN4vr164HDfvpPGkiQxhpEvEGQxCtnyjlkhZDyBkGSUFw3vagabtD7suHm83mMjIzYPYn5rDCBT4jKsMgzgK/qoo4rrHJzMtH8tTudDlZXV9FsNp2EErV8ebhc/lqthosXL+KBBx7o89nWjpCSsmsjJhkH/+0iVKp3gN8bgPJGlp2sR650yzRXq1W89tpr6jubm5vY3t7G5OQkcrkcRkdHsW/fPhw4cAA//uM/jo9//ON2ToYWx8g81vKax+VSXoLQaDR8ymGQckPxl0olvPrqqzh+/LjPpYye0Xx4oyqVgYuEyY2C/GXJZknDkjNnzoRGsBsMDw/jwIEDvsrW7XbtEj55lhSHrCya7Ut7RyPFKBnJK00UFzoX4bri0+SK4y4G3BkmBVXcIHJy9fLj4+O2wvE16dVqta8xeZ6Hra0tW47nz5/HCy+84Ns7I65vJidZDlqhF2aaiArX841GA5ubm3Zvh0wmY89y08qyUqnY/QRIY+REKIexPO+kKx195OpK/gz9p3KRS1d5+vg1bbS3ublptVnZyc3OziKfz+Ohhx7Cj/7oj+L06dMYGhrC4cOHcerUKfvcvn37MDIy4qtPsg5yjw2gfylvWLvkz5D7XFh5y7xoNBp2JM3DlPFov4MQqOkuLCzgy1/+Mp544glrP6LZYjpVd9ChbpRegZYl8rX9fJOKOL2eRnCyEHaj7fI0yYkIrWA0rUk+5+oYNNnjQtO0eMfGNSctfg5a7y9l4hMzfMb9hRdewOjoqN2bAAA2NjbsSrW4hCsbFCfh3eZTVBlo5SQA2y5Iu5J1nQiF6jGZYrLZrN13gA9ntXopNV0agWrmB17n+H+NcOk5l8Y9NjaGYrHo25FseHgYY2Nj+MAHPoDZ2VlMTExgbW0NtVrNbkpFJh3qIMgUIk1xvA1xc92gk1hcCYrKVZwfyuWybze7QfmOI1DTrVQquHz5Mj7/+c/j/PnzaLVadtMHAHjyySd3LQCHJMNarYY33ngD9Xrdatna8EMiTJOjb9mz07Nh9iPXuxxSq5aNRLsu49QaDEeQ65iWP6QJufKOy8VtcFImSh/d1zbH4flCp/rStXa7jY2NDVQqFVWbi4ooI4K9hCu8TCaDsbEx33MaUXJILbXZbKJSqWB7extbW1vY2tqys+uyvkmNWKtrUsN1ac8u2VwYGRnBsWPHfNfe/e5345FHHkG73cbS0hK2t7fx9NNP41Of+hQajQauXbuGj33sY7a+kp9skEy8nvL7WmeigWu5fOlxXLRaLV8HEyXeMIQfXYnemvu/+Zu/wec+9zk740u+mXcbW1tbePXVV7G1teWbYda0VAlNC+LvyqEcf08bzrggKwtBC1c+FzRk0a7TPgrGGNsDa7JpaS6VSlhYWPCRdVC6aDgq0xdUgV1DVhmXbDyLi4s2PXulnWodl7y/m7DpO5vNOleZaQgiUSLhVquFSqViCbhSqVjy0EiUa4eaCYK/w4laElsUHDlyxHoPAb1Op1wu4/Of/zz+8i//Esb0zExnz571jWQozkajgZs3b9r2KW22sgMOq3MuzR6A3fFtkDpF8S4tLfmWfsv5A/7hoxQXQr0XKBPa7TaWl5dx/fp1TExMoFqt4rOf/exdH755Xs+kcP36dRw+fNhJcFxm/i2vawUK3NEaXTPuQVoLhROF/Pi1qMN3rvnfunULc3NzGBkZwejoqNUYwkBlWC6XMTIyYldFySGlrLRahecuNFQJ+Z4EMj1B2if9v3btGhqNBh555JHApd1avuwGYeUaRY5B45J1kpcBr5dEwsb0PBXy+Tw6nY7dVtPVyKVmSP/b7bbd0lCujgsaCVFdMKa3kU6pVILneXj22Wft6rxz587h9OnTyGQyuHXrlm8nNj5KIlu7HF1ppg36H8d/mNJJk6iDwvN6k4dbW1t9e/RKWalsgjbbAiL46VIEREpf/epXsba2hnw+j5deemmQdATG44Krtxqkwbi0MaoE/BRgrhFEbaCyMfHfcQmXwqvX61hYWLD7GQPhp+3yuCkdfNc2nmZt5MBl5iQrK34+n8f4+DiWlpZ86Qj65toWza4vLi6iWq3i0UcftfsOB5lC5GIUvmy01Wr5tjLk35QmHpbMr7C4tW8+USVP/tV8uGU8rjLgZUEmPgDW7JbL5ZDP5+2RQJrGxwmXNkkqFot9m8G46je/LuuunKw8fvy4NTnNz89jbm4OV69e9b0j/ebpv9w8Kkw2rezomty6cRCQbBsbG74OSqs/lP9hiES6PPB2u41bt27h6tWru1qOqkFr0ATaCT/oRIOgBLvCleYAvpsZvSc1kbiQpC4bg0t2GqoYY+y+qPPz874JE5lGTVYi2Ewmg9nZWd8KGyIpTQZJvHK4SmGnUilMT09jaWnJvqMNweRvkosvktje3saLL76IkydP4uDBg4Gr/HgDM8bvNubK0ygY5D3egXDtiu9J3Wg0rGbKNynXyNdV1yRB02QznRKRz+ftykPKE25+2N7etrP51WrV+q4WCgXVhi89SUjb1uY9KB7aDJyeocUHUoumukQr6+QEoqtthLVzAp9HCNM+g0Aylstl64+txRe13oRuYs5JyPM8HDp0CD/2Yz+GxcVFPP3003Z2z3W+kiYUDU94zx90PhMAS/aHDx/uO67DVSjyW9vKUVbsbrdrG4dWsYKIN0hj4e/zuPjxQdwfUFbASqWCfD7vG+LIOFxycUIgbYKTKeWJto8F13Bl58HTxBu453m28fOTDnhezM/P4/bt2312XM/rzRifOXMGjUYDJ06c8Mkjy4TPcEuZyBsg6taXUaDVNdqDgfZoBmB3w5MdFN8cnjwW+J7FFGaQyYHAr3OXMNrMnLuwUd0nrZTCotMTyIVLhq9B7sbFn7t48SI6nd7R7ktLS5iYmMCxY8fw1ltv+fbGkLZp3vFqo1C5LacsDzma0dr2oJ0whb+9vY18Pq/mC7XfXWu6xPDkrwsA999/PwqFAm7duoVyudwLZMfNhWcc0J9IWjueyWTwMz/zMzh9+rS1L/3BH/wBrl+/7jNYywKtVqu4du0a5ubmMDo66pPTFScHL6CgCkbppnTJMLQGEBQvPcv3HOBLMEkLkY2I8qxcLqNSqWB8fNwXNxW0y1EbgC9cOq7GGOPbpIY+Lmd+/pyWfk7qRCqzs7N45JFHsLKyYo9V4sPJkZERX97K4evMzAwOHjzYp31pdj9O/BpZyXTI34NAhttoNOxkGo1EeEciy4UIiJQaImEiSCK2IK1X5hu/RltPVqtVpNPpPhs+l8s10uF1g6PT6WB8fFzdU5fMOvv27cPGxoY1ffzAD/wAvvrVr6JWq9n8ojgor6Jqo1pbk5160AgySvhavmqbZfHfe6Lp9j2cyWD//v12d6ewCHlly2azvnXq9Xodo6OjaLfbmJiYwK/8yq/g6aefxqc//WnfhIzMxHq9jlu3buHAgQOWeLWZTU3bddnUZEWkb3KNCxp2a+FpshCpUUfleZ7VprUOgA9TV1ZW+k4rKJVKKJVKoYtDuGaVSqV8W1DKBqW5ifHnOLHJDmx+fh6XL1+2J2qQBjg/P49ut4uhoSG8/vrr9t0rV66oW0CmUik88MADOH78uDpBRJq51IC4/LzBhXXCcYjXFRaFI3f4ksQrw+Bx8wkz4I4iQ3VEalGy8bvqozHGLlTR3tdIWIYh5ddWGNK9breLlZUVrKysYHR0FL/7u7+Lixcv4syZMz7/+ldffRXvfve77V4RFIeWZzKOoP+UBpfWKXlB3guqD1QWWhvhZ6uFITLpUkXK5/MoFAp9tg2XSweRCxGu53kYGxvDI4884mu4+Xwejz76KP78z//cOQtOGdJut3Hz5k1MTU35zm4LSzAVhvRr5N9SdtL0NY03aoN1kTpp09rR3NxmOzExYc0eN2/ehOd5vlOMNXmklkJaFJEWvRMGrim70kYdFJGo1KCNMZiYmPB5OGgLFwqFAu6//34cOXJE1XqCyooP3Xme8LPMNK0xajm6RnA0ggBgbZOkVWorBoPIl8tCE2YkOxEwkTq31WrafZCGLDVwjXhdeS07OnmSRqVSwUc/+lHcvn0bnufh7Nmzar4tLy+j0WjY+s8XIPB6q9W9KKNZ7XoQ4fLnXPU9xyFV8QAAGYtJREFUyHwQ1bQARDw5gnpaGja5NCKgP5OMMX07MZVKJZw9exbf/d3fbXtirvGFaRRAr5Gtra1ZwuIrqDQtF/Avg40zXOMbQodVUk1m/qxsIDSkp4YrSSuTydi17BsbG3a/C5dGwuPhq3oIfHUOEXCr1bLXOXGRdu1aLKLFSb9pk2rSfvL5PIaGhuwepjIvs9ksHnvsMefJuS7tXJanvEczykFEG0a8QfURuLM0nfKVTAZBtuQw8pVpIf/ser0OY3rHzUhvhbC6KMPk/7V8Jcg6lM1mMTY25iMaHk7QnizGGIyPj+M7v/M7MTw8bOupJgtvB/z9IGKUcfHvOKMaLSw5KuXxy5F/ECLtMkb+Z9/1Xd+F8fFx1Ot1vPLKK7GE5fA8D9evX0ej0cDQ0JA17vPZRg1ahaEd4KPIAeg9Upi2C/hPgdXCDZJZDsVlpSHC4wTBGxPFH7SPAI+Pky6XkdsauXuTJAciX97BuYZs9IyUTR7LHiWvaJIiqHy08Hha5T2pAIR1jmHyaloXjYS4Fijz0xVeXPKl/3S2F33IY0Gm1UXGsi66INsGyUQLlYIWLdBE3tzcHG7fvm3nMU6fPo33ve99Nu5MJmMX/LjqCB9xBT3HITsF6YURF65DXaNq0YRA0iUb4OTkJN7//vfjwIEDKJfL+NSnPoXPf/7zvmc17U9e5wI///zzWF1dxc///M9jYmICtVoNX/va19StIsOIhu+uJG0rMhNot7GwVSpaRSeiiut+ooUlybXb7drDC7k7EfWupVLJHv3Cw9Dikj643MYqbcO0yolXHG0jcbqeSqXsbnPcTFEul/vIkX80wuamHvLk0PLN1SnyOuYiD9p0RnvXVTejgHdkNErhmpmrbFzhRyFfeY2PZMh7gntEaDbxoBGSJpOrAyDzAQ93fHzcLmA6efIk3ve+9+G+++7D4cOH8Ru/8Rv40pe+hGaziZWVFTU/pO8uJ0qXlinJj4eZz+d9ZguuhOxG69Xii9oRABE03f379+P7vu/7MDs7i83NTfz2b/82nnrqKbUnl4IFCeF5Hi5duoRPfOIT+NCHPoRKpYJnnnlGTVyQfPQdVOH5cy7bS1h6CHy4HfScVrk1TY0XGmn7NLKg92hNu+wotN88Dp4X1BA7nY6PIKXHCVV0aa7hYdJ1ImfZWHh6KBxjDI4cOWJn1AH/Vnnz8/OYnJzsyyP5PyivtXKV/rBaI40KrV7RNU3Tdk1KhrULFzG6yJeXJe3jQHU9m832uaWFpTlI26f/R48eRa1Wszum5fN5/NZv/Rbe8573WDnIO6PRaODYsWM2v2j1GpUNjbT4Tmk8PlkfuLlDyxMtD/k7UeuTDEub9KaRThzNOZB0Dx48iA9+8IPI5/N488038Zu/+Zu4cuWKmgkagioQ4fr16/jkJz/Zd7x3lMbAn+GuUy6fPgC28sm9aaPEG2U4FkdmF2joRhtLy1n5KMQrQROffEECLx+tnGTl5lo5OeXzCkfxyspJcc7MzGBsbMzupSG1HK0swjrAsMrO5yI0RC1PHg//TVq/1hjjtAt5X8bjuh7UgRD50cIJIuAgrSxqp7Bv3z7Mzs7irbfewmuvvYaDBw9iZmYG9XrdjjbT6bQ90eL06dP2YMzFxUWsrKxgfn7ehkedv9R0JfgoTssjnhcyb1wjJS2NWpjyaDCtvu/avDA6OopsNovnnnsOH/vYx/oM5FyzjFJ5Xffr9Xro4oiwMF0kSuDkQRpfkD3KFYZLm47yriY318Co4tHvWq1mNxbha935+/y3di+V6h3rks/nVRucNlEmw6XflAaazKHrnufZxS6dTgeZTKZvkoQa4Nramt1HltuSNzc30W63fZ4cYfVJdgquEZeEfDas7gaVtYtcg0iX5AqrQ65nwshX/qdOljxgjDHW3soJw0VK3BwF+Jef0zPkS847de5hceDAAUxPT9sFMW+88Qbm5+cB3OkYeVpcxMrlcJUbz4Mg7TZOO6bydHW+3AwShkDS7Xa7+MIXvoBPfvKTfbZWyhQqyDANMeg+t+XtRosM03bpN7frhtl2XQgq7LBrWhg8j0gb5w23UChYs0AU4iWk02mMjY31zRDz4b/nec4hkiw7uf8pge86lcvlsL29jcXFRaytrcHzPORyObtblpZ3ZHYYHR3tGya6Rkyu9MtGqplJJLTGGVSe1DkGabpRlJBBidd1T0u/Jg95rPDJIS18MhFQGjmRclSrVayvr+PYsWPWNZE69Ha7bd1EaUHFG2+8gSeffNL6vnJ/XZ4OrtlKMnaVVZz2HPVZubWslk9Rwwwk3ZdffhkXL170ES5PNK0rDyJVz+st6xweHlYN+7RyJopmE4SgTJcVhAgtjm+dy87DMYgGrBUWVULeiWSzWUxNTVkSk/Hyby4L3/RFVloKn+LQ/IVleFJ7kI0D6O2f8I1vfMOXNtoRq1gs+k6JIFMKuUONjIz4NilxNXLKuyBNkP+Oqt2GabUaXBqta6VXXIQRr4wj7Br97na79piloLhdyoHneXaEWiwWsbi4aP2U+eiX2v3MzIztAG/cuIGrV6/igQceANCri3zLAc1kRTJrXgham4/SZqOAm9U08InoKEpcIOlqO+tQgGQgp+sueJ5nh5Pj4+O+/QSIcMPCiAoi0m63G2rc5qemRrXrEaIWoCx4PvTQiJPLwntPejabzWJ8fNxu/iPfl79TqVTfgYOcPPnznudZjUZz8dN+87Rxj4ehoSEcOHDAylgsFn0HFG5tbeHcuXP2PUrnlStXYEzPr5vMQKQtSQ8LLncYuPM9T3uccg8iPZfmNUg8LoTVOZfWK2WXnSaAQH9irYPlz/KRErkNkpfPxsYGqtUqqtUqXn/9dTzzzDO+SbPz58/j/vvvhzF3NhsnBUNOfLlsvUEKENdOd9vx8fyifOHlzttvGCKvSONaEmWQFCQI7XYbm5ubGBkZQTabtduuxQkjDJK0XJUQuGNH4g16r+WQ1yhelw1QVnCtMRcKBXv6bq1Wc+4XSuTF49MIk/s/UuVuNps+spY2Mtn4PM/D+vq67ezGxsZw6tQpdV+ITqeD9fV1Kwc3VywvL2NjYwMPPfQQZmZmrPatzTyTrFEquTQvxCXEoLIM04BcYUjsRd0LIqAoJgetfmjKAX13u12sr6/bcMjLZmtrC5/73OfwyiuvoFwu28UjFC49v76+3ncsPI+f25Lld5SR7V61ZwDWZKKB54nm4dAXVpQIOQnQGWWDJKjT6aBUKvl26tkrDZfAh8xB69WBO1snhjl58zBkgUYZjnKCIxllWHIoKuWRjSOfzyOXy9kOkGaJKQzuT6tpBhpxEinevHnTLvfmcWuNlWSnlWZA77SPN998E8ePH/c57He7XVy6dMna9TStutVq4ezZs3Z3qqmpKZ+NELgzKx9V0w2yx8n0aPe0/9oQWCJI+3ERxd2CSwnR6kDYO1QWpVIJ29vb8DwPGxsb+OxnP4tPf/rT9pQXoFfX9+/fj6mpKRw7dgxf/epX7Qj3ypUrWF5extzcnM0n2ouYH7IJ6D68gJ5vQfU1Lri5w6UscezavAD4M3k3R1/w8KRv6F6CD0vDiJ2eJRtjVI3HVTG1NPFrfN9Q/o42OaXJrv3mG6PUajW7ixvd12zGPB7yKuCoVqu4evUq5ufn7SSctFvV63VsbW3ZbT2JSKmx3Lx5E9Vq1boKeZ6HpaUlLC4u9uW/BGlQ29vbmJqasp30ysqKrT+lUsnuuhYGSS4Ut6bZyfyJErbrukYMUTrroHCjyBNkCgka/UkCDorD8zzfBFuj0cDy8jJGR0ftdpz79u3Dk08+ie/4ju/AyMiI9Vq4ceMGgJ4XzJkzZ/C93/u9faM84I4JUGq6mhIRlN445emCix9cdSgIoVs7UiWnbQl3q5nSMITvybkX0LRdTROSmcc9BTTNJa6MLtMCJyuaVHLJr1UsDhchS/ldM6rab16pgd6y3mvXrtnjR2h1D2mZ3OOEkzbXXNfX1/HSSy/hoYceQr1ex4ULF2KZlGq1Wt+JA/RuVC0X8JdJFNOCq8xl5yqHkpJQtf9BBBFFht0iigw8b3ley85ra2vLKlCjo6N4/PHH8ZM/+ZM4deqUrR/kz720tISnnnoKS0tLvrR+/etfR7VaxRNPPIGRkRGUSiW8/vrrltD5mX5XrlzxySnlJgSNTONC6wx2i1BNN+qEWRzQbPVeEy8Ht9UStJ6ctK04w1WOoAZKH7kGnOywfIc0/k2IQrzyv2wkEi7C1Z6l2W1jjF24QnnIG69r2TDQ05rPnDljw5MyaDvK0TOue4M0JG5LjEK8rjj5fz4pLL+1OIKGwlE19nuFIHlJDr78eXh4GPv27UOxWMTt27dx5swZPPfcc7bTLJfL1hTB51A6nQ6ef/55vPnmmzhy5Ig1TVDctF9Dt9tFpVLB0NDQQPmwG55xmRfCzDUuhJ4csdeESyD/P+3oj0ERxcam/ZcTakFhBMWtNUzpsUDh8rXyQbKF3dOepUrKbchauqTGGgQXgdfrdd+y4mazaV2GCHz/ZRl/mI93kDxR3wsbXoY1FE1zku+58ifI9BQm173SfrW46FpQvlDZUT6srKzgT//0T3Hu3Dmsrq4639cmILe2tnD16lU7sqL2KJWhuOW+G/ARjRZenA6TI5J54W6BtJkoxxaHwTUM4L9dhCqHn0Hhh2Uw126DVnoZc2fNuVxvHka4UlvgYdLwX3YCUbRp7ZoclvN7tVrNai8EmmSVp6LKeGjybzf1K8q7vGPltvswGy9dC0IU0nWFE0S4gzZm+X4YeNnGMa3RM3zksrm5iaeffjq0TPkpMxxExESyVI9kXuxlZxsHcerprjTdewEaYhD57LaiBRVKEJmFeTpocBExH8rSf5JLG5ZHsfPJ30S82j25b7BGDFyLCEqbvMY1Wkm4BFo0UygU1I6n2Wz6dn8aFGFlJAkkjGijkKwMK6iuucgzrMzjtoHdaMTcDBb0jKxDpNnykVuUuKiNS22X12du96f84NrmXmq6YWagoE51UHzTSRe4Q7wA9oR4Xf81bYZrQrIAJFzDTPrvqsBanNylTZJxmPZJz/Ln5YGbLlLg6aM0k1YRRZsnbcS1nJdA9mCaNKRn6/V6pFOkg9wSd9sQotpx5TtBsgTdk414Lwg3TmcRBHpXTpJLJYZroLTYiVajUV3STu6ge4BfsZHtRJv85ooKgNBJ2KByDavTrue0zjoMu9J0ecbeK0g7aBhce7CGQSMjbRMcV+XWCoe+NQdpmjzjK8k48cWRVbsvzQ1Bq8okuMbdbDaxtrYWWAbtdtu3I5wLxvSOjqedrcj0EQY+vAySeWtrKzQsGSb9lveCoJVPKpVS937mz9O8hdxof7da015qXdTp3rp1y5fnskOn1Wa8/NbX11Gv1+2ydT5yosVHclTAZef1IZPJ+LhGGylIso8CrtAEKTVheRSlY6NnwkZxJkiAYrHojY2NAfBnQtAwnWOQYU/QM3yPBj5En5yctMe8SI2TD2XoU6vVcOvWLadL1cjICN71rndhbm4OGxsbaDabviOLjDF2ApAIj3/TdTpipdvt4urVq3jttddC058gQYK3PzzPcxJZ6MkRcvWYRr5hw2ENUdV9Ap0yq6FQKNj1+hrZahvc0GotjXgbjQZeeOEF3HfffXjooYesRkof2puUhk1kHyXZiXDpcMF2u421tbXA/EiQIMG3B0JPjpAqeRQVPYr6HscuI8nRFU5UmxgnZleH0W63ceHCBSwtLeHRRx/FzMyM084r9yQl7VYb2iRIkODbG6HG06AJojhG/ygf17O7dS2SoEP8XB0IT9f29jaee+45vPXWW3YTcS0/SFbuI8sn6fbKFzlBggRvbwQygYto5fUg8o076+ci4b3E2NgY5ufn+9Ii08QN4+fOncPXv/51LC0tOeWSZEs24N36ICdIkOA/HkTSdOXeBPxeFDcjiSgaL39uL2GMweTkJN73vvdZO3DY8/RMuVzGc889hzfffNO34bIEt/0mGm6CBAk4YpkX5LWgd1yIau/VSDgoLpeJQOsUWq0Wpqam8OCDD4bKwsMzxqBer+Oll17Cl7/8ZVy6dMnuIZHNZu1qG+4isxebBCVIkOA/HoSSLtkwNW03aHgeB2HabhTE8XWlc5mOHTvm2+9VhhfUuaysrOCpp57CM88849ugGYDPritNDgkSJPj2RuRNzIH+Uw/kwgGNWPj1OE7pQe5oLsSZ2AN6x8rI4b8xBkeOHMF73/teeJ6Hs2fP4o033ujzjCBH8YsXL2JpaQkPPPCA1Zy1zZYT0k2QIAEQccMbudqK+6TyZwdd8REVQS5e0qVNe5dAm7GPjo5iamrKd7R8Lpezx1TTZFun08GFCxfU8DzPswcxplIp3H///X1mhYRwEyRIQIg0yyOH/WGn6LqG5XE1UZfm7Aovqi2ZzAv5fB5zc3P2Oi16qFQq+NKXvoS/+qu/wtbWFg4ePBhqhvA8z+5HkHgsJEiQwIVIx/UQuMarERxpu1zrdGl5YavY4qxo48Qn5dKu8fPIpqam7HXyOKAFDktLS1hfX8fQ0BAymUzo3rPlctl3Rhs/9DFBggQJgIgTafy3a5VVkO/ubkknTKMNMy3Ie7lcTtVEp6en8SM/8iM4ceKEJfFms4mtra1Irl/Dw8M+sr0bLm8JEiR4eyPUpsu/CRqhuibVuAaqfe8FBnVfM8Zg3759dlu6YrGIU6dOYf/+/bh9+3aknbRkeLQEmHZQoi0cE/JNkCABEFHT1bwOOKm6lgqHuZLJiTgtbu3Z3YL70U5PT9ujxulesVjEww8/7JPDtdBB82igw/jk0uAECRIkiLxcKsiXliPIvzWIhOP42cZ5zwV6b3h42B7lzX2R77vvPt+JvTLN0j+ZVrq54kk03QQJEgADarravb1cJqzFuxeariYf37icxymPmglLizGm71j1BAkSJJCIfFxPFE0zaIIrzMarPafFtddEnM1mkcvlYIzB6uoq1tbWsL29jVKpZO2yrhMHJDQTQuK9kCBBAo6BXMbC7oeRb5xJNTk8l2G7/GddadFWoNG1SqWCz3zmM9je3lYPbgwj0ETTTZAgQRgieS+4/rvI1fVMFK+FMG2XI5/Px5qgci1JpkmybreLjY0N514Jg2qsiT03QYIEhD2x6bqeCfKvjWPrlWGnUikUi8VdaZak3abTaRw5csTGsxuCTMwICRIkCMOeei9EJeAw74WwCblCoTDQMluXzNxEEZd4pdzcR1fboyJBggTf3ohk03WRRpi5gV8PWhocdaEEd+sKey7selQTRpw4yCzBfXQTJEiQgGOgDW+iPKc9H7acN2hZMV3L5XI2rt2A3m+327h9+3bfdQmShzYs56cB03ubm5t97meJlpsgQQKO6FP/O9irRQpRlwPz+1LLjepZIK93Oh3U63U7gUbeCtrz/Pj2brfrO3Zdyl0qlexqNE6+CfEmSJCAEGk/XSD+ijFNS3W5jmnXNC8GIjxNpiD5JPHVajVUq1V0u11cvXoVq6urfa5vRLaZTMbnPtZqtXwyRF0inSBBggTALhdHhBGdhij2W9dEm/SxDTN5uORrNptYXV0FAFy+fNmn5Y6MjGBqagqFQgHFYhFjY2PodDpoNptYXl7G8vIypqamkE6n4Xm9PXQ3NjasPNoOY8neugkSJCDENi9w7MbUoGmyQQRKhBtnAsw1tO92u9jc3EQqlUKz2fTFceLECYyMjFj5ut0ujDHI5/PWlnvgwAHkcjl4noeVlRVsbGzYZ6rVal866NkECRIk2PWKtLB3tPfCvBnkc8aYPm0xLolxzbPT6aBSqdi9FwjZbBae56lbOnpeb2/dTCaDRqNhzQ18Y/NOp+N7l2Rvt9s+ck+QIMG3L2JpuoNqtlHec832h7l5uXxvpaYKwEewhFKpZM0LmUwGrVbL2m0lOp0OUqkU2u022u229fGl/XKbzaZzUx1XmAkSJPj2wq7MC4S9WjLLbbZyma4Wn0baWvxcY5bhcJ/aTCYTqJGm02m0223U63UbFtd0W62Wmu6EdBMkSECIZV4ABjcxxHlXvkOapGuvh2636yO1oHj4PdpBjDTdVCoVSI6dTgfGGB+5EmmT+UN7n8g9QYIECfbET3c3RCzDkVqsy5TAJ9/49otRZaJhvzRrEGlqtmZ6jhMoETHQ03q1eBPSTZAgASHWLmNxnhtUqyWyi7L6jdDtdi3pxlkiLEk3lUr1bedI7/BJOGlGSKfTaLVadqKNy0jvJqSbIEEC4C55L8j3orzvWuggNU6XC9ggm8tw0wL53YYds04EyuOhCTh5nb+TnJGWIEECYA+8F3ZLxK4womyywz0UgkhNI24+iQbAarlR4IqLk7+UOSHdBAkSAHvgvbAXpoXduqJp2mfUeIkMjTGRiFEjUPpP7mlBk34JEiT49kZk88Ld0mgHCUeGJZfdRg1bfqKSrgva+1wbT5AgQYKB914YZKJsrybcKCzuwTAIqZFZgibwdku6mhx8cjBBggQJBjYv7AUJa+HECSuulipBm9Nks9nIpOsKh761hRwJ6SZIkICwJyvSgL3XYl1huVajDUKY3ONhN8N/blvWwqHFHQkSJEhggsggnU573+rHipP8cY5i5+96Xv+x7INA03I5kk1vEiT49oHneU6NM5B0EyRIkCDB3mL3Kl6CBAkSJIiMhHQTJEiQ4B4iId0ECRIkuIdISDdBggQJ7iES0k2QIEGCe4iEdBMkSJDgHuL/Bzt0zf+YdyUZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntransform = transforms.Compose([\\n    transforms.Pad(12, padding_mode='reflect'),\\n    transforms.ToTensor()])\\n\\ntrainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        path, dirs, files = next(os.walk(self.root_dir))\n",
    "        file_count = len(files)\n",
    "        return file_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path, dirs, files = next(os.walk(self.root_dir))\n",
    "        img_name = os.path.join(self.root_dir, files[idx])\n",
    "        image = io.imread(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        if h!= new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top = 0\n",
    "        if w!= new_w:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0\n",
    "     \n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        \n",
    "        image = np.expand_dims(image, axis=2)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class Downsample(object):\n",
    "    \"\"\"Downsample the image\n",
    "\n",
    "    Args:\n",
    "        downsampling_factor (int or tuple): Desired downsampling factor for rows and columns.\n",
    "        If the downsampling factor is an int, then both rows and columns are sampled by the same factor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsampling_factor):\n",
    "        assert isinstance(downsampling_factor, (int, tuple))\n",
    "        if isinstance(downsampling_factor, int):\n",
    "            self.downsampling_factor = (downsampling_factor, downsampling_factor)\n",
    "        else:\n",
    "            assert len(downsampling_factor) == 2\n",
    "            self.downsampling_factor = downsampling_factor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        down_fact_h, down_fact_w = self.downsampling_factor\n",
    "        image = image[::down_fact_h,\n",
    "                      ::down_fact_w]\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class ConvertDepthToColor(object):\n",
    "    \"\"\" convert a 1xmxn 16-bits depthmap to a 2xmxn 8-bits colormap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if len(image.shape[:]) <3:\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            \n",
    "        h, w = image.shape[:2]\n",
    "        image_r_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color= np.zeros((h,w,1), dtype=int)\n",
    "        image_g_color[(image > 2**8 - 1)] = image[(image > 2**8 - 1)] >> 8\n",
    "        image_r_color = image - (image_g_color <<8)\n",
    "\n",
    "        return np.concatenate((image_r_color, image_g_color), axis=2)\n",
    "    \n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in images to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, images):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        images = images.transpose((2, 0, 1))\n",
    "        images = images.astype(float)\n",
    "        return torch.from_numpy(images)\n",
    "    \n",
    "\n",
    "    \n",
    "class ConvertColorToDepth(object):\n",
    "    \"\"\" convert a 2xmxn 8-bits colormap to a 1xmxn 16-bits depthmap\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, color_image):\n",
    "            \n",
    "        depth_image = color_image[:, 0, :, :]\n",
    "        depth_image += color_image[:, 1, :, :] << 8\n",
    "\n",
    "        return depth_image\n",
    "\n",
    "    \n",
    "def show_image_batch(images_batch):\n",
    "    \"\"\"Show image for a batch of samples.\"\"\"\n",
    "    if images_batch.size(1) == 1:\n",
    "        images_batch_normed = images_batch/torch.max(images_batch)\n",
    "        grid = utils.make_grid(images_batch_normed)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    else:\n",
    "        print(images_batch.size())\n",
    "        images_b_batch = torch.zeros(images_batch.size(0), 1, images_batch.size(2) , images_batch.size(3))\n",
    "        images_color_batch = torch.cat((images_batch, images_b_batch), 1) \n",
    "        print(images_color_batch.size())\n",
    "        grid = utils.make_grid(images_color_batch)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Load training dataset\n",
    "\n",
    "#mirror_padding_transform = transforms.Compose([transforms.ToPILImage(), transforms.Pad(padding=12, padding_mode='reflect'), transforms.ToTensor()])\n",
    "transformed_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/training/dilated', \n",
    "                                           transform=transforms.Compose([\n",
    "                                            RandomCrop((384, 640)), Downsample(( 3, 5)), ToTensor()])\n",
    "                                  )\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=18, shuffle=True, num_workers=0)\n",
    "\n",
    "for i_batch, batch_images in enumerate(dataloader):\n",
    "    print(i_batch, batch_images.size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 2:\n",
    "        plt.figure()\n",
    "        show_image_batch(batch_images)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(12, padding_mode='reflect'),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.Flickr8k(root='./data/flickr', ann_file = './data/flickr/file.csv', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GaussianDistribution(torch.autograd.Function):\\n    @staticmethod\\n    def forward(ctx, input, var, phi, nScale, nChannel):\\n        batch_size = input.size(0)\\n        h = input.size(-1)\\n        w = input.size(-2)\\n        ctx.save_for_backward(input, var, phi)\\n        #print(\"input : \", input)\\n        coeff = torch.sqrt(1.0 / (2 * np.pi * var))\\n        #print(\"coeff : \", coeff)\\n        input_resized = input.repeat((nScale, 1, 1, 1, 1))\\n        #print(\"input : \", input_resized)\\n        exponent = (-0.5*(input_resized ** 2)/var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w))\\n        #print(\"exponent : \", exponent)\\n        coeffs_resized = coeff.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\\n        #print(\"coeffs : \", coeffs_resized)\\n        gaussian = coeffs_resized * torch.exp(exponent)\\n        #print(\"gaussian : \", gaussian)\\n        phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\\n        phi_gaussian = phi_resized*gaussian\\n        sum_phi_gaussian = phi_gaussian.sum(dim=0)\\n        #print(\"sum over scales : \", sum_phi_gaussian)\\n        result = -torch.log2(sum_phi_gaussian).sum()\\n\\n        return result\\n\\n    @staticmethod\\n    def backward(ctx, grad_output):\\n        input, var, phi = ctx.saved_tensors\\n        batch_size = input.size(0)\\n        h = input.size(-1)\\n        w = input.size(-2)\\n        nChannel = var.size(1)\\n        nScale = var.size(0)\\n        \\n        \\n        var_resized = var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\\n        phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\\n        input_resized = input.repeat((nScale, 1, 1, 1, 1))\\n        \\n        d_phi = -torch.log2(1.0/torch.sqrt(2.0*np.pi*var_resized)*(torch.exp(-0.5*input_resized**2/var_resized))).sum(dim = [3, 4])\\n        d_var = -torch.log2(phi_resized*(1.0/(np.sqrt(2.0*np.pi)*var_resized)*torch.exp(-0.5*input_resized**2/var_resized))*(1.0 + input_resized**2/var_resized)).sum(dim = [3, 4])\\n        d_xq = -torch.log2((phi_resized*(-input_resized/torch.sqrt(2.0*np.pi*var_resized**3))*torch.exp(-0.5*input_resized**2/var_resized)).sum(dim=[0]))\\n        print(\"d_xq : \", (phi_resized*(-input_resized/torch.sqrt(2.0*np.pi*var_resized**3))*torch.exp(-0.5*input_resized**2/var_resized)).sum(dim=[0]))\\n        \\n        return grad_output*d_phi, grad_output*d_var, grad_output*d_xq\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define additionnnal functions\n",
    "def periodic_shuffling(T, C):\n",
    "    T_copy = T.clone()\n",
    "    batch_size = T.size()[0]\n",
    "    H = T.size()[2]\n",
    "    W = T.size()[3]\n",
    "    T = T.view(batch_size, C, H*2, W*2)\n",
    "    \"\"\"\n",
    "    for k in range(C):\n",
    "        for i in range(2*H):\n",
    "            for j in range(2*W):\n",
    "                T[:, k, i, j] = T_copy[:, C*((j&1)<<1)+C*(i&1)+k, i>>1, j>>1]\n",
    "    \"\"\"\n",
    "                \n",
    "    T[:, :, ::2, ::2] = T_copy[:, 0:C, :, :]\n",
    "    T[:, :, 1::2, ::2] = T_copy[:, C:2*C, :, :]\n",
    "    T[:, :, ::2, 1::2] = T_copy[:, 2*C:3*C, :, :]\n",
    "    T[:, :, 1::2, 1::2] = T_copy[:, 3*C:4*C, :, :]\n",
    "\n",
    "    return T\n",
    "    \n",
    "    \n",
    "def mirror_padding(x, padding_size):\n",
    "    up_line = x[:, :, 0:padding_size, :].flip(2)\n",
    "    left_col = x[:, :, :, 0:padding_size].flip(3)\n",
    "    right_col = x[:, :, :, -padding_size:].flip(3)\n",
    "    bottom_line = x[:, :, -padding_size:, :].flip(2)\n",
    "    left_up_corner = left_col[:, :, 0:padding_size, :].flip(2)\n",
    "    right_up_corner = right_col[:, :, 0:padding_size, :].flip(2)\n",
    "    left_bottom_corner = left_col[:, :, -padding_size:, :].flip(2)\n",
    "    right_bottom_corner = right_col[:, :, -padding_size:, :].flip(2)\n",
    "\n",
    "    x_mirror_pad = torch.cat((torch.cat((left_up_corner, up_line, right_up_corner), 3), torch.cat((left_col, x, right_col), 3), torch.cat((left_bottom_corner, bottom_line, right_bottom_corner), 3)), 2)\n",
    "    return x_mirror_pad\n",
    "\n",
    "\n",
    "def get_zero_mask(x):\n",
    "    return(x > 0)\n",
    "\n",
    "\n",
    "def black_pixels_removal_by_dilatation(x, se):\n",
    "    # se must be a rectangle element\n",
    "    if x.is_cuda:\n",
    "        se = se.cuda()\n",
    "        zero_tensor = torch.zeros(1, dtype=torch.int64).cuda()\n",
    "        one_tensor = torch.ones(1, dtype=torch.int64).cuda()\n",
    "    else:\n",
    "        zero_tensor = torch.zeros(1, dtype=torch.int64)\n",
    "        one_tensor = torch.ones(1, dtype=torch.int64)\n",
    "        \n",
    "    se_height = se.size(0)\n",
    "    se_width = se.size(1)\n",
    "    zero_indices =  torch.nonzero((x == 0))\n",
    "    row_ind = torch.stack([torch.max(zero_tensor, zero_indices[:, 2] - se_height//2), torch.min(one_tensor*x.size(2), zero_indices[:, 2] + se_height//2)], dim=1)\n",
    "    col_ind = torch.stack([torch.max(zero_tensor, zero_indices[:, 3] - se_width//2), torch.min(one_tensor*x.size(3), zero_indices[:, 3] + se_width//2)], dim=1)\n",
    "    x_with_reduced_black_px = x.clone().detach()\n",
    "    for k in range(zero_indices.size(0)):\n",
    "        x_with_reduced_black_px[tuple(zero_indices[k, :])] = torch.max(x[int(zero_indices[k, 0]), int(zero_indices[k, 1]), row_ind[k, 0]:row_ind[k, 1]+1:1, col_ind[k, 0]:col_ind[k, 1]+1:1])\n",
    "        \n",
    "    \n",
    "    if torch.any(x_with_reduced_black_px==0):\n",
    "        print(\"+1\")\n",
    "        x_with_reduced_black_px = black_pixels_removal_by_dilatation(x_with_reduced_black_px, se)\n",
    "    \n",
    "    return x_with_reduced_black_px\n",
    "\n",
    "\"\"\"\n",
    "def black_pixels_removal_by_dilatation(x):\n",
    "    if x.is_cuda:\n",
    "        kernel_tensor = torch.ones(1, 1, 109, 119).cuda()\n",
    "    else:\n",
    "        kernel_tensor = torch.ones(1, 1, 109, 119)\n",
    "    mask_of_zero = (x==0)\n",
    "    torch_result = torch.clamp(torch.nn.functional.conv2d(x, kernel_tensor, padding=(54, 59)), 0, 2**16-1)\n",
    "    x_with_reduced_black_px = x.clone().detach()\n",
    "    x_with_reduced_black_px[mask_of_zero] = torch_result[mask_of_zero]\n",
    "\n",
    "    \n",
    "    if torch.any(x_with_reduced_black_px==0):\n",
    "        x_with_reduced_black_px = black_pixels_removal_by_dilatation(x_with_reduced_black_px)\n",
    "    \n",
    "    return x_with_reduced_black_px\n",
    "   \n",
    "\"\"\"\n",
    "def normalize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    max_value = torch.max(x)\n",
    "    min_value = torch.min(x)\n",
    "    radius = max(max_value, abs(min_value))\n",
    "    x_centered_normalized = x_centered/radius\n",
    "    return x_centered_normalized, radius, mean_channels\n",
    "\n",
    "def standardize_input(x):\n",
    "    mean_channels = torch.mean(1.0*x, [2,3])\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_centered = x - mean_channels_images\n",
    "    var = torch.sum(x_centered**2, (2, 3))/(x.size()[2]*x.size()[3])\n",
    "    x_standardized = x_centered / torch.sqrt(var.view(x_centered.size()[0], x_centered.size()[1], 1, 1))\n",
    "    return x_standardized, mean_channels, var\n",
    "    \n",
    "def denormalize_output(x, radius, mean_channels):\n",
    "    x_denormalized = x*radius\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_denormalized_centered = x_denormalized + mean_channels_images\n",
    "    return x_denormalized_centered\n",
    "\n",
    "def destandardize_output(x, mean_channels, var):\n",
    "    x_rescaled = x*torch.sqrt(var.view(x.size()[0], x.size()[1], 1, 1))\n",
    "    mean_channels_images = (mean_channels.repeat_interleave(x.size()[2]*x.size()[3])).view(x.size()[0], x.size()[1], x.size()[2], x.size()[3])\n",
    "    x_destandardized = x_rescaled + mean_channels_images\n",
    "    return x_destandardized\n",
    "\n",
    "\n",
    "\n",
    "def compute_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    return gsm\n",
    "\n",
    "\n",
    "def sum_gsm(x, var, phi, nScale):\n",
    "    gsm = 0.0\n",
    "    \n",
    "    phi = torch.abs(phi)\n",
    "    var = torch.abs(var)\n",
    "    phi_s_sum = torch.sum(phi, 0).unsqueeze(0)\n",
    "    phi_norm = phi/phi_s_sum\n",
    "    \n",
    "    for s in range(nScale):\n",
    "        var_s = var[s, :].view(1, -1, 1, 1)\n",
    "        phi_s = phi_norm[s, :].view(1, -1, 1, 1)\n",
    "        gaussian = phi_s*(1.0/(torch.sqrt(2*np.pi*var_s)))*torch.exp(-0.5*(x**2/var_s))\n",
    "        gsm += gaussian\n",
    "    #gsm_sum = (torch.log2(gsm)).sum()\n",
    "    gsm_sum = gsm.sum()\n",
    "    return gsm_sum\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    h = x.size(-1)\n",
    "    w = x.size(-2)\n",
    "    #print(\"input : \", input)\n",
    "    coeff = torch.sqrt(1.0 / (2 * np.pi * var))\n",
    "    #print(\"coeff : \", coeff)\n",
    "    x_resized = x.repeat((nScale, 1, 1, 1, 1))\n",
    "    #print(\"input : \", input_resized)\n",
    "    exponent = (-0.5*(x_resized ** 2)/var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w))\n",
    "    #print(\"exponent : \", exponent)\n",
    "    coeffs_resized = coeff.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    #print(\"coeffs : \", coeffs_resized)\n",
    "    gaussian = coeffs_resized * torch.exp(exponent)\n",
    "    #print(\"gaussian : \", gaussian)\n",
    "    phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "    phi_gaussian = phi_resized*gaussian\n",
    "    sum_phi_gaussian = phi_gaussian.sum(dim=0)\n",
    "    #print(\"sum over scales : \", sum_phi_gaussian)\n",
    "    result = -torch.log2(sum_phi_gaussian).sum()\n",
    "\n",
    "    return result\n",
    "    \"\"\"\n",
    "    \n",
    "def compute_mask(nb_ones, dims):\n",
    "    mask = torch.zeros(dims)\n",
    "    indices = np.arange(nb_ones)\n",
    "    mask_flatten = mask.view(-1, 1, 1, 1)\n",
    "    mask_flatten[indices] = 1\n",
    "    mask_reshaped = mask_flatten.view(dims)\n",
    "    return mask_reshaped\n",
    "\n",
    "\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    integral_u = torch.trapz(gsm_sum, u)\n",
    "    #print(\"gsm sum : \", gsm_sum)\n",
    "    #print(\"integral over u : \", integral_u)\n",
    "    entropy = -torch.log2(integral_u)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def mean_bit_per_px(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.1, 0.1).cuda()   \n",
    "    gsm_stacked = []\n",
    "    #u_stacked = []\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        gsm_stacked.append(gsm)\n",
    "        #u_stacked.append(torch.ones(gsm.size()).cuda()*u[i])\n",
    "    \n",
    "    gsms = torch.stack(gsm_stacked, dim=0)\n",
    "    #us = torch.stack(u_stacked, dim=0)\n",
    "    integral_u = torch.trapz(gsms, dx=0.1, dim=0)\n",
    "    if torch.any(integral_u.isnan()):\n",
    "        print(\"integral u is nan\", integral_u)\n",
    "        integral_u[integral_u.isnan()] = 1\n",
    "    nb_bits = (-torch.log2(torch.clamp(integral_u, min=np.exp(-10**2), max=1))).sum()\n",
    "    if nb_bits.isnan():\n",
    "        print(\"nb bits is nan\")\n",
    "    if nb_bits < 0:\n",
    "        #print(\"integral u : \", integral_u)\n",
    "        print(\"nb_bits negative : \", nb_bits)\n",
    "    return nb_bits/reduce(lambda x, y: x*y, list(x_quantized.size()))\n",
    "\n",
    "\n",
    "def distortion_pc(x, x_reconstructed, focal_length, skew, scaling_factor, image_size, principal_point):\n",
    "    # compute intrinsic matrix and its inverse\n",
    "    K = torch.tensor([[focal_length, skew, principal_point[0]], [ 0.0, focal_length, principal_point[1]], [0.0, 0.0, 1.0]])\n",
    "    K_ext = torch.eye(4)\n",
    "    K_ext[0:3:1, 0:3:1] = K\n",
    "    K_ext_inv = torch.inverse(K_ext).cuda()\n",
    "    \n",
    "    size_batch = x.size()[0]\n",
    "    total_mse = 0.0\n",
    "    for example in range(size_batch):\n",
    "        # compute x,y,z coords from u,v coords and gray-level value\n",
    "        z_reconstructed = x_reconstructed[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        z = x[example, :, :, :].view(image_size[0]*image_size[1])/scaling_factor\n",
    "        bool_matrix = torch.logical_and(z_reconstructed != 0 , z != 0)\n",
    "            # for reconstructed depthmap\n",
    "        v_reconstructed = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v_reconstructed = v_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        u_reconstructed = (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u_reconstructed = u_reconstructed[bool_matrix].view(1, -1).cuda()\n",
    "        z_reconstructed = z_reconstructed[bool_matrix].view(1, -1)\n",
    "        homogeneous_reconstructed_camera_points = torch.cat((u_reconstructed, v_reconstructed, torch.ones(1, u_reconstructed.size()[1]).cuda(), 1.0/z_reconstructed), 0)\n",
    "        homogeneous_reconstructed_coords = z_reconstructed*(torch.mm(K_ext_inv,homogeneous_reconstructed_camera_points))\n",
    "        coords_reconstructed = homogeneous_reconstructed_coords[0:3:1, :]\n",
    "            # for original depthmap\n",
    "        v = (torch.arange(1, image_size[0]+1)).repeat_interleave(image_size[1])\n",
    "        v = v[bool_matrix].view(1, -1).cuda()\n",
    "        u =  (torch.arange(1, image_size[1]+1)).repeat(image_size[0])\n",
    "        u = u[bool_matrix].view(1, -1).cuda()    \n",
    "        z = z[bool_matrix].view(1, -1)\n",
    "        homogeneous_camera_points = torch.cat((u, v, torch.ones(1, u.size()[1]).cuda(), 1.0/z))\n",
    "        homogeneous_coords = z*(torch.mm(K_ext_inv,homogeneous_camera_points))\n",
    "        coords = homogeneous_coords[0:3:1, :]\n",
    "        \n",
    "        print(\" x original : \", torch.squeeze(z)[10]*scaling_factor)\n",
    "        print(\" original coords : \", coords[:, 10])\n",
    "        \n",
    "        # compute MSE on points\n",
    "        total_mse += torch.sum(torch.sqrt(torch.sum((coords_reconstructed - coords)**2, 0)))\n",
    "    return total_mse\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.05, 0.05).cuda()   \n",
    "    sum_log_gsm = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm = compute_gsm(x, var, phi, 6)\n",
    "        sum_log_gsm[i] = (-torch.log2(gsm)).sum()\n",
    "    \n",
    "    entropy = torch.trapz(sum_log_gsm, u)\n",
    "    if entropy < 0:\n",
    "        print(\"negative entropy\")\n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def clip(x):\n",
    "    x_n = (x - torch.min(x))/(torch.max(x) - torch.min(x))\n",
    "    x_clipped = torch.round(255*x_n).float()\n",
    "    return x_clipped\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MyQuantization(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "        \n",
    "        \n",
    "class MyClipping(torch.autograd.Function):\n",
    "  \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.round(input).clamp(min=0, max=2**16-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return grad_output\n",
    "\n",
    "    \n",
    "    \n",
    "class MyBilateralFilter(nn.Module):\n",
    "    def __init__(self, n_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyBilateralFilter, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernal_size_number = kernel_size * kernel_size\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.conv = Parameter(torch.Tensor(2, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        width = self.calculateNewWidth(x)\n",
    "        height = self.calculateNewHeight(x)\n",
    "        result = torch.zeros(\n",
    "            [x.shape[0] * self.n_channels, width, height], dtype=torch.float32, device=device\n",
    "        )\n",
    "        windows = self.calculateWindows(x)\n",
    "\n",
    "        for channel in range(x.shape[1]):\n",
    "            for i_convNumber in range(self.n_channels):\n",
    "                xx = torch.matmul(windows[channel], self.conv[i_convNumber][channel])\n",
    "                xx = xx.view(-1, width, height)\n",
    "                result[i_convNumber * xx.shape[0] : (i_convNumber + 1) * xx.shape[0]] += xx\n",
    "\n",
    "        result = result.view(x.shape[0], self.n_channels, width, height)\n",
    "        return result\n",
    "\n",
    "    def calculateWindows(self, x):\n",
    "        windows = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride\n",
    "        )\n",
    "\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernal_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "\n",
    "        return windows\n",
    "\n",
    "    def calculateNewWidth(self, x):\n",
    "        return (\n",
    "            (x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n",
    "            // self.stride[0]\n",
    "        ) + 1\n",
    "\n",
    "    def calculateNewHeight(self, x):\n",
    "        return (\n",
    "            (x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n",
    "            // self.stride[1]\n",
    "        ) + 1\n",
    "    \n",
    "\"\"\"\n",
    "class GaussianDistribution(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, var, phi, nScale, nChannel):\n",
    "        batch_size = input.size(0)\n",
    "        h = input.size(-1)\n",
    "        w = input.size(-2)\n",
    "        ctx.save_for_backward(input, var, phi)\n",
    "        #print(\"input : \", input)\n",
    "        coeff = torch.sqrt(1.0 / (2 * np.pi * var))\n",
    "        #print(\"coeff : \", coeff)\n",
    "        input_resized = input.repeat((nScale, 1, 1, 1, 1))\n",
    "        #print(\"input : \", input_resized)\n",
    "        exponent = (-0.5*(input_resized ** 2)/var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w))\n",
    "        #print(\"exponent : \", exponent)\n",
    "        coeffs_resized = coeff.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "        #print(\"coeffs : \", coeffs_resized)\n",
    "        gaussian = coeffs_resized * torch.exp(exponent)\n",
    "        #print(\"gaussian : \", gaussian)\n",
    "        phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "        phi_gaussian = phi_resized*gaussian\n",
    "        sum_phi_gaussian = phi_gaussian.sum(dim=0)\n",
    "        #print(\"sum over scales : \", sum_phi_gaussian)\n",
    "        result = -torch.log2(sum_phi_gaussian).sum()\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, var, phi = ctx.saved_tensors\n",
    "        batch_size = input.size(0)\n",
    "        h = input.size(-1)\n",
    "        w = input.size(-2)\n",
    "        nChannel = var.size(1)\n",
    "        nScale = var.size(0)\n",
    "        \n",
    "        \n",
    "        var_resized = var.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "        phi_resized = phi.repeat_interleave(batch_size*h*w, 1).view(nScale, batch_size, nChannel, h, w)\n",
    "        input_resized = input.repeat((nScale, 1, 1, 1, 1))\n",
    "        \n",
    "        d_phi = -torch.log2(1.0/torch.sqrt(2.0*np.pi*var_resized)*(torch.exp(-0.5*input_resized**2/var_resized))).sum(dim = [3, 4])\n",
    "        d_var = -torch.log2(phi_resized*(1.0/(np.sqrt(2.0*np.pi)*var_resized)*torch.exp(-0.5*input_resized**2/var_resized))*(1.0 + input_resized**2/var_resized)).sum(dim = [3, 4])\n",
    "        d_xq = -torch.log2((phi_resized*(-input_resized/torch.sqrt(2.0*np.pi*var_resized**3))*torch.exp(-0.5*input_resized**2/var_resized)).sum(dim=[0]))\n",
    "        print(\"d_xq : \", (phi_resized*(-input_resized/torch.sqrt(2.0*np.pi*var_resized**3))*torch.exp(-0.5*input_resized**2/var_resized)).sum(dim=[0]))\n",
    "        \n",
    "        return grad_output*d_phi, grad_output*d_var, grad_output*d_xq\n",
    "\"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entrée : 128*128\n",
    "class LossyCompAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = nn.Conv2d(128, 96, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = nn.Conv2d(96, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = nn.Conv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = nn.Conv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 96))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 96))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(96).view(1, 96, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "        \n",
    "        # batch norm\n",
    "        self.batchNormed = nn.BatchNorm2d(96, momentum = False, affine=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # get zero mask\n",
    "            #dilatation_mask = (x>0)\n",
    "                # removing black pixels\n",
    "            #x = black_pixels_removal_by_dilatation(x)\n",
    "            #if torch.any(x.isnan()):\n",
    "                #print(\"x after dilatation is nan\", x)\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            #x, radius, mean_channels = normalize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"mean channels : \", mean_channels)\n",
    "                print(\"standard deviation : \", torch.sqrt(var))\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.resConv1_1(x))\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weight gradients: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x = F.relu(self.resConv2_1(x))\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "                # residual block 3\n",
    "            x = F.relu(self.resConv3_1(x))\n",
    "            x = self.resConv3_2(x)\n",
    "            x += x_c3\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.deconv1_1(x))\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                   # residual block 2\n",
    "            x = F.relu(self.deconv2_1(x))\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                   # residual block 3\n",
    "            x = F.relu(self.deconv3_1(x))\n",
    "            x = self.deconv3_2(x)\n",
    "            x += x_c6\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = F.relu(periodic_shuffling(x, 256//4))\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            #x = denormalize_output(x, radius, mean_channels)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "                    # replace black pixels\n",
    "           # x = x*dilatation_mask\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "            \n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            # batch normalization\n",
    "            x = self.batchNormed(x)\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                    print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                    print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                    print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                    print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                    print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = F.relu(periodic_shuffling(x, 256//4))\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                    print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                    print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                    print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                    print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Convolutional Autoencoder with integrated classifer\n",
    "    #taille de l'image d'entrée : 128*128\n",
    "class LossyCompAutoencoder_bis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossyCompAutoencoder_bis, self).__init__()\n",
    "       \n",
    "        #Encoder\n",
    "            # input block\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, stride=2, padding=0)  \n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, stride=2, padding=0)\n",
    "            # residual block 1\n",
    "        self.resConv1_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 2\n",
    "        self.resConv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # residual block 3\n",
    "        self.resConv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.resConv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            # output block\n",
    "        self.conv3 = nn.Conv2d(128, 96, 5, stride=2, padding=0)\n",
    "        self.quantization = MyQuantization.apply\n",
    "        #self.gaussian_distribution = GaussianDistribution.apply\n",
    "        \n",
    "\n",
    "       \n",
    "        #Decoder\n",
    "            # subpixel 1\n",
    "        self.subpix1 = nn.Conv2d(96, 512, 3, stride=1, padding=1)\n",
    "            #residual block 1\n",
    "        self.deconv1_1 = nn.Conv2d(512//4, 128, 3, stride=1, padding=1)\n",
    "        self.deconv1_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)    \n",
    "            #residual block 2\n",
    "        self.deconv2_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "            #residual block 3\n",
    "        self.deconv3_1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.deconv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1) \n",
    "            # subpixel 2\n",
    "        self.subpix2 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "            # subpixel 3\n",
    "        self.subpix3 = nn.Conv2d(256//4, 4, 3, stride=1, padding=1)\n",
    "            # clipping\n",
    "        self.clip = MyClipping.apply\n",
    "        \n",
    "        #Bit-rate      \n",
    "        self.var = nn.Parameter(torch.Tensor(6, 96))\n",
    "        self.phi = nn.Parameter(torch.Tensor(6, 96))\n",
    "        self.var.data.uniform_(0, 1)\n",
    "        self.phi.data.uniform_(0, 1)\n",
    "        \n",
    "        # lambda (variable bit-rate)\n",
    "        self.lamb = nn.Parameter(torch.Tensor(96).view(1, 96, 1, 1))\n",
    "        self.lamb.data.uniform_(0.985, 1.015)\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        self.gsm_pi = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        self.gsm_sigma = torch.nn.Parameter(torch.randn(6, 96))\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask= 1, return_xq=False, is_lambda = False):\n",
    "        if not is_lambda:\n",
    "            #encoder\n",
    "                # normalization\n",
    "            x, mean_channels, var = standardize_input(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after normalization is nan\", x)\n",
    "                # mirror padding\n",
    "            x = mirror_padding(x, 14)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mirror padding is nan\", x)\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                x_copy = x.cpu()\n",
    "                show_image_batch(x_copy)\n",
    "            \"\"\"\n",
    "                # input blocks\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_c1 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after input block is nan\", x)\n",
    "                print(\"input weights conv 1 gradient: \", self.conv1.weight.grad)\n",
    "                print(\"input bias conv 1 gradient: \",  self.conv1.bias.grad)\n",
    "                print(\"input weights conv 2 gradient: \", self.conv2.weight.grad)\n",
    "                print(\"input bias conv 2 gradient: \",  self.conv2.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.resConv1_1(x))\n",
    "            x = self.resConv1_2(x)\n",
    "            x += x_c1\n",
    "            x_c2 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv1_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv1_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv1_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv1_2.bias.grad)\n",
    "                # residual block 2\n",
    "            x = F.relu(self.resConv2_1(x))\n",
    "            x = self.resConv2_2(x)\n",
    "            x += x_c2\n",
    "            x_c3 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv2_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv2_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv2_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv2_2.bias.grad)\n",
    "                # residual block 3\n",
    "            x = F.relu(self.resConv3_1(x))\n",
    "            x = self.resConv3_2(x)\n",
    "            x += x_c3\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"residual block 1 weights gradient: \", self.resConv3_1.weight.grad)\n",
    "                print(\"residual block 1 bias gradient: \",  self.resConv3_1.bias.grad)\n",
    "                print(\"residual block 2 weights gradient: \", self.resConv3_2.weight.grad)\n",
    "                print(\"residual block 2 bias gradient: \",  self.resConv3_2.bias.grad)\n",
    "                # output block\n",
    "            x = self.conv3(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after output block is nan\", x)\n",
    "                print(\"output weights gradient: \", self.conv3.weight.grad)\n",
    "                print(\"output bias gradient: \",  self.conv3.bias.grad)\n",
    "                # quantization\n",
    "            x = self.quantization(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after quantization block is nan\", x)\n",
    "                # add mask for incremental training\n",
    "            x = x*mask\n",
    "            x_quantized = x\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after mask is nan\", x)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            w = x_quantized.size()[2]\n",
    "            h = x_quantized.size()[3]\n",
    "            gsm = 0.0\n",
    "            for i in range(6)\n",
    "                mi = torch.flatten(x_quantized),torch.diagonal(self.gsm_sigma[s, :].repeat(w*h, 1).squeeze(0))\n",
    "                gsm += \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            #decoder\n",
    "                # subpixel 1\n",
    "            x = self.subpix1(x)\n",
    "            x = periodic_shuffling(x, 512//4)\n",
    "            x_c4 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 1 is nan\", x)\n",
    "                print(\"subpix 1 weights gradient: \", self.subpix1.weight.grad)\n",
    "                print(\"subpix 1 bias gradient: \",  self.subpix1.bias.grad)\n",
    "                # residual block 1\n",
    "            x = F.relu(self.deconv1_1(x))\n",
    "            x = self.deconv1_2(x)\n",
    "            x += x_c4\n",
    "            x_c5 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 1 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv1_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv1_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv1_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv1_2.bias.grad)\n",
    "                   # residual block 2\n",
    "            x = F.relu(self.deconv2_1(x))\n",
    "            x = self.deconv2_2(x)\n",
    "            x += x_c5\n",
    "            x_c6 = x.clone()\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 2 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv2_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv2_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv2_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv2_2.bias.grad)\n",
    "                   # residual block 3\n",
    "            x = F.relu(self.deconv3_1(x))\n",
    "            x = self.deconv3_2(x)\n",
    "            x += x_c6\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after residual block 3 is nan\", x)\n",
    "                print(\"deconv 1 weights gradient: \", self.deconv3_1.weight.grad)\n",
    "                print(\"deconv 1 bias gradient: \",  self.deconv3_1.bias.grad)\n",
    "                print(\"deconv 2 weights gradient: \", self.deconv3_2.weight.grad)\n",
    "                print(\"deconv 2 bias gradient: \",  self.deconv3_2.bias.grad)\n",
    "                    # subpixel 2\n",
    "            x = self.subpix2(x)\n",
    "            x = F.relu(periodic_shuffling(x, 256//4))\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 2 is nan\", x)\n",
    "                print(\"subpix 2 weights gradient: \", self.subpix2.weight.grad)\n",
    "                print(\"subpix 2 bias gradient: \",  self.subpix2.bias.grad)\n",
    "                    # subpixel 3\n",
    "            x = self.subpix3(x)\n",
    "            x = periodic_shuffling(x, 4//4)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after subpix 3 is nan\", x)\n",
    "                print(\"subpix 3 weights gradient: \", self.subpix3.weight.grad)\n",
    "                print(\"subpix 3 bias gradient: \",  self.subpix3.bias.grad)\n",
    "                    # denormalization\n",
    "            x = destandardize_output(x, mean_channels, var)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after denormalization is nan\", x)\n",
    "                    # clipping\n",
    "            x = self.clip(x)\n",
    "            if torch.any(x.isnan()):\n",
    "                print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "            if return_xq:\n",
    "                return x, x_quantized\n",
    "            else:\n",
    "                return x\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #encoder\n",
    "                    # normalization\n",
    "                x, mean_channels, var = standardize_input(x)\n",
    "                    # mirror padding\n",
    "                x = mirror_padding(x, 14)\n",
    "                    # input blocks\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x_c1 = x.clone()\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.resConv1_1(x))\n",
    "                x = self.resConv1_2(x)\n",
    "                x += x_c1\n",
    "                x_c2 = x.clone()\n",
    "                    # residual block 2\n",
    "                x = F.relu(self.resConv2_1(x))\n",
    "                x = self.resConv2_2(x)\n",
    "                x += x_c2\n",
    "                x_c3 = x.clone()\n",
    "                    # residual block 3\n",
    "                x = F.relu(self.resConv3_1(x))\n",
    "                x = self.resConv3_2(x)\n",
    "                x += x_c3\n",
    "                    # output block\n",
    "                x = self.conv3(x)\n",
    "            # quantization with bit-rate variation\n",
    "            x = self.quantization(x / self.lamb)\n",
    "            x_quantized = x\n",
    "\n",
    "            #decoder\n",
    "            x = x*self.lamb\n",
    "            with torch.no_grad():\n",
    "                    # subpixel 1\n",
    "                x = self.subpix1(x)\n",
    "                x = periodic_shuffling(x, 512//4)\n",
    "                x_c4 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 1 is nan\", x)\n",
    "                    # residual block 1\n",
    "                x = F.relu(self.deconv1_1(x))\n",
    "                x = self.deconv1_2(x)\n",
    "                x += x_c4\n",
    "                x_c5 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 1 is nan\", x)\n",
    "                       # residual block 2\n",
    "                x = F.relu(self.deconv2_1(x))\n",
    "                x = self.deconv2_2(x)\n",
    "                x += x_c5\n",
    "                x_c6 = x.clone()\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 2 is nan\", x)\n",
    "                       # residual block 3\n",
    "                x = F.relu(self.deconv3_1(x))\n",
    "                x = self.deconv3_2(x)\n",
    "                x += x_c6\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after residual block 3 is nan\", x)\n",
    "                        # subpixel 2\n",
    "                x = self.subpix2(x)\n",
    "                x = F.relu(periodic_shuffling(x, 256//4))\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 2 is nan\", x)\n",
    "                        # subpixel 3\n",
    "                x = self.subpix3(x)\n",
    "                x = periodic_shuffling(x, 4//4)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after subpix 3 is nan\", x)\n",
    "                        # denormalization\n",
    "                x = destandardize_output(x, mean_channels, var)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after denormalization is nan\", x)\n",
    "                        # clipping\n",
    "                x = self.clip(x)\n",
    "                if torch.any(x.isnan()):\n",
    "                    print(\"x after clipping is nan\", x)\n",
    "\n",
    "\n",
    "                if return_xq:\n",
    "                    return x, x_quantized\n",
    "                else:\n",
    "                    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossyCompAutoencoder(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (resConv1_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (resConv1_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (resConv2_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (resConv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (resConv3_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (resConv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 96, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (subpix1): Conv2d(96, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (deconv1_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (deconv1_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (deconv2_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (deconv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (deconv3_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (deconv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (subpix2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (subpix3): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batchNormed): BatchNorm2d(96, eps=1e-05, momentum=False, affine=False, track_running_stats=True)\n",
      ")\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# instanciate the model\n",
    "model = LossyCompAutoencoder()\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 14.362107\n",
      "running loss : 14.093316\n",
      "running loss : 13.800763\n",
      "running loss : 14.282894\n",
      "running loss : 14.693201\n",
      "running loss : 13.892935\n",
      "running loss : 13.788348\n",
      "running loss : 13.559413\n",
      "running loss : 13.892868\n",
      "running loss : 12.973113\n",
      "running loss : 13.500135\n",
      "running loss : 12.856393\n",
      "running loss : 12.381445\n",
      "running loss : 11.587316\n",
      "running loss : 11.062894\n",
      "running loss : 10.115018\n",
      "running loss : 9.453446\n",
      "running loss : 9.040045\n",
      "running loss : 9.288101\n",
      "running loss : 9.054910\n",
      "running loss : 8.908645\n",
      "running loss : 8.386779\n",
      "running loss : 8.247413\n",
      "running loss : 7.917086\n",
      "running loss : 8.023369\n",
      "running loss : 7.761471\n",
      "running loss : 7.599841\n",
      "running loss : 7.593328\n",
      "running loss : 7.623165\n",
      "running loss : 7.443053\n",
      "running loss : 7.425522\n",
      "running loss : 7.330220\n",
      "running loss : 7.071379\n",
      "running loss : 6.723874\n",
      "running loss : 6.773882\n",
      "running loss : 6.284757\n",
      "running loss : 5.815998\n",
      "running loss : 5.315807\n",
      "running loss : 4.927183\n",
      "running loss : 4.651763\n",
      "running loss : 4.458063\n",
      "running loss : 4.348652\n",
      "running loss : 4.406380\n",
      "running loss : 4.279487\n",
      "running loss : 4.274364\n",
      "running loss : 4.333182\n",
      "running loss : 4.121826\n",
      "running loss : 4.370460\n",
      "running loss : 4.307896\n",
      "running loss : 4.100141\n",
      "running loss : 4.082425\n",
      "running loss : 4.071493\n",
      "running loss : 4.099577\n",
      "running loss : 4.001220\n",
      "running loss : 4.001830\n",
      "running loss : 3.855046\n",
      "running loss : 3.878580\n",
      "running loss : 3.814505\n",
      "running loss : 3.755589\n",
      "running loss : 3.773178\n",
      "running loss : 3.664922\n",
      "running loss : 3.791875\n",
      "running loss : 3.649422\n",
      "running loss : 3.638734\n",
      "running loss : 3.604970\n",
      "running loss : 3.566175\n",
      "running loss : 3.470560\n",
      "running loss : 3.537453\n",
      "running loss : 3.482995\n",
      "running loss : 3.455200\n",
      "running loss : 3.569919\n",
      "running loss : 3.420590\n",
      "running loss : 3.348116\n",
      "running loss : 3.292379\n",
      "running loss : 3.272860\n",
      "running loss : 3.393453\n",
      "running loss : 3.369104\n",
      "running loss : 3.311622\n",
      "running loss : 3.207349\n",
      "running loss : 3.187925\n",
      "running loss : 3.241055\n",
      "running loss : 3.223250\n",
      "running loss : 3.176473\n",
      "running loss : 3.221241\n",
      "running loss : 3.115678\n",
      "running loss : 3.129172\n",
      "running loss : 3.118122\n",
      "running loss : 3.180179\n",
      "running loss : 3.152718\n",
      "running loss : 3.111930\n",
      "running loss : 3.061123\n",
      "running loss : 3.041099\n",
      "running loss : 2.957715\n",
      "running loss : 2.968992\n",
      "running loss : 3.024184\n",
      "running loss : 2.982786\n",
      "running loss : 2.867571\n",
      "running loss : 2.965618\n",
      "running loss : 2.911422\n",
      "running loss : 2.869759\n",
      "running loss : 2.889813\n",
      "running loss : 2.890245\n",
      "running loss : 2.929901\n",
      "running loss : 2.883778\n",
      "running loss : 2.811909\n",
      "running loss : 2.865391\n",
      "running loss : 2.808709\n",
      "running loss : 2.802292\n",
      "running loss : 2.811003\n",
      "running loss : 2.873550\n",
      "running loss : 2.737859\n",
      "running loss : 2.761260\n",
      "running loss : 2.713544\n",
      "running loss : 2.782678\n",
      "running loss : 2.671740\n",
      "running loss : 2.863528\n",
      "running loss : 2.726289\n",
      "running loss : 2.679507\n",
      "running loss : 2.723846\n",
      "running loss : 2.666382\n",
      "running loss : 2.727617\n",
      "running loss : 2.669678\n",
      "running loss : 2.684203\n",
      "running loss : 2.617699\n",
      "running loss : 2.747309\n",
      "running loss : 2.613912\n",
      "running loss : 2.646673\n",
      "running loss : 2.598029\n",
      "running loss : 2.605120\n",
      "running loss : 2.619908\n",
      "running loss : 2.570918\n",
      "running loss : 2.563510\n",
      "running loss : 2.638680\n",
      "running loss : 2.629154\n",
      "running loss : 2.553156\n",
      "running loss : 2.606387\n",
      "running loss : 2.554168\n",
      "running loss : 2.541307\n",
      "running loss : 2.552341\n",
      "running loss : 2.476934\n",
      "running loss : 2.541421\n",
      "running loss : 2.512674\n",
      "running loss : 2.466479\n",
      "running loss : 2.523581\n",
      "running loss : 2.478329\n",
      "running loss : 2.438761\n",
      "running loss : 2.428497\n",
      "running loss : 2.452635\n",
      "running loss : 2.376886\n",
      "running loss : 2.458404\n",
      "running loss : 2.445381\n",
      "running loss : 2.463230\n",
      "running loss : 2.412520\n",
      "running loss : 2.384284\n",
      "running loss : 2.427216\n",
      "running loss : 2.405920\n",
      "running loss : 2.362676\n",
      "running loss : 2.419371\n",
      "running loss : 2.392804\n",
      "running loss : 2.469023\n",
      "running loss : 2.339581\n",
      "running loss : 2.350907\n",
      "running loss : 2.370841\n",
      "running loss : 2.363101\n",
      "running loss : 2.325841\n",
      "running loss : 2.329742\n",
      "running loss : 2.396842\n",
      "running loss : 2.317886\n",
      "running loss : 2.334549\n",
      "running loss : 2.264065\n",
      "running loss : 2.350521\n",
      "running loss : 2.289304\n",
      "running loss : 2.330551\n",
      "running loss : 2.276222\n",
      "running loss : 2.267938\n",
      "running loss : 2.343124\n",
      "running loss : 2.291271\n",
      "running loss : 2.262595\n",
      "running loss : 2.310649\n",
      "running loss : 2.338464\n",
      "running loss : 2.277089\n",
      "running loss : 2.242001\n",
      "running loss : 2.236999\n",
      "running loss : 2.228245\n",
      "running loss : 2.284440\n",
      "running loss : 2.222585\n",
      "running loss : 2.203105\n",
      "running loss : 2.179518\n",
      "running loss : 2.212381\n",
      "running loss : 2.196663\n",
      "running loss : 2.223655\n",
      "running loss : 2.217335\n",
      "running loss : 2.168261\n",
      "running loss : 2.199144\n",
      "running loss : 2.158403\n",
      "running loss : 2.210358\n",
      "running loss : 2.165169\n",
      "running loss : 2.187335\n",
      "running loss : 2.173414\n",
      "running loss : 2.223135\n",
      "running loss : 2.170602\n",
      "running loss : 2.178545\n",
      "running loss : 2.185494\n",
      "running loss : 2.121703\n",
      "running loss : 2.135772\n",
      "running loss : 2.145974\n",
      "running loss : 2.143605\n",
      "running loss : 2.141060\n",
      "running loss : 2.197228\n",
      "running loss : 2.133862\n",
      "running loss : 2.132774\n",
      "running loss : 2.128443\n",
      "running loss : 2.089187\n",
      "running loss : 2.146852\n",
      "running loss : 2.122630\n",
      "running loss : 2.097744\n",
      "running loss : 2.104185\n",
      "running loss : 2.161207\n",
      "running loss : 2.145271\n",
      "running loss : 2.148200\n",
      "running loss : 2.124193\n",
      "running loss : 2.084803\n",
      "running loss : 2.092291\n",
      "running loss : 2.173473\n",
      "running loss : 2.126219\n",
      "running loss : 2.074922\n",
      "running loss : 2.055013\n",
      "running loss : 2.083648\n",
      "running loss : 2.088841\n",
      "running loss : 2.115775\n",
      "running loss : 2.079057\n",
      "running loss : 2.073674\n",
      "running loss : 2.057264\n",
      "running loss : 2.073896\n",
      "running loss : 2.084749\n",
      "running loss : 2.085336\n",
      "running loss : 2.063415\n",
      "running loss : 2.032515\n",
      "running loss : 2.050692\n",
      "running loss : 2.096416\n",
      "running loss : 2.073295\n",
      "running loss : 2.122508\n",
      "running loss : 2.076517\n",
      "running loss : 2.164785\n",
      "running loss : 2.057723\n",
      "running loss : 1.993790\n",
      "running loss : 2.047943\n",
      "running loss : 2.001472\n",
      "running loss : 2.013295\n",
      "running loss : 2.032034\n",
      "running loss : 2.018636\n",
      "running loss : 2.042907\n",
      "running loss : 2.056751\n",
      "running loss : 2.035091\n",
      "running loss : 1.994487\n",
      "running loss : 2.054040\n",
      "running loss : 2.064393\n",
      "running loss : 2.027563\n",
      "running loss : 1.995492\n",
      "running loss : 1.997112\n",
      "running loss : 2.052927\n",
      "running loss : 2.005337\n",
      "running loss : 2.043107\n",
      "running loss : 1.964057\n",
      "running loss : 2.032373\n",
      "running loss : 1.971442\n",
      "running loss : 1.973697\n",
      "running loss : 1.971418\n",
      "running loss : 2.009250\n",
      "running loss : 1.989471\n",
      "running loss : 1.990993\n",
      "running loss : 2.072998\n",
      "running loss : 1.973641\n",
      "running loss : 1.972380\n",
      "running loss : 1.943457\n",
      "running loss : 1.968435\n",
      "running loss : 1.997036\n",
      "running loss : 1.950031\n",
      "running loss : 1.992566\n",
      "running loss : 1.970440\n",
      "running loss : 1.972381\n",
      "running loss : 1.964119\n",
      "running loss : 1.956972\n",
      "running loss : 1.954646\n",
      "running loss : 1.983871\n",
      "running loss : 1.971574\n",
      "running loss : 1.938815\n",
      "running loss : 1.943024\n",
      "running loss : 1.916179\n",
      "running loss : 2.004067\n",
      "running loss : 1.928556\n",
      "running loss : 1.925449\n",
      "running loss : 1.928387\n",
      "running loss : 1.928180\n",
      "running loss : 1.938206\n",
      "running loss : 1.916075\n",
      "running loss : 1.923170\n",
      "running loss : 1.906468\n",
      "running loss : 1.943402\n",
      "running loss : 1.935345\n",
      "running loss : 1.946391\n",
      "running loss : 1.893572\n",
      "running loss : 1.923020\n",
      "running loss : 1.918825\n",
      "running loss : 1.925177\n",
      "running loss : 1.904469\n",
      "running loss : 1.926366\n",
      "running loss : 1.906889\n",
      "running loss : 1.919314\n",
      "running loss : 1.937018\n",
      "running loss : 1.954855\n",
      "running loss : 1.916525\n",
      "running loss : 1.881318\n",
      "running loss : 1.925147\n",
      "running loss : 1.876306\n",
      "running loss : 1.884437\n",
      "running loss : 1.961672\n",
      "running loss : 1.879988\n",
      "running loss : 1.913124\n",
      "running loss : 1.907237\n",
      "running loss : 1.863570\n",
      "running loss : 1.893521\n",
      "running loss : 1.902365\n",
      "running loss : 1.878958\n",
      "running loss : 1.875329\n",
      "running loss : 1.866515\n",
      "running loss : 1.853852\n",
      "running loss : 1.857765\n",
      "running loss : 1.861925\n",
      "running loss : 1.908585\n",
      "running loss : 1.913130\n",
      "running loss : 1.877360\n",
      "running loss : 1.866722\n",
      "running loss : 1.863246\n",
      "running loss : 1.859745\n",
      "running loss : 1.898129\n",
      "running loss : 1.882682\n",
      "running loss : 1.858793\n",
      "running loss : 1.884751\n",
      "running loss : 1.838408\n",
      "running loss : 1.815731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.839481\n",
      "running loss : 1.924346\n",
      "running loss : 1.857772\n",
      "running loss : 1.880925\n",
      "running loss : 1.846384\n",
      "running loss : 1.810175\n",
      "running loss : 1.813932\n",
      "running loss : 1.817051\n",
      "running loss : 1.850115\n",
      "running loss : 1.847480\n",
      "running loss : 1.823813\n",
      "running loss : 1.811623\n",
      "running loss : 1.827734\n",
      "running loss : 1.814158\n",
      "running loss : 1.837136\n",
      "running loss : 1.836135\n",
      "running loss : 1.813553\n",
      "running loss : 1.834549\n",
      "running loss : 1.829906\n",
      "running loss : 1.814463\n",
      "running loss : 1.845001\n",
      "running loss : 1.781638\n",
      "running loss : 1.785616\n",
      "running loss : 1.814782\n",
      "running loss : 1.812862\n",
      "running loss : 1.847391\n",
      "running loss : 1.791659\n",
      "running loss : 1.825243\n",
      "running loss : 1.808056\n",
      "running loss : 1.825862\n",
      "running loss : 1.775674\n",
      "running loss : 1.795983\n",
      "running loss : 1.824402\n",
      "running loss : 1.801932\n",
      "running loss : 1.773690\n",
      "running loss : 1.828092\n",
      "running loss : 1.789556\n",
      "running loss : 1.783628\n",
      "running loss : 1.769006\n",
      "running loss : 1.757716\n",
      "running loss : 1.806522\n",
      "running loss : 1.801184\n",
      "running loss : 1.757340\n",
      "running loss : 1.766428\n",
      "running loss : 1.791959\n",
      "running loss : 1.786382\n",
      "running loss : 1.776887\n",
      "running loss : 1.788293\n",
      "running loss : 1.750968\n",
      "running loss : 1.768117\n",
      "running loss : 1.765157\n",
      "running loss : 1.815841\n",
      "running loss : 1.746872\n",
      "running loss : 1.788756\n",
      "running loss : 1.808550\n",
      "running loss : 1.774148\n",
      "running loss : 1.732192\n",
      "running loss : 1.764153\n",
      "running loss : 1.753554\n",
      "running loss : 1.727768\n",
      "running loss : 1.766207\n",
      "running loss : 1.794658\n",
      "running loss : 1.730698\n",
      "running loss : 1.746355\n",
      "running loss : 1.696020\n",
      "running loss : 1.724414\n",
      "running loss : 1.783681\n",
      "running loss : 1.739039\n",
      "running loss : 1.771496\n",
      "running loss : 1.735934\n",
      "running loss : 1.739234\n",
      "running loss : 1.750007\n",
      "running loss : 1.735166\n",
      "running loss : 1.736153\n",
      "running loss : 1.741261\n",
      "running loss : 1.735746\n",
      "running loss : 1.727347\n",
      "running loss : 1.717308\n",
      "running loss : 1.739505\n",
      "running loss : 1.712355\n",
      "running loss : 1.736696\n",
      "running loss : 1.708080\n",
      "running loss : 1.713413\n",
      "running loss : 1.716367\n",
      "running loss : 1.725385\n",
      "running loss : 1.714630\n",
      "running loss : 1.734451\n",
      "running loss : 1.689440\n",
      "running loss : 1.724576\n",
      "running loss : 1.703442\n",
      "running loss : 1.679238\n",
      "running loss : 1.699623\n",
      "running loss : 1.691251\n",
      "running loss : 1.696037\n",
      "running loss : 1.748391\n",
      "running loss : 1.714522\n",
      "running loss : 1.693276\n",
      "running loss : 1.731351\n",
      "running loss : 1.747375\n",
      "running loss : 1.697945\n",
      "running loss : 1.692609\n",
      "running loss : 1.708557\n",
      "running loss : 1.690136\n",
      "running loss : 1.702665\n",
      "running loss : 1.679557\n",
      "running loss : 1.676016\n",
      "running loss : 1.702574\n",
      "running loss : 1.670505\n",
      "running loss : 1.678589\n",
      "running loss : 1.675458\n",
      "running loss : 1.659539\n",
      "running loss : 1.677649\n",
      "running loss : 1.693714\n",
      "running loss : 1.730319\n",
      "running loss : 1.659063\n",
      "running loss : 1.665744\n",
      "running loss : 1.708989\n",
      "running loss : 1.711326\n",
      "running loss : 1.663753\n",
      "running loss : 1.696064\n",
      "running loss : 1.680650\n",
      "running loss : 1.695339\n",
      "running loss : 1.661252\n",
      "running loss : 1.697276\n",
      "running loss : 1.688130\n",
      "running loss : 1.677819\n",
      "running loss : 1.707389\n",
      "running loss : 1.663010\n",
      "running loss : 1.683915\n",
      "running loss : 1.693332\n",
      "running loss : 1.645267\n",
      "running loss : 1.678836\n",
      "running loss : 1.665959\n",
      "running loss : 1.649512\n",
      "running loss : 1.650385\n",
      "running loss : 1.672116\n",
      "running loss : 1.666750\n",
      "running loss : 1.681952\n",
      "running loss : 1.655341\n",
      "running loss : 1.661006\n",
      "running loss : 1.700624\n",
      "running loss : 1.672764\n",
      "running loss : 1.669556\n",
      "running loss : 1.668975\n",
      "running loss : 1.641069\n",
      "running loss : 1.638892\n",
      "running loss : 1.643193\n",
      "running loss : 1.695970\n",
      "running loss : 1.663980\n",
      "running loss : 1.679471\n",
      "running loss : 1.691544\n",
      "running loss : 1.655044\n",
      "running loss : 1.654605\n",
      "running loss : 1.652784\n",
      "running loss : 1.633867\n",
      "running loss : 1.624388\n",
      "running loss : 1.664111\n",
      "running loss : 1.630520\n",
      "running loss : 1.617405\n",
      "running loss : 1.655863\n",
      "running loss : 1.651341\n",
      "running loss : 1.659172\n",
      "running loss : 1.649129\n",
      "running loss : 1.650385\n",
      "running loss : 1.643329\n",
      "running loss : 1.670832\n",
      "running loss : 1.658490\n",
      "running loss : 1.620207\n",
      "running loss : 1.659177\n",
      "running loss : 1.619966\n",
      "running loss : 1.620748\n",
      "running loss : 1.644814\n",
      "running loss : 1.662552\n",
      "running loss : 1.634894\n",
      "running loss : 1.616726\n",
      "running loss : 1.626428\n",
      "running loss : 1.628045\n",
      "running loss : 1.621794\n",
      "running loss : 1.608490\n",
      "running loss : 1.613129\n",
      "running loss : 1.633856\n",
      "running loss : 1.622773\n",
      "running loss : 1.635463\n",
      "running loss : 1.620391\n",
      "running loss : 1.638559\n",
      "running loss : 1.626328\n",
      "running loss : 1.679886\n",
      "running loss : 1.650333\n",
      "running loss : 1.604038\n",
      "running loss : 1.620976\n",
      "running loss : 1.627218\n",
      "running loss : 1.650491\n",
      "running loss : 1.620197\n",
      "running loss : 1.613246\n",
      "running loss : 1.617552\n",
      "running loss : 1.645879\n",
      "running loss : 1.616735\n",
      "running loss : 1.635760\n",
      "running loss : 1.621733\n",
      "running loss : 1.648757\n",
      "running loss : 1.591970\n",
      "running loss : 1.604778\n",
      "running loss : 1.606164\n",
      "running loss : 1.607002\n",
      "running loss : 1.598291\n",
      "running loss : 1.615739\n",
      "running loss : 1.649859\n",
      "running loss : 1.583208\n",
      "running loss : 1.592714\n",
      "running loss : 1.623500\n",
      "running loss : 1.626825\n",
      "running loss : 1.613434\n",
      "running loss : 1.579117\n",
      "running loss : 1.598599\n",
      "running loss : 1.587997\n",
      "running loss : 1.578847\n",
      "running loss : 1.616982\n",
      "running loss : 1.619223\n",
      "running loss : 1.566136\n",
      "running loss : 1.581652\n",
      "running loss : 1.593828\n",
      "running loss : 1.594590\n",
      "running loss : 1.594356\n",
      "running loss : 1.583423\n",
      "running loss : 1.610907\n",
      "running loss : 1.581192\n",
      "running loss : 1.609568\n",
      "running loss : 1.560395\n",
      "running loss : 1.578857\n",
      "running loss : 1.600496\n",
      "running loss : 1.621882\n",
      "running loss : 1.574549\n",
      "running loss : 1.582155\n",
      "running loss : 1.611082\n",
      "running loss : 1.598614\n",
      "running loss : 1.584065\n",
      "running loss : 1.604105\n",
      "running loss : 1.566958\n",
      "running loss : 1.575838\n",
      "running loss : 1.580795\n",
      "running loss : 1.566958\n",
      "running loss : 1.592324\n",
      "running loss : 1.597011\n",
      "running loss : 1.567800\n",
      "running loss : 1.626157\n",
      "running loss : 1.580933\n",
      "running loss : 1.561627\n",
      "running loss : 1.553290\n",
      "running loss : 1.585158\n",
      "running loss : 1.594158\n",
      "running loss : 1.594565\n",
      "running loss : 1.558999\n",
      "running loss : 1.560834\n",
      "running loss : 1.592107\n",
      "running loss : 1.571967\n",
      "running loss : 1.564508\n",
      "running loss : 1.572497\n",
      "running loss : 1.586185\n",
      "running loss : 1.546768\n",
      "running loss : 1.580407\n",
      "running loss : 1.570255\n",
      "running loss : 1.581617\n",
      "running loss : 1.569771\n",
      "running loss : 1.566445\n",
      "running loss : 1.579308\n",
      "running loss : 1.563654\n",
      "running loss : 1.580740\n",
      "running loss : 1.576454\n",
      "running loss : 1.544712\n",
      "running loss : 1.582159\n",
      "running loss : 1.568707\n",
      "running loss : 1.577015\n",
      "running loss : 1.566546\n",
      "running loss : 1.553534\n",
      "running loss : 1.573245\n",
      "running loss : 1.568783\n",
      "running loss : 1.601166\n",
      "running loss : 1.551469\n",
      "running loss : 1.569234\n",
      "running loss : 1.558970\n",
      "running loss : 1.543344\n",
      "running loss : 1.538348\n",
      "running loss : 1.568412\n",
      "running loss : 1.554973\n",
      "running loss : 1.530759\n",
      "running loss : 1.567024\n",
      "running loss : 1.550362\n",
      "running loss : 1.544723\n",
      "running loss : 1.562717\n",
      "running loss : 1.543329\n",
      "running loss : 1.545904\n",
      "running loss : 1.553005\n",
      "running loss : 1.547992\n",
      "running loss : 1.626190\n",
      "running loss : 1.544894\n",
      "running loss : 1.542449\n",
      "running loss : 1.543923\n",
      "running loss : 1.531933\n",
      "running loss : 1.551533\n",
      "running loss : 1.549199\n",
      "running loss : 1.560681\n",
      "running loss : 1.553820\n",
      "running loss : 1.540567\n",
      "running loss : 1.531605\n",
      "running loss : 1.552157\n",
      "running loss : 1.529943\n",
      "running loss : 1.556346\n",
      "running loss : 1.519317\n",
      "running loss : 1.538423\n",
      "running loss : 1.557104\n",
      "running loss : 1.540001\n",
      "running loss : 1.552128\n",
      "running loss : 1.565749\n",
      "running loss : 1.535446\n",
      "running loss : 1.528578\n",
      "running loss : 1.556991\n",
      "running loss : 1.535224\n",
      "running loss : 1.533013\n",
      "running loss : 1.527122\n",
      "running loss : 1.585863\n",
      "running loss : 1.516332\n",
      "running loss : 1.528265\n",
      "running loss : 1.531338\n",
      "running loss : 1.545483\n",
      "running loss : 1.542659\n",
      "running loss : 1.519778\n",
      "running loss : 1.511903\n",
      "running loss : 1.521889\n",
      "running loss : 1.557725\n",
      "running loss : 1.520259\n",
      "running loss : 1.531626\n",
      "running loss : 1.511235\n",
      "running loss : 1.572933\n",
      "running loss : 1.551136\n",
      "running loss : 1.539081\n",
      "running loss : 1.530831\n",
      "running loss : 1.513393\n",
      "running loss : 1.520796\n",
      "running loss : 1.536263\n",
      "running loss : 1.559115\n",
      "running loss : 1.512045\n",
      "running loss : 1.531046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.537150\n",
      "running loss : 1.509248\n",
      "running loss : 1.537673\n",
      "running loss : 1.531267\n",
      "running loss : 1.516809\n",
      "running loss : 1.492584\n",
      "running loss : 1.525840\n",
      "running loss : 1.521271\n",
      "running loss : 1.501484\n",
      "running loss : 1.538977\n",
      "running loss : 1.496118\n",
      "running loss : 1.554101\n",
      "running loss : 1.521213\n",
      "running loss : 1.510295\n",
      "running loss : 1.533027\n",
      "running loss : 1.534444\n",
      "running loss : 1.534991\n",
      "running loss : 1.529512\n",
      "running loss : 1.508756\n",
      "running loss : 1.532970\n",
      "running loss : 1.529910\n",
      "running loss : 1.497501\n",
      "running loss : 1.516423\n",
      "running loss : 1.509979\n",
      "running loss : 1.511190\n",
      "running loss : 1.526498\n",
      "running loss : 1.516666\n",
      "running loss : 1.516037\n",
      "running loss : 1.502379\n",
      "running loss : 1.498043\n",
      "running loss : 1.551367\n",
      "running loss : 1.520010\n",
      "running loss : 1.503894\n",
      "running loss : 1.534585\n",
      "running loss : 1.535195\n",
      "running loss : 1.499537\n",
      "running loss : 1.559960\n",
      "running loss : 1.498496\n",
      "running loss : 1.509293\n",
      "running loss : 1.547379\n",
      "running loss : 1.501526\n",
      "running loss : 1.504553\n",
      "running loss : 1.508926\n",
      "running loss : 1.517793\n",
      "running loss : 1.533711\n",
      "running loss : 1.523674\n",
      "running loss : 1.512201\n",
      "running loss : 1.492506\n",
      "running loss : 1.491099\n",
      "running loss : 1.525818\n",
      "running loss : 1.488912\n",
      "running loss : 1.519931\n",
      "running loss : 1.496340\n",
      "running loss : 1.485193\n",
      "running loss : 1.515331\n",
      "running loss : 1.486905\n",
      "running loss : 1.494433\n",
      "running loss : 1.491995\n",
      "running loss : 1.491589\n",
      "running loss : 1.491889\n",
      "running loss : 1.487615\n",
      "running loss : 1.500397\n",
      "running loss : 1.497545\n",
      "running loss : 1.478624\n",
      "running loss : 1.487816\n",
      "running loss : 1.489732\n",
      "running loss : 1.498218\n",
      "running loss : 1.465538\n",
      "running loss : 1.501038\n",
      "running loss : 1.497234\n",
      "running loss : 1.493237\n",
      "running loss : 1.462042\n",
      "running loss : 1.505897\n",
      "running loss : 1.526722\n",
      "running loss : 1.491566\n",
      "running loss : 1.470483\n",
      "running loss : 1.461322\n",
      "running loss : 1.471308\n",
      "running loss : 1.497139\n",
      "running loss : 1.478169\n",
      "running loss : 1.511517\n",
      "running loss : 1.491610\n",
      "running loss : 1.514059\n",
      "running loss : 1.460264\n",
      "running loss : 1.463838\n",
      "running loss : 1.460770\n",
      "running loss : 1.467645\n",
      "running loss : 1.500397\n",
      "running loss : 1.461110\n",
      "running loss : 1.465851\n",
      "running loss : 1.475111\n",
      "running loss : 1.479889\n",
      "running loss : 1.499201\n",
      "running loss : 1.458239\n",
      "running loss : 1.452913\n",
      "running loss : 1.461003\n",
      "running loss : 1.462831\n",
      "running loss : 1.467429\n",
      "running loss : 1.449427\n",
      "running loss : 1.465085\n",
      "running loss : 1.452395\n",
      "running loss : 1.473670\n",
      "running loss : 1.463252\n",
      "running loss : 1.436582\n",
      "running loss : 1.452761\n",
      "running loss : 1.448425\n",
      "running loss : 1.462673\n",
      "running loss : 1.480788\n",
      "running loss : 1.447495\n",
      "running loss : 1.451335\n",
      "running loss : 1.449251\n",
      "running loss : 1.455924\n",
      "running loss : 1.494734\n",
      "running loss : 1.458786\n",
      "running loss : 1.459322\n",
      "running loss : 1.462389\n",
      "running loss : 1.475408\n",
      "running loss : 1.446364\n",
      "running loss : 1.443002\n",
      "running loss : 1.482386\n",
      "running loss : 1.459793\n",
      "running loss : 1.471384\n",
      "running loss : 1.477967\n",
      "running loss : 1.472175\n",
      "running loss : 1.457908\n",
      "running loss : 1.442344\n",
      "running loss : 1.456790\n",
      "running loss : 1.450695\n",
      "running loss : 1.449460\n",
      "running loss : 1.446264\n",
      "running loss : 1.446795\n",
      "running loss : 1.474967\n",
      "running loss : 1.433960\n",
      "running loss : 1.454805\n",
      "running loss : 1.447213\n",
      "running loss : 1.458037\n",
      "running loss : 1.453532\n",
      "running loss : 1.440678\n",
      "running loss : 1.462261\n",
      "running loss : 1.420307\n",
      "running loss : 1.449132\n",
      "running loss : 1.446775\n",
      "running loss : 1.446945\n",
      "running loss : 1.433791\n",
      "running loss : 1.463101\n",
      "running loss : 1.443560\n",
      "running loss : 1.441073\n",
      "running loss : 1.452430\n",
      "running loss : 1.437906\n",
      "running loss : 1.457955\n",
      "running loss : 1.440765\n",
      "running loss : 1.429709\n",
      "running loss : 1.440718\n",
      "running loss : 1.431795\n",
      "running loss : 1.465171\n",
      "running loss : 1.493511\n",
      "running loss : 1.430838\n",
      "running loss : 1.420183\n",
      "running loss : 1.430965\n",
      "running loss : 1.419334\n",
      "running loss : 1.483711\n",
      "running loss : 1.430509\n",
      "running loss : 1.434633\n",
      "running loss : 1.482348\n",
      "running loss : 1.427937\n",
      "running loss : 1.433543\n",
      "running loss : 1.439607\n",
      "running loss : 1.445815\n",
      "running loss : 1.428286\n",
      "running loss : 1.419179\n",
      "running loss : 1.432694\n",
      "running loss : 1.426852\n",
      "running loss : 1.437387\n",
      "running loss : 1.469745\n",
      "running loss : 1.436775\n",
      "running loss : 1.448870\n",
      "running loss : 1.416680\n",
      "running loss : 1.426019\n",
      "running loss : 1.424664\n",
      "running loss : 1.409722\n",
      "running loss : 1.426000\n",
      "running loss : 1.430880\n",
      "running loss : 1.417735\n",
      "running loss : 1.456723\n",
      "running loss : 1.425924\n",
      "running loss : 1.442706\n",
      "running loss : 1.438685\n",
      "running loss : 1.433046\n",
      "running loss : 1.440404\n",
      "running loss : 1.439269\n",
      "running loss : 1.444007\n",
      "running loss : 1.419567\n",
      "running loss : 1.451110\n",
      "running loss : 1.442820\n",
      "running loss : 1.408507\n",
      "running loss : 1.432595\n",
      "running loss : 1.425714\n",
      "running loss : 1.435432\n",
      "running loss : 1.428116\n",
      "running loss : 1.423566\n",
      "running loss : 1.436676\n",
      "running loss : 1.416049\n",
      "running loss : 1.419141\n",
      "running loss : 1.408962\n",
      "running loss : 1.417567\n",
      "running loss : 1.403480\n",
      "running loss : 1.407502\n",
      "running loss : 1.425882\n",
      "running loss : 1.436894\n",
      "running loss : 1.416811\n",
      "running loss : 1.433184\n",
      "running loss : 1.417527\n",
      "running loss : 1.398926\n",
      "running loss : 1.416168\n",
      "running loss : 1.437289\n",
      "running loss : 1.418315\n",
      "running loss : 1.430070\n",
      "running loss : 1.401600\n",
      "running loss : 1.434973\n",
      "running loss : 1.418520\n",
      "running loss : 1.437296\n",
      "running loss : 1.419894\n",
      "running loss : 1.401162\n",
      "running loss : 1.426063\n",
      "running loss : 1.420489\n",
      "running loss : 1.407647\n",
      "running loss : 1.401550\n",
      "running loss : 1.465350\n",
      "running loss : 1.405744\n",
      "running loss : 1.422476\n",
      "running loss : 1.404268\n",
      "running loss : 1.419722\n",
      "running loss : 1.400497\n",
      "running loss : 1.396321\n",
      "running loss : 1.405682\n",
      "running loss : 1.396278\n",
      "running loss : 1.419790\n",
      "running loss : 1.403025\n",
      "running loss : 1.397884\n",
      "running loss : 1.409822\n",
      "running loss : 1.426800\n",
      "running loss : 1.420673\n",
      "running loss : 1.400401\n",
      "running loss : 1.415371\n",
      "running loss : 1.389572\n",
      "running loss : 1.416942\n",
      "running loss : 1.410893\n",
      "running loss : 1.418724\n",
      "running loss : 1.420664\n",
      "running loss : 1.402145\n",
      "running loss : 1.425343\n",
      "running loss : 1.449553\n",
      "running loss : 1.414116\n",
      "running loss : 1.401909\n",
      "running loss : 1.403196\n",
      "running loss : 1.424021\n",
      "running loss : 1.401301\n",
      "running loss : 1.397083\n",
      "running loss : 1.401220\n",
      "running loss : 1.386374\n",
      "running loss : 1.379088\n",
      "running loss : 1.400881\n",
      "running loss : 1.395844\n",
      "running loss : 1.393345\n",
      "running loss : 1.381743\n",
      "running loss : 1.394768\n",
      "running loss : 1.411527\n",
      "running loss : 1.403975\n",
      "running loss : 1.399648\n",
      "running loss : 1.398866\n",
      "running loss : 1.396972\n",
      "running loss : 1.409537\n",
      "running loss : 1.388378\n",
      "running loss : 1.418790\n",
      "running loss : 1.385805\n",
      "running loss : 1.392658\n",
      "running loss : 1.393915\n",
      "running loss : 1.414095\n",
      "running loss : 1.389805\n",
      "running loss : 1.397463\n",
      "running loss : 1.376874\n",
      "running loss : 1.385828\n",
      "running loss : 1.370587\n",
      "running loss : 1.369877\n",
      "running loss : 1.405277\n",
      "running loss : 1.387543\n",
      "running loss : 1.391686\n",
      "running loss : 1.389995\n",
      "running loss : 1.378926\n",
      "running loss : 1.400958\n",
      "running loss : 1.384040\n",
      "running loss : 1.407607\n",
      "running loss : 1.384740\n",
      "running loss : 1.390496\n",
      "running loss : 1.365395\n",
      "running loss : 1.365654\n",
      "running loss : 1.377941\n",
      "running loss : 1.364354\n",
      "running loss : 1.387161\n",
      "running loss : 1.374179\n",
      "running loss : 1.376514\n",
      "running loss : 1.387374\n",
      "running loss : 1.355112\n",
      "running loss : 1.373471\n",
      "running loss : 1.367744\n",
      "running loss : 1.389496\n",
      "running loss : 1.387955\n",
      "running loss : 1.368293\n",
      "running loss : 1.356344\n",
      "running loss : 1.373362\n",
      "running loss : 1.385451\n",
      "running loss : 1.361303\n",
      "running loss : 1.364838\n",
      "running loss : 1.373399\n",
      "running loss : 1.363090\n",
      "running loss : 1.375861\n",
      "running loss : 1.359330\n",
      "running loss : 1.352396\n",
      "running loss : 1.367241\n",
      "running loss : 1.361795\n",
      "running loss : 1.356039\n",
      "running loss : 1.349728\n",
      "running loss : 1.382961\n",
      "running loss : 1.405802\n",
      "running loss : 1.357858\n",
      "running loss : 1.373542\n",
      "running loss : 1.371864\n",
      "running loss : 1.375302\n",
      "running loss : 1.345857\n",
      "running loss : 1.368215\n",
      "running loss : 1.359040\n",
      "running loss : 1.409907\n",
      "running loss : 1.353719\n",
      "running loss : 1.348452\n",
      "running loss : 1.362798\n",
      "running loss : 1.347476\n",
      "running loss : 1.354903\n",
      "running loss : 1.366796\n",
      "running loss : 1.360021\n",
      "running loss : 1.368404\n",
      "running loss : 1.363366\n",
      "running loss : 1.355533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.360029\n",
      "running loss : 1.341422\n",
      "running loss : 1.396389\n",
      "running loss : 1.358250\n",
      "running loss : 1.342766\n",
      "running loss : 1.364412\n",
      "running loss : 1.332934\n",
      "running loss : 1.336666\n",
      "running loss : 1.355870\n",
      "running loss : 1.361147\n",
      "running loss : 1.339132\n",
      "running loss : 1.346626\n",
      "running loss : 1.349168\n",
      "running loss : 1.363598\n",
      "running loss : 1.359451\n",
      "running loss : 1.338832\n",
      "running loss : 1.344180\n",
      "running loss : 1.343341\n",
      "running loss : 1.366627\n",
      "running loss : 1.343091\n",
      "running loss : 1.338518\n",
      "running loss : 1.336382\n",
      "running loss : 1.358980\n",
      "running loss : 1.342407\n",
      "running loss : 1.350026\n",
      "running loss : 1.364337\n",
      "running loss : 1.341999\n",
      "running loss : 1.346576\n",
      "running loss : 1.342539\n",
      "running loss : 1.373652\n",
      "running loss : 1.358084\n",
      "running loss : 1.377077\n",
      "running loss : 1.374973\n",
      "running loss : 1.331993\n",
      "running loss : 1.326254\n",
      "running loss : 1.345160\n",
      "running loss : 1.361764\n",
      "running loss : 1.359324\n",
      "running loss : 1.350474\n",
      "running loss : 1.352383\n",
      "running loss : 1.342066\n",
      "running loss : 1.348368\n",
      "running loss : 1.358762\n",
      "running loss : 1.376993\n",
      "running loss : 1.356815\n",
      "running loss : 1.342555\n",
      "running loss : 1.344631\n",
      "running loss : 1.382249\n",
      "running loss : 1.336051\n",
      "running loss : 1.350366\n",
      "running loss : 1.340838\n",
      "running loss : 1.366229\n",
      "running loss : 1.349630\n",
      "running loss : 1.336016\n",
      "running loss : 1.329757\n",
      "running loss : 1.332846\n",
      "running loss : 1.336224\n",
      "running loss : 1.351697\n",
      "running loss : 1.337836\n",
      "running loss : 1.346541\n",
      "running loss : 1.346170\n",
      "running loss : 1.344716\n",
      "running loss : 1.341731\n",
      "running loss : 1.352141\n",
      "running loss : 1.366854\n",
      "running loss : 1.338615\n",
      "running loss : 1.352419\n",
      "running loss : 1.330582\n",
      "running loss : 1.330519\n",
      "running loss : 1.323268\n",
      "running loss : 1.338017\n",
      "running loss : 1.352904\n",
      "running loss : 1.329311\n",
      "running loss : 1.354876\n",
      "running loss : 1.339589\n",
      "running loss : 1.346035\n",
      "running loss : 1.330989\n",
      "running loss : 1.344240\n",
      "running loss : 1.362166\n",
      "running loss : 1.329641\n",
      "running loss : 1.313251\n",
      "running loss : 1.323666\n",
      "running loss : 1.332071\n",
      "running loss : 1.342312\n",
      "running loss : 1.356442\n",
      "running loss : 1.349192\n",
      "running loss : 1.335991\n",
      "running loss : 1.357336\n",
      "running loss : 1.332750\n",
      "running loss : 1.326673\n",
      "running loss : 1.373086\n",
      "running loss : 1.318810\n",
      "running loss : 1.326163\n",
      "running loss : 1.344638\n",
      "running loss : 1.355706\n",
      "running loss : 1.317920\n",
      "running loss : 1.323076\n",
      "running loss : 1.304785\n",
      "running loss : 1.304911\n",
      "running loss : 1.329057\n",
      "running loss : 1.354444\n",
      "running loss : 1.318324\n",
      "running loss : 1.312170\n",
      "running loss : 1.341732\n",
      "running loss : 1.323753\n",
      "running loss : 1.345987\n",
      "running loss : 1.331254\n",
      "running loss : 1.318853\n",
      "running loss : 1.336961\n",
      "running loss : 1.354666\n",
      "running loss : 1.309962\n",
      "running loss : 1.338302\n",
      "running loss : 1.312186\n",
      "running loss : 1.311274\n",
      "running loss : 1.342999\n",
      "running loss : 1.338227\n",
      "running loss : 1.307878\n",
      "running loss : 1.319461\n",
      "running loss : 1.331827\n",
      "running loss : 1.328565\n",
      "running loss : 1.331879\n",
      "running loss : 1.311360\n",
      "running loss : 1.299134\n",
      "running loss : 1.324760\n",
      "running loss : 1.313306\n",
      "running loss : 1.316281\n",
      "running loss : 1.305253\n",
      "running loss : 1.315685\n",
      "running loss : 1.317775\n",
      "running loss : 1.318027\n",
      "running loss : 1.339236\n",
      "running loss : 1.330730\n",
      "running loss : 1.323120\n",
      "running loss : 1.319015\n",
      "running loss : 1.312017\n",
      "running loss : 1.307061\n",
      "running loss : 1.319166\n",
      "running loss : 1.346056\n",
      "running loss : 1.315712\n",
      "running loss : 1.294081\n",
      "running loss : 1.319392\n",
      "running loss : 1.303028\n",
      "running loss : 1.327935\n",
      "running loss : 1.297460\n",
      "running loss : 1.317943\n",
      "running loss : 1.311589\n",
      "running loss : 1.296249\n",
      "running loss : 1.323215\n",
      "running loss : 1.297894\n",
      "running loss : 1.316654\n",
      "running loss : 1.302424\n",
      "running loss : 1.328416\n",
      "running loss : 1.306414\n",
      "running loss : 1.307190\n",
      "running loss : 1.320270\n",
      "running loss : 1.308217\n",
      "running loss : 1.302082\n",
      "running loss : 1.314547\n",
      "running loss : 1.310705\n",
      "running loss : 1.309901\n",
      "running loss : 1.306000\n",
      "running loss : 1.324364\n",
      "running loss : 1.296754\n",
      "running loss : 1.307630\n",
      "running loss : 1.314983\n",
      "running loss : 1.293583\n",
      "running loss : 1.297408\n",
      "running loss : 1.298493\n",
      "running loss : 1.332852\n",
      "running loss : 1.307246\n",
      "running loss : 1.303714\n",
      "running loss : 1.295330\n",
      "running loss : 1.303102\n",
      "running loss : 1.321368\n",
      "running loss : 1.313322\n",
      "running loss : 1.307099\n",
      "running loss : 1.315525\n",
      "running loss : 1.315473\n",
      "running loss : 1.332779\n",
      "running loss : 1.304766\n",
      "running loss : 1.327366\n",
      "running loss : 1.296244\n",
      "running loss : 1.286119\n",
      "running loss : 1.308799\n",
      "running loss : 1.318947\n",
      "running loss : 1.291203\n",
      "running loss : 1.291919\n",
      "running loss : 1.308334\n",
      "running loss : 1.307815\n",
      "running loss : 1.308051\n",
      "running loss : 1.294500\n",
      "running loss : 1.309705\n",
      "running loss : 1.295401\n",
      "running loss : 1.292415\n",
      "running loss : 1.298779\n",
      "running loss : 1.309553\n",
      "running loss : 1.294938\n",
      "running loss : 1.298032\n",
      "running loss : 1.301592\n",
      "running loss : 1.326619\n",
      "running loss : 1.291768\n",
      "running loss : 1.291700\n",
      "running loss : 1.307786\n",
      "running loss : 1.304635\n",
      "running loss : 1.290442\n",
      "running loss : 1.312888\n",
      "running loss : 1.300882\n",
      "running loss : 1.304709\n",
      "running loss : 1.311761\n",
      "running loss : 1.287583\n",
      "running loss : 1.295226\n",
      "running loss : 1.281776\n",
      "running loss : 1.295059\n",
      "running loss : 1.285077\n",
      "running loss : 1.281783\n",
      "running loss : 1.315960\n",
      "running loss : 1.319297\n",
      "running loss : 1.310222\n",
      "running loss : 1.300240\n",
      "running loss : 1.296300\n",
      "running loss : 1.284387\n",
      "running loss : 1.281396\n",
      "running loss : 1.287001\n",
      "running loss : 1.283489\n",
      "running loss : 1.295750\n",
      "running loss : 1.280480\n",
      "running loss : 1.292585\n",
      "running loss : 1.313123\n",
      "running loss : 1.305695\n",
      "running loss : 1.342429\n",
      "running loss : 1.310381\n",
      "running loss : 1.316955\n",
      "running loss : 1.308688\n",
      "running loss : 1.278080\n",
      "running loss : 1.299677\n",
      "running loss : 1.295306\n",
      "running loss : 1.284868\n",
      "running loss : 1.285491\n",
      "running loss : 1.292060\n",
      "running loss : 1.282300\n",
      "running loss : 1.291821\n",
      "running loss : 1.278511\n",
      "running loss : 1.285663\n",
      "running loss : 1.276724\n",
      "running loss : 1.293538\n",
      "running loss : 1.315926\n",
      "running loss : 1.294549\n",
      "running loss : 1.306651\n",
      "running loss : 1.293159\n",
      "running loss : 1.312771\n",
      "running loss : 1.284521\n",
      "running loss : 1.288765\n",
      "running loss : 1.308333\n",
      "running loss : 1.281742\n",
      "running loss : 1.287726\n",
      "running loss : 1.293822\n",
      "running loss : 1.293281\n",
      "running loss : 1.276481\n",
      "running loss : 1.291931\n",
      "running loss : 1.315721\n",
      "running loss : 1.300457\n",
      "running loss : 1.291740\n",
      "running loss : 1.322081\n",
      "running loss : 1.303027\n",
      "running loss : 1.283774\n",
      "running loss : 1.296355\n",
      "running loss : 1.296946\n",
      "running loss : 1.308672\n",
      "running loss : 1.291813\n",
      "running loss : 1.323099\n",
      "running loss : 1.301213\n",
      "running loss : 1.283457\n",
      "running loss : 1.268331\n",
      "running loss : 1.295973\n",
      "running loss : 1.273197\n",
      "running loss : 1.313126\n",
      "running loss : 1.317788\n",
      "running loss : 1.272413\n",
      "running loss : 1.299623\n",
      "running loss : 1.305961\n",
      "running loss : 1.274554\n",
      "running loss : 1.296188\n",
      "running loss : 1.281517\n",
      "running loss : 1.259198\n",
      "running loss : 1.291144\n",
      "running loss : 1.279602\n",
      "running loss : 1.281434\n",
      "running loss : 1.279281\n",
      "running loss : 1.278834\n",
      "running loss : 1.276706\n",
      "running loss : 1.293759\n",
      "running loss : 1.271042\n",
      "running loss : 1.305040\n",
      "running loss : 1.270618\n",
      "running loss : 1.267501\n",
      "running loss : 1.305159\n",
      "running loss : 1.273279\n",
      "running loss : 1.271507\n",
      "running loss : 1.321874\n",
      "running loss : 1.261374\n",
      "running loss : 1.273816\n",
      "running loss : 1.280922\n",
      "running loss : 1.263342\n",
      "running loss : 1.274527\n",
      "running loss : 1.263978\n",
      "running loss : 1.275786\n",
      "running loss : 1.286972\n",
      "running loss : 1.270228\n",
      "running loss : 1.298264\n",
      "running loss : 1.263915\n",
      "running loss : 1.271465\n",
      "running loss : 1.265583\n",
      "running loss : 1.271941\n",
      "running loss : 1.273792\n",
      "running loss : 1.258561\n",
      "running loss : 1.277911\n",
      "running loss : 1.311572\n",
      "running loss : 1.271203\n",
      "running loss : 1.268134\n",
      "running loss : 1.278459\n",
      "running loss : 1.271878\n",
      "running loss : 1.268762\n",
      "running loss : 1.253604\n",
      "running loss : 1.270452\n",
      "running loss : 1.255710\n",
      "running loss : 1.280091\n",
      "running loss : 1.269017\n",
      "running loss : 1.261922\n",
      "running loss : 1.270501\n",
      "running loss : 1.259109\n",
      "running loss : 1.255278\n",
      "running loss : 1.265431\n",
      "running loss : 1.238313\n",
      "running loss : 1.270453\n",
      "running loss : 1.281058\n",
      "running loss : 1.257326\n",
      "running loss : 1.284543\n",
      "running loss : 1.271059\n",
      "running loss : 1.242640\n",
      "running loss : 1.260815\n",
      "running loss : 1.274934\n",
      "running loss : 1.267736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.267160\n",
      "running loss : 1.258045\n",
      "running loss : 1.265366\n",
      "running loss : 1.271636\n",
      "running loss : 1.253043\n",
      "running loss : 1.305992\n",
      "running loss : 1.253648\n",
      "running loss : 1.291248\n",
      "running loss : 1.249438\n",
      "running loss : 1.246509\n",
      "running loss : 1.257937\n",
      "running loss : 1.241104\n",
      "running loss : 1.255326\n",
      "running loss : 1.263945\n",
      "running loss : 1.261370\n",
      "running loss : 1.250158\n",
      "running loss : 1.244916\n",
      "running loss : 1.235820\n",
      "running loss : 1.248609\n",
      "running loss : 1.238990\n",
      "running loss : 1.225600\n",
      "running loss : 1.228770\n",
      "running loss : 1.239446\n",
      "running loss : 1.242232\n",
      "running loss : 1.241832\n",
      "running loss : 1.276359\n",
      "running loss : 1.245770\n",
      "running loss : 1.258276\n",
      "running loss : 1.266466\n",
      "running loss : 1.246553\n",
      "running loss : 1.242128\n",
      "running loss : 1.258255\n",
      "running loss : 1.265401\n",
      "running loss : 1.240727\n",
      "running loss : 1.239180\n",
      "running loss : 1.244977\n",
      "running loss : 1.240612\n",
      "running loss : 1.242270\n",
      "running loss : 1.240866\n",
      "running loss : 1.251731\n",
      "running loss : 1.274460\n",
      "running loss : 1.251560\n",
      "running loss : 1.237957\n",
      "running loss : 1.258748\n",
      "running loss : 1.239573\n",
      "running loss : 1.241650\n",
      "running loss : 1.234505\n",
      "running loss : 1.223459\n",
      "running loss : 1.244873\n",
      "running loss : 1.272918\n",
      "running loss : 1.238848\n",
      "running loss : 1.234008\n",
      "running loss : 1.256698\n",
      "running loss : 1.251679\n",
      "running loss : 1.242237\n",
      "running loss : 1.243809\n",
      "running loss : 1.241152\n",
      "running loss : 1.239636\n",
      "running loss : 1.240928\n",
      "running loss : 1.227116\n",
      "running loss : 1.241802\n",
      "running loss : 1.241646\n",
      "running loss : 1.254352\n",
      "running loss : 1.256583\n",
      "running loss : 1.245652\n",
      "running loss : 1.231553\n",
      "running loss : 1.232922\n",
      "running loss : 1.232458\n",
      "running loss : 1.228173\n",
      "running loss : 1.242596\n",
      "running loss : 1.260307\n",
      "running loss : 1.224408\n",
      "running loss : 1.228676\n",
      "running loss : 1.238299\n",
      "running loss : 1.237622\n",
      "running loss : 1.233104\n",
      "running loss : 1.235921\n",
      "running loss : 1.225967\n",
      "running loss : 1.232498\n",
      "running loss : 1.238190\n",
      "running loss : 1.240256\n",
      "running loss : 1.238650\n",
      "running loss : 1.225322\n",
      "running loss : 1.247437\n",
      "running loss : 1.237060\n",
      "running loss : 1.227938\n",
      "running loss : 1.231161\n",
      "running loss : 1.235035\n",
      "running loss : 1.258850\n",
      "running loss : 1.230320\n",
      "running loss : 1.237307\n",
      "running loss : 1.226543\n",
      "running loss : 1.234420\n",
      "running loss : 1.233221\n",
      "running loss : 1.220963\n",
      "running loss : 1.238549\n",
      "running loss : 1.227471\n",
      "running loss : 1.231425\n",
      "running loss : 1.232301\n",
      "running loss : 1.210316\n",
      "running loss : 1.219481\n",
      "running loss : 1.209090\n",
      "running loss : 1.230977\n",
      "running loss : 1.227630\n",
      "running loss : 1.237712\n",
      "running loss : 1.218202\n",
      "running loss : 1.222381\n",
      "running loss : 1.237454\n",
      "running loss : 1.241462\n",
      "running loss : 1.220434\n",
      "running loss : 1.226425\n",
      "running loss : 1.216752\n",
      "running loss : 1.222037\n",
      "running loss : 1.217627\n",
      "running loss : 1.235053\n",
      "running loss : 1.224675\n",
      "running loss : 1.241215\n",
      "running loss : 1.211270\n",
      "running loss : 1.227668\n",
      "running loss : 1.238719\n",
      "running loss : 1.221310\n",
      "running loss : 1.229888\n",
      "running loss : 1.235111\n",
      "running loss : 1.223990\n",
      "running loss : 1.229188\n",
      "running loss : 1.231893\n",
      "running loss : 1.228154\n",
      "running loss : 1.214357\n",
      "running loss : 1.234041\n",
      "running loss : 1.216022\n",
      "running loss : 1.240165\n",
      "running loss : 1.238206\n",
      "running loss : 1.212237\n",
      "running loss : 1.231373\n",
      "running loss : 1.225200\n",
      "running loss : 1.219109\n",
      "running loss : 1.224750\n",
      "running loss : 1.224919\n",
      "running loss : 1.229032\n",
      "running loss : 1.215012\n",
      "running loss : 1.216803\n",
      "running loss : 1.230371\n",
      "running loss : 1.213065\n",
      "running loss : 1.218442\n",
      "running loss : 1.209069\n",
      "running loss : 1.208908\n",
      "running loss : 1.221695\n",
      "running loss : 1.210408\n",
      "running loss : 1.219748\n",
      "running loss : 1.223008\n",
      "running loss : 1.208038\n",
      "running loss : 1.220370\n",
      "running loss : 1.219235\n",
      "running loss : 1.223454\n",
      "running loss : 1.218263\n",
      "running loss : 1.212482\n",
      "running loss : 1.215322\n",
      "running loss : 1.217968\n",
      "running loss : 1.207621\n",
      "running loss : 1.221840\n",
      "running loss : 1.209059\n",
      "running loss : 1.212044\n",
      "running loss : 1.225197\n",
      "running loss : 1.198015\n",
      "running loss : 1.223945\n",
      "running loss : 1.244268\n",
      "running loss : 1.211533\n",
      "running loss : 1.221798\n",
      "running loss : 1.232066\n",
      "running loss : 1.234642\n",
      "running loss : 1.208906\n",
      "running loss : 1.240144\n",
      "running loss : 1.234852\n",
      "running loss : 1.210713\n",
      "running loss : 1.215707\n",
      "running loss : 1.222920\n",
      "running loss : 1.225641\n",
      "running loss : 1.222323\n",
      "running loss : 1.218734\n",
      "running loss : 1.236537\n",
      "running loss : 1.214817\n",
      "running loss : 1.243915\n",
      "running loss : 1.206826\n",
      "running loss : 1.225382\n",
      "running loss : 1.234308\n",
      "running loss : 1.214521\n",
      "running loss : 1.205686\n",
      "running loss : 1.240153\n",
      "running loss : 1.225539\n",
      "running loss : 1.194611\n",
      "running loss : 1.192195\n",
      "running loss : 1.223435\n",
      "running loss : 1.193994\n",
      "running loss : 1.213159\n",
      "running loss : 1.212745\n",
      "running loss : 1.211214\n",
      "running loss : 1.193661\n",
      "running loss : 1.215673\n",
      "running loss : 1.216787\n",
      "running loss : 1.219853\n",
      "running loss : 1.235553\n",
      "running loss : 1.201654\n",
      "running loss : 1.206529\n",
      "running loss : 1.210116\n",
      "running loss : 1.205815\n",
      "running loss : 1.216683\n",
      "running loss : 1.221585\n",
      "running loss : 1.214599\n",
      "running loss : 1.229142\n",
      "running loss : 1.203094\n",
      "running loss : 1.212914\n",
      "running loss : 1.209544\n",
      "running loss : 1.209915\n",
      "running loss : 1.214985\n",
      "running loss : 1.214520\n",
      "running loss : 1.225647\n",
      "running loss : 1.219423\n",
      "running loss : 1.193533\n",
      "running loss : 1.209040\n",
      "running loss : 1.225803\n",
      "running loss : 1.238490\n",
      "running loss : 1.205486\n",
      "running loss : 1.194923\n",
      "running loss : 1.207429\n",
      "running loss : 1.198840\n",
      "running loss : 1.196940\n",
      "running loss : 1.213600\n",
      "running loss : 1.196307\n",
      "running loss : 1.190709\n",
      "running loss : 1.203523\n",
      "running loss : 1.196286\n",
      "running loss : 1.202566\n",
      "running loss : 1.194037\n",
      "running loss : 1.208516\n",
      "running loss : 1.195700\n",
      "running loss : 1.206232\n",
      "running loss : 1.184090\n",
      "running loss : 1.213594\n",
      "running loss : 1.194260\n",
      "running loss : 1.198538\n",
      "running loss : 1.207170\n",
      "running loss : 1.209740\n",
      "running loss : 1.198157\n",
      "running loss : 1.193960\n",
      "running loss : 1.180504\n",
      "running loss : 1.189989\n",
      "running loss : 1.213036\n",
      "running loss : 1.193874\n",
      "running loss : 1.207213\n",
      "running loss : 1.204887\n",
      "running loss : 1.189698\n",
      "running loss : 1.197524\n",
      "running loss : 1.200611\n",
      "running loss : 1.200918\n",
      "running loss : 1.187491\n",
      "running loss : 1.184279\n",
      "running loss : 1.189579\n",
      "running loss : 1.177180\n",
      "running loss : 1.180357\n",
      "running loss : 1.189077\n",
      "running loss : 1.181990\n",
      "running loss : 1.190558\n",
      "running loss : 1.188992\n",
      "running loss : 1.183348\n",
      "running loss : 1.180824\n",
      "running loss : 1.196036\n",
      "running loss : 1.197821\n",
      "running loss : 1.190984\n",
      "running loss : 1.177662\n",
      "running loss : 1.188154\n",
      "running loss : 1.194905\n",
      "running loss : 1.183494\n",
      "running loss : 1.191967\n",
      "running loss : 1.207183\n",
      "running loss : 1.182579\n",
      "running loss : 1.186054\n",
      "running loss : 1.179980\n",
      "running loss : 1.210522\n",
      "running loss : 1.181470\n",
      "running loss : 1.188638\n",
      "running loss : 1.177992\n",
      "running loss : 1.171983\n",
      "running loss : 1.179578\n",
      "running loss : 1.181313\n",
      "running loss : 1.190870\n",
      "running loss : 1.201963\n",
      "running loss : 1.174170\n",
      "running loss : 1.181312\n",
      "running loss : 1.181134\n",
      "running loss : 1.196233\n",
      "running loss : 1.184844\n",
      "running loss : 1.180386\n",
      "running loss : 1.173730\n",
      "running loss : 1.202430\n",
      "running loss : 1.186717\n",
      "running loss : 1.190411\n",
      "running loss : 1.164662\n",
      "running loss : 1.176861\n",
      "running loss : 1.167033\n",
      "running loss : 1.175437\n",
      "running loss : 1.177568\n",
      "running loss : 1.186662\n",
      "running loss : 1.196263\n",
      "running loss : 1.176988\n",
      "running loss : 1.171894\n",
      "running loss : 1.175720\n",
      "running loss : 1.170597\n",
      "running loss : 1.167332\n",
      "running loss : 1.193688\n",
      "running loss : 1.169665\n",
      "running loss : 1.184550\n",
      "running loss : 1.181275\n",
      "running loss : 1.185758\n",
      "running loss : 1.177024\n",
      "running loss : 1.179204\n",
      "running loss : 1.173540\n",
      "running loss : 1.179208\n",
      "running loss : 1.176201\n",
      "running loss : 1.186596\n",
      "running loss : 1.179911\n",
      "running loss : 1.171764\n",
      "running loss : 1.177659\n",
      "running loss : 1.192587\n",
      "running loss : 1.178128\n",
      "running loss : 1.174038\n",
      "running loss : 1.178701\n",
      "running loss : 1.183925\n",
      "running loss : 1.182598\n",
      "running loss : 1.176755\n",
      "running loss : 1.177418\n",
      "running loss : 1.173613\n",
      "running loss : 1.184251\n",
      "running loss : 1.175190\n",
      "running loss : 1.198965\n",
      "running loss : 1.182586\n",
      "running loss : 1.196972\n",
      "running loss : 1.191616\n",
      "running loss : 1.170654\n",
      "running loss : 1.180247\n",
      "running loss : 1.179960\n",
      "running loss : 1.164927\n",
      "running loss : 1.176256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.168035\n",
      "running loss : 1.177131\n",
      "running loss : 1.169541\n",
      "running loss : 1.184110\n",
      "running loss : 1.171692\n",
      "running loss : 1.168042\n",
      "running loss : 1.184067\n",
      "running loss : 1.174212\n",
      "running loss : 1.184579\n",
      "running loss : 1.184333\n",
      "running loss : 1.153397\n",
      "running loss : 1.174612\n",
      "running loss : 1.187734\n",
      "running loss : 1.170934\n",
      "running loss : 1.180597\n",
      "running loss : 1.172961\n",
      "running loss : 1.178810\n",
      "running loss : 1.227051\n",
      "running loss : 1.169555\n",
      "running loss : 1.171658\n",
      "running loss : 1.189149\n",
      "running loss : 1.171815\n",
      "running loss : 1.201739\n",
      "running loss : 1.177510\n",
      "running loss : 1.174195\n",
      "running loss : 1.174510\n",
      "running loss : 1.176440\n",
      "running loss : 1.183484\n",
      "running loss : 1.199525\n",
      "running loss : 1.162340\n",
      "running loss : 1.183398\n",
      "running loss : 1.167393\n",
      "running loss : 1.179574\n",
      "running loss : 1.170342\n",
      "running loss : 1.174819\n",
      "running loss : 1.187078\n",
      "running loss : 1.162319\n",
      "running loss : 1.175559\n",
      "running loss : 1.162379\n",
      "running loss : 1.175358\n",
      "running loss : 1.189886\n",
      "running loss : 1.165128\n",
      "running loss : 1.173229\n",
      "running loss : 1.177695\n",
      "running loss : 1.194849\n",
      "running loss : 1.181467\n",
      "running loss : 1.174026\n",
      "running loss : 1.171095\n",
      "running loss : 1.176565\n",
      "running loss : 1.175584\n",
      "running loss : 1.167301\n",
      "running loss : 1.164627\n",
      "running loss : 1.181762\n",
      "running loss : 1.162978\n",
      "running loss : 1.168407\n",
      "running loss : 1.182408\n",
      "running loss : 1.170804\n",
      "running loss : 1.184172\n",
      "running loss : 1.194098\n",
      "running loss : 1.156182\n",
      "running loss : 1.179481\n",
      "running loss : 1.174320\n",
      "running loss : 1.178982\n",
      "running loss : 1.163562\n",
      "running loss : 1.191544\n",
      "running loss : 1.174956\n",
      "running loss : 1.182484\n",
      "running loss : 1.183714\n",
      "running loss : 1.182285\n",
      "running loss : 1.157624\n",
      "running loss : 1.186196\n",
      "running loss : 1.158907\n",
      "running loss : 1.191118\n",
      "running loss : 1.162262\n",
      "running loss : 1.163997\n",
      "running loss : 1.157625\n",
      "running loss : 1.160515\n",
      "running loss : 1.166121\n",
      "running loss : 1.182756\n",
      "running loss : 1.170505\n",
      "running loss : 1.180392\n",
      "running loss : 1.170259\n",
      "running loss : 1.180738\n",
      "running loss : 1.170192\n",
      "running loss : 1.182565\n",
      "running loss : 1.158321\n",
      "running loss : 1.156620\n",
      "running loss : 1.169701\n",
      "running loss : 1.201799\n",
      "running loss : 1.181099\n",
      "running loss : 1.169085\n",
      "running loss : 1.175633\n",
      "running loss : 1.179945\n",
      "running loss : 1.164717\n",
      "running loss : 1.165932\n",
      "running loss : 1.165578\n",
      "running loss : 1.158353\n",
      "running loss : 1.164520\n",
      "running loss : 1.157491\n",
      "running loss : 1.157803\n",
      "running loss : 1.162260\n",
      "running loss : 1.158149\n",
      "running loss : 1.167458\n",
      "running loss : 1.183505\n",
      "running loss : 1.164161\n",
      "running loss : 1.148606\n",
      "running loss : 1.162706\n",
      "running loss : 1.157485\n",
      "running loss : 1.168911\n",
      "running loss : 1.166154\n",
      "running loss : 1.181206\n",
      "running loss : 1.157064\n",
      "running loss : 1.190522\n",
      "running loss : 1.169075\n",
      "running loss : 1.180461\n",
      "running loss : 1.163092\n",
      "running loss : 1.171272\n",
      "running loss : 1.181930\n",
      "running loss : 1.170142\n",
      "running loss : 1.179570\n",
      "running loss : 1.186839\n",
      "running loss : 1.164401\n",
      "running loss : 1.170173\n",
      "running loss : 1.146691\n",
      "running loss : 1.165649\n",
      "running loss : 1.167776\n",
      "running loss : 1.161640\n",
      "running loss : 1.155104\n",
      "running loss : 1.157051\n",
      "running loss : 1.163962\n",
      "running loss : 1.169721\n",
      "running loss : 1.176926\n",
      "running loss : 1.193578\n",
      "running loss : 1.162150\n",
      "running loss : 1.166709\n",
      "running loss : 1.166820\n",
      "running loss : 1.177993\n",
      "running loss : 1.164576\n",
      "running loss : 1.170025\n",
      "running loss : 1.194406\n",
      "running loss : 1.169117\n",
      "running loss : 1.158217\n",
      "running loss : 1.174693\n",
      "running loss : 1.169638\n",
      "running loss : 1.166308\n",
      "running loss : 1.175858\n",
      "running loss : 1.159379\n",
      "running loss : 1.144170\n",
      "running loss : 1.172078\n",
      "running loss : 1.166338\n",
      "running loss : 1.173661\n",
      "running loss : 1.154725\n",
      "running loss : 1.182839\n",
      "running loss : 1.190086\n",
      "running loss : 1.180156\n",
      "running loss : 1.182707\n",
      "running loss : 1.183892\n",
      "running loss : 1.163412\n",
      "running loss : 1.155686\n",
      "running loss : 1.164172\n",
      "running loss : 1.157898\n",
      "running loss : 1.165732\n",
      "running loss : 1.145522\n",
      "running loss : 1.184452\n",
      "running loss : 1.165760\n",
      "running loss : 1.172241\n",
      "running loss : 1.183796\n",
      "running loss : 1.174269\n",
      "running loss : 1.167026\n",
      "running loss : 1.165838\n",
      "running loss : 1.159637\n",
      "running loss : 1.161348\n",
      "running loss : 1.160008\n",
      "running loss : 1.179445\n",
      "running loss : 1.140582\n",
      "running loss : 1.159760\n",
      "running loss : 1.158554\n",
      "running loss : 1.156977\n",
      "running loss : 1.159774\n",
      "running loss : 1.159003\n",
      "running loss : 1.162992\n",
      "running loss : 1.166386\n",
      "running loss : 1.168669\n",
      "running loss : 1.161946\n",
      "running loss : 1.155957\n",
      "running loss : 1.164163\n",
      "running loss : 1.155023\n",
      "running loss : 1.150751\n",
      "running loss : 1.162100\n",
      "running loss : 1.157601\n",
      "running loss : 1.160210\n",
      "running loss : 1.173463\n",
      "running loss : 1.150875\n",
      "running loss : 1.191213\n",
      "running loss : 1.168786\n",
      "running loss : 1.155264\n",
      "running loss : 1.156397\n",
      "running loss : 1.156625\n",
      "running loss : 1.144357\n",
      "running loss : 1.173260\n",
      "running loss : 1.149103\n",
      "running loss : 1.176021\n",
      "running loss : 1.149762\n",
      "running loss : 1.164895\n",
      "running loss : 1.149647\n",
      "running loss : 1.157595\n",
      "running loss : 1.158815\n",
      "running loss : 1.166416\n",
      "running loss : 1.159058\n",
      "running loss : 1.171707\n",
      "running loss : 1.144711\n",
      "running loss : 1.152744\n",
      "running loss : 1.136914\n",
      "running loss : 1.157494\n",
      "running loss : 1.147882\n",
      "running loss : 1.160530\n",
      "running loss : 1.159794\n",
      "running loss : 1.144503\n",
      "running loss : 1.137917\n",
      "running loss : 1.140251\n",
      "running loss : 1.135276\n",
      "running loss : 1.156276\n",
      "running loss : 1.150147\n",
      "running loss : 1.151611\n",
      "running loss : 1.164031\n",
      "running loss : 1.150913\n",
      "running loss : 1.144429\n",
      "running loss : 1.143104\n",
      "running loss : 1.142267\n",
      "running loss : 1.163976\n",
      "running loss : 1.172618\n",
      "running loss : 1.158603\n",
      "running loss : 1.155737\n",
      "running loss : 1.167720\n",
      "running loss : 1.146950\n",
      "running loss : 1.145840\n",
      "running loss : 1.147870\n",
      "running loss : 1.150903\n",
      "running loss : 1.143557\n",
      "running loss : 1.128461\n",
      "running loss : 1.162142\n",
      "running loss : 1.159552\n",
      "running loss : 1.134742\n",
      "running loss : 1.133555\n",
      "running loss : 1.140872\n",
      "running loss : 1.135682\n",
      "running loss : 1.151228\n",
      "running loss : 1.135315\n",
      "running loss : 1.149875\n",
      "running loss : 1.152336\n",
      "running loss : 1.132848\n",
      "running loss : 1.135664\n",
      "running loss : 1.141001\n",
      "running loss : 1.160234\n",
      "running loss : 1.156160\n",
      "running loss : 1.143704\n",
      "running loss : 1.128384\n",
      "running loss : 1.152765\n",
      "running loss : 1.147870\n",
      "running loss : 1.148237\n",
      "running loss : 1.150223\n",
      "running loss : 1.150155\n",
      "running loss : 1.131120\n",
      "running loss : 1.127650\n",
      "running loss : 1.163216\n",
      "running loss : 1.130367\n",
      "running loss : 1.153165\n",
      "running loss : 1.144643\n",
      "running loss : 1.137420\n",
      "running loss : 1.167700\n",
      "running loss : 1.138088\n",
      "running loss : 1.145165\n",
      "running loss : 1.138796\n",
      "running loss : 1.147310\n",
      "running loss : 1.144172\n",
      "running loss : 1.138730\n",
      "running loss : 1.147354\n",
      "running loss : 1.149916\n",
      "running loss : 1.132521\n",
      "running loss : 1.151670\n",
      "running loss : 1.129622\n",
      "running loss : 1.127154\n",
      "running loss : 1.131891\n",
      "running loss : 1.148541\n",
      "running loss : 1.133552\n",
      "running loss : 1.144083\n",
      "running loss : 1.140896\n",
      "running loss : 1.120699\n",
      "running loss : 1.143767\n",
      "running loss : 1.137826\n",
      "running loss : 1.144664\n",
      "running loss : 1.141792\n",
      "running loss : 1.147056\n",
      "running loss : 1.129899\n",
      "running loss : 1.118618\n",
      "running loss : 1.129005\n",
      "running loss : 1.134547\n",
      "running loss : 1.149739\n",
      "running loss : 1.133208\n",
      "running loss : 1.126290\n",
      "running loss : 1.136272\n",
      "running loss : 1.136775\n",
      "running loss : 1.123324\n",
      "running loss : 1.129143\n",
      "running loss : 1.119081\n",
      "running loss : 1.130739\n",
      "running loss : 1.117308\n",
      "running loss : 1.129697\n",
      "running loss : 1.139143\n",
      "running loss : 1.140489\n",
      "running loss : 1.119944\n",
      "running loss : 1.122238\n",
      "running loss : 1.137394\n",
      "running loss : 1.134602\n",
      "running loss : 1.123922\n",
      "running loss : 1.134833\n",
      "running loss : 1.142611\n",
      "running loss : 1.130521\n",
      "running loss : 1.121597\n",
      "running loss : 1.123938\n",
      "running loss : 1.121358\n",
      "running loss : 1.124456\n",
      "running loss : 1.135656\n",
      "running loss : 1.133436\n",
      "running loss : 1.132991\n",
      "running loss : 1.131015\n",
      "running loss : 1.120399\n",
      "running loss : 1.125927\n",
      "running loss : 1.132751\n",
      "running loss : 1.142251\n",
      "running loss : 1.131971\n",
      "running loss : 1.117710\n",
      "running loss : 1.158541\n",
      "running loss : 1.132617\n",
      "running loss : 1.118800\n",
      "running loss : 1.137945\n",
      "running loss : 1.115578\n",
      "running loss : 1.120974\n",
      "running loss : 1.123020\n",
      "running loss : 1.127704\n",
      "running loss : 1.136825\n",
      "running loss : 1.149232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.147721\n",
      "running loss : 1.133149\n",
      "running loss : 1.124233\n",
      "running loss : 1.124856\n",
      "running loss : 1.133965\n",
      "running loss : 1.133265\n",
      "running loss : 1.129545\n",
      "running loss : 1.120841\n",
      "running loss : 1.128602\n",
      "running loss : 1.130722\n",
      "running loss : 1.139900\n",
      "running loss : 1.129132\n",
      "running loss : 1.137043\n",
      "running loss : 1.131058\n",
      "running loss : 1.139340\n",
      "running loss : 1.122029\n",
      "running loss : 1.119665\n",
      "running loss : 1.121690\n",
      "running loss : 1.137413\n",
      "running loss : 1.142731\n",
      "running loss : 1.122546\n",
      "running loss : 1.128498\n",
      "running loss : 1.122811\n",
      "running loss : 1.151673\n",
      "running loss : 1.127672\n",
      "running loss : 1.133700\n",
      "running loss : 1.131048\n",
      "running loss : 1.135637\n",
      "running loss : 1.127515\n",
      "running loss : 1.117530\n",
      "running loss : 1.115123\n",
      "running loss : 1.113537\n",
      "running loss : 1.144067\n",
      "running loss : 1.121379\n",
      "running loss : 1.115643\n",
      "running loss : 1.121345\n",
      "running loss : 1.126926\n",
      "running loss : 1.106541\n",
      "running loss : 1.107351\n",
      "running loss : 1.122868\n",
      "running loss : 1.134146\n",
      "running loss : 1.111914\n",
      "running loss : 1.112691\n",
      "running loss : 1.118135\n",
      "running loss : 1.134824\n",
      "running loss : 1.116375\n",
      "running loss : 1.112418\n",
      "running loss : 1.146509\n",
      "running loss : 1.120867\n",
      "running loss : 1.110368\n",
      "running loss : 1.106457\n",
      "running loss : 1.117061\n",
      "running loss : 1.125432\n",
      "running loss : 1.105311\n",
      "running loss : 1.112743\n",
      "running loss : 1.117097\n",
      "running loss : 1.128661\n",
      "running loss : 1.110090\n",
      "running loss : 1.117557\n",
      "running loss : 1.116226\n",
      "running loss : 1.117840\n",
      "running loss : 1.101114\n",
      "running loss : 1.115553\n",
      "running loss : 1.108618\n",
      "running loss : 1.140878\n",
      "running loss : 1.123119\n",
      "running loss : 1.108847\n",
      "running loss : 1.106302\n",
      "running loss : 1.114646\n",
      "running loss : 1.115144\n",
      "running loss : 1.105760\n",
      "running loss : 1.124491\n",
      "running loss : 1.109696\n",
      "running loss : 1.131646\n",
      "running loss : 1.129105\n",
      "running loss : 1.104426\n",
      "running loss : 1.139236\n",
      "running loss : 1.107337\n",
      "running loss : 1.108561\n",
      "running loss : 1.106897\n",
      "running loss : 1.106100\n",
      "running loss : 1.110132\n",
      "running loss : 1.123927\n",
      "running loss : 1.110021\n",
      "running loss : 1.103149\n",
      "running loss : 1.112167\n",
      "running loss : 1.109612\n",
      "running loss : 1.105734\n",
      "running loss : 1.102206\n",
      "running loss : 1.117336\n",
      "running loss : 1.107268\n",
      "running loss : 1.098344\n",
      "running loss : 1.119180\n",
      "running loss : 1.099497\n",
      "running loss : 1.099824\n",
      "running loss : 1.113972\n",
      "running loss : 1.098949\n",
      "running loss : 1.126322\n",
      "running loss : 1.108055\n",
      "running loss : 1.118488\n",
      "running loss : 1.108253\n",
      "running loss : 1.113769\n",
      "running loss : 1.105784\n",
      "running loss : 1.104506\n",
      "running loss : 1.094860\n",
      "running loss : 1.107995\n",
      "running loss : 1.094781\n",
      "running loss : 1.102726\n",
      "running loss : 1.097042\n",
      "running loss : 1.097312\n",
      "running loss : 1.112282\n",
      "running loss : 1.097053\n",
      "running loss : 1.096559\n",
      "running loss : 1.105936\n",
      "running loss : 1.116487\n",
      "running loss : 1.095752\n",
      "running loss : 1.096314\n",
      "running loss : 1.097071\n",
      "running loss : 1.117888\n",
      "running loss : 1.099397\n",
      "running loss : 1.133983\n",
      "running loss : 1.106252\n",
      "running loss : 1.094864\n",
      "running loss : 1.104807\n",
      "running loss : 1.105292\n",
      "running loss : 1.128064\n",
      "running loss : 1.104893\n",
      "running loss : 1.116709\n",
      "running loss : 1.129819\n",
      "running loss : 1.116193\n",
      "running loss : 1.114097\n",
      "running loss : 1.124165\n",
      "running loss : 1.105948\n",
      "running loss : 1.102818\n",
      "running loss : 1.108243\n",
      "running loss : 1.102651\n",
      "running loss : 1.106122\n",
      "running loss : 1.098943\n",
      "running loss : 1.118649\n",
      "running loss : 1.101122\n",
      "running loss : 1.097315\n",
      "running loss : 1.093123\n",
      "running loss : 1.101561\n",
      "running loss : 1.093654\n",
      "running loss : 1.088180\n",
      "running loss : 1.098355\n",
      "running loss : 1.104375\n",
      "running loss : 1.092017\n",
      "running loss : 1.112806\n",
      "running loss : 1.098232\n",
      "running loss : 1.091277\n",
      "running loss : 1.119154\n",
      "running loss : 1.106445\n",
      "running loss : 1.147804\n",
      "running loss : 1.120936\n",
      "running loss : 1.103518\n",
      "running loss : 1.131611\n",
      "running loss : 1.091632\n",
      "running loss : 1.094988\n",
      "running loss : 1.081811\n",
      "running loss : 1.096020\n",
      "running loss : 1.091488\n",
      "running loss : 1.094955\n",
      "running loss : 1.099107\n",
      "running loss : 1.096632\n",
      "running loss : 1.093932\n",
      "running loss : 1.100143\n",
      "running loss : 1.089247\n",
      "running loss : 1.101961\n",
      "running loss : 1.111013\n",
      "running loss : 1.095449\n",
      "running loss : 1.105608\n",
      "running loss : 1.091524\n",
      "running loss : 1.092307\n",
      "running loss : 1.099726\n",
      "running loss : 1.095914\n",
      "running loss : 1.110369\n",
      "running loss : 1.095047\n",
      "running loss : 1.098510\n",
      "running loss : 1.102536\n",
      "running loss : 1.114774\n",
      "running loss : 1.105883\n",
      "running loss : 1.098038\n",
      "running loss : 1.095184\n",
      "running loss : 1.106082\n",
      "running loss : 1.107987\n",
      "running loss : 1.098738\n",
      "running loss : 1.097401\n",
      "running loss : 1.104316\n",
      "running loss : 1.098591\n",
      "running loss : 1.104262\n",
      "running loss : 1.091717\n",
      "running loss : 1.110036\n",
      "running loss : 1.118144\n",
      "running loss : 1.109215\n",
      "running loss : 1.095118\n",
      "running loss : 1.108857\n",
      "running loss : 1.089289\n",
      "running loss : 1.095966\n",
      "running loss : 1.093452\n",
      "running loss : 1.121613\n",
      "running loss : 1.106886\n",
      "running loss : 1.087118\n",
      "running loss : 1.084734\n",
      "running loss : 1.091222\n",
      "running loss : 1.103358\n",
      "running loss : 1.096756\n",
      "running loss : 1.097007\n",
      "running loss : 1.093127\n",
      "running loss : 1.106333\n",
      "running loss : 1.102265\n",
      "running loss : 1.086859\n",
      "running loss : 1.096017\n",
      "running loss : 1.081387\n",
      "running loss : 1.101602\n",
      "running loss : 1.097014\n",
      "running loss : 1.081314\n",
      "running loss : 1.094391\n",
      "running loss : 1.115053\n",
      "running loss : 1.114486\n",
      "running loss : 1.083333\n",
      "running loss : 1.081799\n",
      "running loss : 1.105538\n",
      "running loss : 1.095293\n",
      "running loss : 1.107509\n",
      "running loss : 1.100829\n",
      "running loss : 1.104614\n",
      "running loss : 1.083743\n",
      "running loss : 1.097336\n",
      "running loss : 1.086542\n",
      "running loss : 1.103045\n",
      "running loss : 1.083490\n",
      "running loss : 1.090518\n",
      "running loss : 1.087237\n",
      "running loss : 1.092840\n",
      "running loss : 1.091289\n",
      "running loss : 1.082839\n",
      "running loss : 1.099204\n",
      "running loss : 1.097874\n",
      "running loss : 1.096753\n",
      "running loss : 1.090659\n",
      "running loss : 1.106799\n",
      "running loss : 1.110563\n",
      "running loss : 1.088321\n",
      "running loss : 1.079247\n",
      "running loss : 1.090266\n",
      "running loss : 1.099616\n",
      "running loss : 1.090435\n",
      "running loss : 1.078101\n",
      "running loss : 1.072416\n",
      "running loss : 1.079326\n",
      "running loss : 1.098110\n",
      "running loss : 1.082520\n",
      "running loss : 1.109377\n",
      "running loss : 1.092250\n",
      "running loss : 1.099204\n",
      "running loss : 1.097710\n",
      "running loss : 1.099150\n",
      "running loss : 1.092079\n",
      "running loss : 1.107560\n",
      "running loss : 1.091217\n",
      "running loss : 1.080001\n",
      "running loss : 1.102275\n",
      "running loss : 1.089838\n",
      "running loss : 1.081627\n",
      "running loss : 1.091166\n",
      "running loss : 1.073873\n",
      "running loss : 1.078453\n",
      "running loss : 1.082778\n",
      "running loss : 1.095487\n",
      "running loss : 1.118409\n",
      "running loss : 1.075457\n",
      "running loss : 1.080721\n",
      "running loss : 1.091342\n",
      "running loss : 1.093360\n",
      "running loss : 1.101271\n",
      "running loss : 1.088434\n",
      "running loss : 1.092120\n",
      "running loss : 1.096340\n",
      "running loss : 1.087398\n",
      "running loss : 1.082608\n",
      "running loss : 1.078477\n",
      "running loss : 1.073123\n",
      "running loss : 1.079621\n",
      "running loss : 1.093926\n",
      "running loss : 1.074257\n",
      "running loss : 1.077917\n",
      "running loss : 1.084492\n",
      "running loss : 1.073570\n",
      "running loss : 1.087055\n",
      "running loss : 1.096140\n",
      "running loss : 1.095769\n",
      "running loss : 1.085610\n",
      "running loss : 1.100856\n",
      "running loss : 1.088748\n",
      "running loss : 1.075316\n",
      "running loss : 1.080193\n",
      "running loss : 1.082275\n",
      "running loss : 1.085239\n",
      "running loss : 1.081142\n",
      "running loss : 1.086328\n",
      "running loss : 1.085295\n",
      "running loss : 1.087113\n",
      "running loss : 1.072747\n",
      "running loss : 1.089776\n",
      "running loss : 1.075528\n",
      "running loss : 1.077776\n",
      "running loss : 1.080458\n",
      "running loss : 1.082364\n",
      "running loss : 1.071779\n",
      "running loss : 1.115261\n",
      "running loss : 1.083162\n",
      "running loss : 1.077864\n",
      "running loss : 1.066921\n",
      "running loss : 1.070991\n",
      "running loss : 1.112276\n",
      "running loss : 1.095007\n",
      "running loss : 1.078626\n",
      "running loss : 1.073314\n",
      "running loss : 1.088461\n",
      "running loss : 1.101721\n",
      "running loss : 1.081747\n",
      "running loss : 1.072314\n",
      "running loss : 1.064434\n",
      "running loss : 1.091253\n",
      "running loss : 1.070655\n",
      "running loss : 1.082672\n",
      "running loss : 1.098403\n",
      "running loss : 1.085239\n",
      "running loss : 1.088265\n",
      "running loss : 1.079138\n",
      "running loss : 1.070689\n",
      "running loss : 1.080540\n",
      "running loss : 1.082242\n",
      "running loss : 1.081851\n",
      "running loss : 1.073828\n",
      "running loss : 1.075093\n",
      "running loss : 1.073446\n",
      "running loss : 1.085953\n",
      "running loss : 1.083102\n",
      "running loss : 1.089875\n",
      "running loss : 1.065212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.073114\n",
      "running loss : 1.084135\n",
      "running loss : 1.092479\n",
      "running loss : 1.090614\n",
      "running loss : 1.066759\n",
      "running loss : 1.067190\n",
      "running loss : 1.068821\n",
      "running loss : 1.075010\n",
      "running loss : 1.074053\n",
      "running loss : 1.072592\n",
      "running loss : 1.080607\n",
      "running loss : 1.090550\n",
      "running loss : 1.071325\n",
      "running loss : 1.077995\n",
      "running loss : 1.077708\n",
      "running loss : 1.101391\n",
      "running loss : 1.093335\n",
      "running loss : 1.092428\n",
      "running loss : 1.075692\n",
      "running loss : 1.080393\n",
      "running loss : 1.080865\n",
      "running loss : 1.086615\n",
      "running loss : 1.088164\n",
      "running loss : 1.078844\n",
      "running loss : 1.090448\n",
      "running loss : 1.072774\n",
      "running loss : 1.103170\n",
      "running loss : 1.083602\n",
      "running loss : 1.068521\n",
      "running loss : 1.103627\n",
      "running loss : 1.077872\n",
      "running loss : 1.078883\n",
      "running loss : 1.064360\n",
      "running loss : 1.081866\n",
      "running loss : 1.077420\n",
      "running loss : 1.084126\n",
      "running loss : 1.079885\n",
      "running loss : 1.075286\n",
      "running loss : 1.077408\n",
      "running loss : 1.096494\n",
      "running loss : 1.076990\n",
      "running loss : 1.080555\n",
      "running loss : 1.081815\n",
      "running loss : 1.069339\n",
      "running loss : 1.080852\n",
      "running loss : 1.077966\n",
      "running loss : 1.071317\n",
      "running loss : 1.076447\n",
      "running loss : 1.082334\n",
      "running loss : 1.081185\n",
      "running loss : 1.067260\n",
      "running loss : 1.077145\n",
      "running loss : 1.063124\n",
      "running loss : 1.078889\n",
      "running loss : 1.078245\n",
      "running loss : 1.065367\n",
      "running loss : 1.070491\n",
      "running loss : 1.085587\n",
      "running loss : 1.072685\n",
      "running loss : 1.062522\n",
      "running loss : 1.073517\n",
      "running loss : 1.070621\n",
      "running loss : 1.070847\n",
      "running loss : 1.071470\n",
      "running loss : 1.071020\n",
      "running loss : 1.085157\n",
      "running loss : 1.069412\n",
      "running loss : 1.081941\n",
      "running loss : 1.079486\n",
      "running loss : 1.088636\n",
      "running loss : 1.063047\n",
      "running loss : 1.064900\n",
      "running loss : 1.066312\n",
      "running loss : 1.072515\n",
      "running loss : 1.074243\n",
      "running loss : 1.078251\n",
      "running loss : 1.085188\n",
      "running loss : 1.065488\n",
      "running loss : 1.062250\n",
      "running loss : 1.084570\n",
      "running loss : 1.070073\n",
      "running loss : 1.067010\n",
      "running loss : 1.083915\n",
      "running loss : 1.059918\n",
      "running loss : 1.073563\n",
      "running loss : 1.092616\n",
      "running loss : 1.070692\n",
      "running loss : 1.076943\n",
      "running loss : 1.070618\n",
      "running loss : 1.054493\n",
      "running loss : 1.065400\n",
      "running loss : 1.067327\n",
      "running loss : 1.061442\n",
      "running loss : 1.075181\n",
      "running loss : 1.062059\n",
      "running loss : 1.057173\n",
      "running loss : 1.066851\n",
      "running loss : 1.065786\n",
      "running loss : 1.077240\n",
      "running loss : 1.085614\n",
      "running loss : 1.074087\n",
      "running loss : 1.061990\n",
      "running loss : 1.062760\n",
      "running loss : 1.056917\n",
      "running loss : 1.052295\n",
      "running loss : 1.056324\n",
      "running loss : 1.053669\n",
      "running loss : 1.059744\n",
      "running loss : 1.066812\n",
      "running loss : 1.053092\n",
      "running loss : 1.065128\n",
      "running loss : 1.085783\n",
      "running loss : 1.050521\n",
      "running loss : 1.048533\n",
      "running loss : 1.049625\n",
      "running loss : 1.061598\n",
      "running loss : 1.071638\n",
      "running loss : 1.057365\n",
      "running loss : 1.050012\n",
      "running loss : 1.054749\n",
      "running loss : 1.043242\n",
      "running loss : 1.054381\n",
      "running loss : 1.046321\n",
      "running loss : 1.064780\n",
      "running loss : 1.052476\n",
      "running loss : 1.046911\n",
      "running loss : 1.058090\n",
      "running loss : 1.054872\n",
      "running loss : 1.056415\n",
      "running loss : 1.072187\n",
      "running loss : 1.048768\n",
      "running loss : 1.055710\n",
      "running loss : 1.041348\n",
      "running loss : 1.048842\n",
      "running loss : 1.052581\n",
      "running loss : 1.063911\n",
      "running loss : 1.054748\n",
      "running loss : 1.064762\n",
      "running loss : 1.038333\n",
      "running loss : 1.059646\n",
      "running loss : 1.038937\n",
      "running loss : 1.047814\n",
      "running loss : 1.051940\n",
      "running loss : 1.042212\n",
      "running loss : 1.051216\n",
      "running loss : 1.044062\n",
      "running loss : 1.041076\n",
      "running loss : 1.057590\n",
      "running loss : 1.047648\n",
      "running loss : 1.044956\n",
      "running loss : 1.040229\n",
      "running loss : 1.059835\n",
      "running loss : 1.049731\n",
      "running loss : 1.062181\n",
      "running loss : 1.041376\n",
      "running loss : 1.051503\n",
      "running loss : 1.037511\n",
      "running loss : 1.047167\n",
      "running loss : 1.037597\n",
      "running loss : 1.054524\n",
      "running loss : 1.038117\n",
      "running loss : 1.041785\n",
      "running loss : 1.071087\n",
      "running loss : 1.084307\n",
      "running loss : 1.044390\n",
      "running loss : 1.044601\n",
      "running loss : 1.052838\n",
      "running loss : 1.054637\n",
      "running loss : 1.053522\n",
      "running loss : 1.057066\n",
      "running loss : 1.032288\n",
      "running loss : 1.060545\n",
      "running loss : 1.052712\n",
      "running loss : 1.053452\n",
      "running loss : 1.041991\n",
      "running loss : 1.031121\n",
      "running loss : 1.032055\n",
      "running loss : 1.043431\n",
      "running loss : 1.047432\n",
      "running loss : 1.033794\n",
      "running loss : 1.050584\n",
      "running loss : 1.040318\n",
      "running loss : 1.036202\n",
      "running loss : 1.057686\n",
      "running loss : 1.039853\n",
      "running loss : 1.041775\n",
      "running loss : 1.041835\n",
      "running loss : 1.047732\n",
      "running loss : 1.040225\n",
      "running loss : 1.041875\n",
      "running loss : 1.030927\n",
      "running loss : 1.036322\n",
      "running loss : 1.050460\n",
      "running loss : 1.047314\n",
      "running loss : 1.044606\n",
      "running loss : 1.052917\n",
      "running loss : 1.041709\n",
      "running loss : 1.037985\n",
      "running loss : 1.040729\n",
      "running loss : 1.039309\n",
      "running loss : 1.051972\n",
      "running loss : 1.033494\n",
      "running loss : 1.045805\n",
      "running loss : 1.050026\n",
      "running loss : 1.039038\n",
      "running loss : 1.040038\n",
      "running loss : 1.046759\n",
      "running loss : 1.046701\n",
      "running loss : 1.028678\n",
      "running loss : 1.050323\n",
      "running loss : 1.036504\n",
      "running loss : 1.041222\n",
      "running loss : 1.044621\n",
      "running loss : 1.038548\n",
      "running loss : 1.036407\n",
      "running loss : 1.042690\n",
      "running loss : 1.022375\n",
      "running loss : 1.045148\n",
      "running loss : 1.036349\n",
      "running loss : 1.037032\n",
      "running loss : 1.041324\n",
      "running loss : 1.038855\n",
      "running loss : 1.056459\n",
      "running loss : 1.027873\n",
      "running loss : 1.045282\n",
      "running loss : 1.052953\n",
      "running loss : 1.019427\n",
      "running loss : 1.030643\n",
      "running loss : 1.030420\n",
      "running loss : 1.049304\n",
      "running loss : 1.054392\n",
      "running loss : 1.031311\n",
      "running loss : 1.034528\n",
      "running loss : 1.036009\n",
      "running loss : 1.035399\n",
      "running loss : 1.038156\n",
      "running loss : 1.035680\n",
      "running loss : 1.040807\n",
      "running loss : 1.045982\n",
      "running loss : 1.047924\n",
      "running loss : 1.044685\n",
      "running loss : 1.039827\n",
      "running loss : 1.059132\n",
      "running loss : 1.033543\n",
      "running loss : 1.029364\n",
      "running loss : 1.040238\n",
      "running loss : 1.021800\n",
      "running loss : 1.038215\n",
      "running loss : 1.047801\n",
      "running loss : 1.031477\n",
      "running loss : 1.027203\n",
      "running loss : 1.038289\n",
      "running loss : 1.021771\n",
      "running loss : 1.037785\n",
      "running loss : 1.051498\n",
      "running loss : 1.031805\n",
      "running loss : 1.031029\n",
      "running loss : 1.023591\n",
      "running loss : 1.049650\n",
      "running loss : 1.029834\n",
      "running loss : 1.029033\n",
      "running loss : 1.033134\n",
      "running loss : 1.042773\n",
      "running loss : 1.044792\n",
      "running loss : 1.060776\n",
      "running loss : 1.036927\n",
      "running loss : 1.030638\n",
      "running loss : 1.023954\n",
      "running loss : 1.035370\n",
      "running loss : 1.028129\n",
      "running loss : 1.030747\n",
      "running loss : 1.027282\n",
      "running loss : 1.024452\n",
      "running loss : 1.064792\n",
      "running loss : 1.023212\n",
      "running loss : 1.035397\n",
      "running loss : 1.037395\n",
      "running loss : 1.042354\n",
      "running loss : 1.035160\n",
      "running loss : 1.030827\n",
      "running loss : 1.039200\n",
      "running loss : 1.031422\n",
      "running loss : 1.032194\n",
      "running loss : 1.037745\n",
      "running loss : 1.027008\n",
      "running loss : 1.045726\n",
      "running loss : 1.018440\n",
      "running loss : 1.032727\n",
      "running loss : 1.031846\n",
      "running loss : 1.042682\n",
      "running loss : 1.014774\n",
      "running loss : 1.042063\n",
      "running loss : 1.023838\n",
      "running loss : 1.032321\n",
      "running loss : 1.028573\n",
      "running loss : 1.038677\n",
      "running loss : 1.032967\n",
      "running loss : 1.030504\n",
      "running loss : 1.035896\n",
      "running loss : 1.033411\n",
      "running loss : 1.011653\n",
      "running loss : 1.039525\n",
      "running loss : 1.024687\n",
      "running loss : 1.019102\n",
      "running loss : 1.034460\n",
      "running loss : 1.035647\n",
      "running loss : 1.045137\n",
      "running loss : 1.065401\n",
      "running loss : 1.041548\n",
      "running loss : 1.015618\n",
      "running loss : 1.032947\n",
      "running loss : 1.021258\n",
      "running loss : 1.023049\n",
      "running loss : 1.037539\n",
      "running loss : 1.018058\n",
      "running loss : 1.034608\n",
      "running loss : 1.020761\n",
      "running loss : 1.034639\n",
      "running loss : 1.012475\n",
      "running loss : 1.029024\n",
      "running loss : 1.031409\n",
      "running loss : 1.046696\n",
      "running loss : 1.022795\n",
      "running loss : 1.003678\n",
      "running loss : 1.036678\n",
      "running loss : 1.027901\n",
      "running loss : 1.027151\n",
      "running loss : 1.023329\n",
      "running loss : 1.021863\n",
      "running loss : 1.014430\n",
      "running loss : 1.023522\n",
      "running loss : 0.996138\n",
      "running loss : 1.021446\n",
      "running loss : 1.021070\n",
      "running loss : 1.014515\n",
      "running loss : 1.031327\n",
      "running loss : 1.021996\n",
      "running loss : 1.023469\n",
      "running loss : 1.026234\n",
      "running loss : 1.011936\n",
      "running loss : 1.022245\n",
      "running loss : 1.023984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 1.023683\n",
      "running loss : 1.035339\n",
      "running loss : 1.005809\n",
      "running loss : 1.020410\n",
      "running loss : 1.006128\n",
      "running loss : 1.023772\n",
      "running loss : 1.015591\n",
      "running loss : 1.031318\n",
      "running loss : 1.013105\n",
      "running loss : 1.010593\n",
      "running loss : 1.047530\n",
      "running loss : 1.024879\n",
      "running loss : 1.011005\n",
      "running loss : 1.017423\n",
      "running loss : 1.017469\n",
      "running loss : 1.041072\n",
      "running loss : 1.015360\n",
      "running loss : 1.022459\n",
      "running loss : 1.023937\n",
      "running loss : 1.025137\n",
      "running loss : 1.020946\n",
      "running loss : 1.032262\n",
      "running loss : 1.031326\n",
      "running loss : 1.011047\n",
      "running loss : 1.014181\n",
      "running loss : 1.045698\n",
      "running loss : 1.024480\n",
      "running loss : 1.016237\n",
      "running loss : 1.022115\n",
      "running loss : 1.016315\n",
      "running loss : 1.020775\n",
      "running loss : 1.022991\n",
      "running loss : 1.014575\n",
      "running loss : 1.020491\n",
      "running loss : 1.005698\n",
      "running loss : 1.029355\n",
      "running loss : 1.013193\n",
      "running loss : 1.023920\n",
      "running loss : 1.015625\n",
      "running loss : 1.027147\n",
      "running loss : 1.021129\n",
      "running loss : 1.019480\n",
      "running loss : 1.004222\n",
      "running loss : 1.020528\n",
      "running loss : 1.015138\n",
      "running loss : 1.033058\n",
      "running loss : 1.017061\n",
      "running loss : 1.016736\n",
      "running loss : 1.020317\n",
      "running loss : 1.038506\n",
      "running loss : 1.010567\n",
      "running loss : 1.012956\n",
      "running loss : 1.008321\n",
      "running loss : 1.012784\n",
      "running loss : 1.014827\n",
      "running loss : 0.999616\n",
      "running loss : 1.013580\n",
      "running loss : 1.005331\n",
      "running loss : 1.016419\n",
      "running loss : 1.004147\n",
      "running loss : 1.007789\n",
      "running loss : 1.010734\n",
      "running loss : 1.005295\n",
      "running loss : 1.006150\n",
      "running loss : 1.005765\n",
      "running loss : 0.997980\n",
      "running loss : 1.020091\n",
      "running loss : 1.003137\n",
      "running loss : 1.040372\n",
      "running loss : 1.017721\n",
      "running loss : 1.018594\n",
      "running loss : 1.010378\n",
      "running loss : 1.011534\n",
      "running loss : 1.019474\n",
      "running loss : 1.016087\n",
      "running loss : 1.015322\n",
      "running loss : 1.021728\n",
      "running loss : 1.017601\n",
      "running loss : 1.009224\n",
      "running loss : 1.000059\n",
      "running loss : 0.993983\n",
      "running loss : 1.016869\n",
      "running loss : 1.030973\n",
      "running loss : 1.002923\n",
      "running loss : 1.007285\n",
      "running loss : 1.019590\n",
      "running loss : 1.016220\n",
      "running loss : 1.003663\n",
      "running loss : 1.018683\n",
      "running loss : 1.015960\n",
      "running loss : 1.031650\n",
      "running loss : 1.022521\n",
      "running loss : 1.006890\n",
      "running loss : 1.012492\n",
      "running loss : 1.021492\n",
      "running loss : 1.006610\n",
      "running loss : 1.015282\n",
      "running loss : 1.005916\n",
      "running loss : 1.011148\n",
      "running loss : 1.030072\n",
      "running loss : 0.992018\n",
      "running loss : 0.998001\n",
      "running loss : 1.015902\n",
      "running loss : 1.006526\n",
      "running loss : 1.008652\n",
      "running loss : 1.010704\n",
      "running loss : 1.018855\n",
      "running loss : 1.031879\n",
      "running loss : 1.005641\n",
      "running loss : 1.015410\n",
      "running loss : 1.013058\n",
      "running loss : 1.019499\n",
      "running loss : 1.020603\n",
      "running loss : 1.007547\n",
      "running loss : 1.022852\n",
      "running loss : 1.021996\n",
      "running loss : 1.000449\n",
      "running loss : 1.009580\n",
      "running loss : 1.014620\n",
      "running loss : 1.017335\n",
      "running loss : 1.005335\n",
      "running loss : 1.011104\n",
      "running loss : 1.015086\n",
      "running loss : 1.023387\n",
      "running loss : 1.022781\n",
      "running loss : 1.007245\n",
      "running loss : 1.024989\n",
      "running loss : 1.014622\n",
      "running loss : 1.012586\n",
      "running loss : 1.021207\n",
      "running loss : 1.003921\n",
      "running loss : 1.013971\n",
      "running loss : 1.019482\n",
      "running loss : 1.011039\n",
      "running loss : 1.009898\n",
      "running loss : 1.001741\n",
      "running loss : 1.013559\n",
      "running loss : 1.001329\n",
      "running loss : 1.018402\n",
      "running loss : 1.018450\n",
      "running loss : 0.997395\n",
      "running loss : 1.007491\n",
      "running loss : 0.998630\n",
      "running loss : 1.009267\n",
      "running loss : 1.002145\n",
      "running loss : 0.998480\n",
      "running loss : 1.009871\n",
      "running loss : 1.007598\n",
      "running loss : 0.999733\n",
      "running loss : 1.019619\n",
      "running loss : 1.014538\n",
      "running loss : 1.004975\n",
      "running loss : 1.020051\n",
      "running loss : 1.000655\n",
      "running loss : 1.016601\n",
      "running loss : 0.988481\n",
      "running loss : 1.000874\n",
      "running loss : 1.005365\n",
      "running loss : 1.020540\n",
      "running loss : 0.992618\n",
      "running loss : 1.002440\n",
      "running loss : 1.013280\n",
      "running loss : 1.032759\n",
      "running loss : 1.013712\n",
      "running loss : 0.999390\n",
      "running loss : 1.001335\n",
      "running loss : 0.993940\n",
      "running loss : 1.000530\n",
      "running loss : 1.001354\n",
      "running loss : 0.993640\n",
      "running loss : 1.017513\n",
      "running loss : 1.003981\n",
      "running loss : 0.995840\n",
      "running loss : 1.006694\n",
      "running loss : 1.011391\n",
      "running loss : 0.986370\n",
      "running loss : 0.997463\n",
      "running loss : 1.008393\n",
      "running loss : 1.010789\n",
      "running loss : 1.000543\n",
      "running loss : 1.005550\n",
      "running loss : 1.000842\n",
      "running loss : 0.999715\n",
      "running loss : 1.005268\n",
      "running loss : 0.995835\n",
      "running loss : 0.994197\n",
      "running loss : 0.993711\n",
      "running loss : 0.997954\n",
      "running loss : 1.001144\n",
      "running loss : 1.012039\n",
      "running loss : 0.996815\n",
      "running loss : 0.991967\n",
      "running loss : 0.997330\n",
      "running loss : 1.004552\n",
      "running loss : 1.007872\n",
      "running loss : 1.008957\n",
      "running loss : 1.015803\n",
      "running loss : 1.008391\n",
      "running loss : 1.004750\n",
      "running loss : 1.005993\n",
      "running loss : 1.008690\n",
      "running loss : 0.994864\n",
      "running loss : 0.994565\n",
      "running loss : 1.002923\n",
      "running loss : 1.010966\n",
      "running loss : 0.998459\n",
      "running loss : 0.994447\n",
      "running loss : 0.996978\n",
      "running loss : 0.999208\n",
      "running loss : 1.001183\n",
      "running loss : 0.996763\n",
      "running loss : 1.001834\n",
      "running loss : 0.996018\n",
      "running loss : 1.005618\n",
      "running loss : 0.995119\n",
      "running loss : 0.998653\n",
      "running loss : 0.999553\n",
      "running loss : 1.019272\n",
      "running loss : 0.994110\n",
      "running loss : 1.007623\n",
      "running loss : 0.994620\n",
      "running loss : 1.002876\n",
      "running loss : 1.007425\n",
      "running loss : 0.996576\n",
      "running loss : 0.992419\n",
      "running loss : 1.015627\n",
      "running loss : 1.001884\n",
      "running loss : 1.011028\n",
      "running loss : 0.994796\n",
      "running loss : 1.008993\n",
      "running loss : 1.002201\n",
      "running loss : 1.009090\n",
      "running loss : 0.986783\n",
      "running loss : 0.991338\n",
      "running loss : 0.993835\n",
      "running loss : 0.997809\n",
      "running loss : 1.006535\n",
      "running loss : 0.986212\n",
      "running loss : 1.028161\n",
      "running loss : 0.995971\n",
      "running loss : 0.981607\n",
      "running loss : 1.008698\n",
      "running loss : 1.030885\n",
      "running loss : 1.007941\n",
      "running loss : 0.999936\n",
      "running loss : 0.998549\n",
      "running loss : 0.995478\n",
      "running loss : 1.002031\n",
      "running loss : 1.002561\n",
      "running loss : 0.989910\n",
      "running loss : 1.013280\n",
      "running loss : 0.996777\n",
      "running loss : 1.005700\n",
      "running loss : 0.987937\n",
      "running loss : 0.995889\n",
      "running loss : 1.004225\n",
      "running loss : 0.987748\n",
      "running loss : 0.995204\n",
      "running loss : 1.006576\n",
      "running loss : 1.005387\n",
      "running loss : 1.012193\n",
      "running loss : 1.014482\n",
      "running loss : 1.004365\n",
      "running loss : 0.988778\n",
      "running loss : 0.992364\n",
      "running loss : 0.994551\n",
      "running loss : 0.984240\n",
      "running loss : 1.018988\n",
      "running loss : 0.992019\n",
      "running loss : 1.003668\n",
      "running loss : 1.002482\n",
      "running loss : 0.989833\n",
      "running loss : 1.005710\n",
      "running loss : 0.992725\n",
      "running loss : 0.996990\n",
      "running loss : 0.994317\n",
      "running loss : 1.004916\n",
      "running loss : 0.986999\n",
      "running loss : 0.997994\n",
      "running loss : 1.017720\n",
      "running loss : 1.011091\n",
      "running loss : 0.988607\n",
      "running loss : 0.991098\n",
      "running loss : 0.997606\n",
      "running loss : 0.994126\n"
     ]
    }
   ],
   "source": [
    "# transfert du model au gpu\n",
    "model.to(device)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "        \n",
    "# incremental update of coefficients        \n",
    "\n",
    "# define beta\n",
    "beta = 0.00001\n",
    "#beta = 1000\n",
    "# define threshold and loss_init\n",
    "threshold = 0.95\n",
    "loss_init = float(\"Inf\")\n",
    "nb_ones = 1\n",
    "iteration = 0\n",
    "mask=(compute_mask(1, (96, 16, 16)).unsqueeze(0)).cuda()\n",
    "dim_latent = 16*16*96\n",
    "output_flag = False\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 6150\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \"\"\"\n",
    "    if epoch==100:\n",
    "        # define a new learning rate and so a new optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "    \"\"\"\n",
    "        \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized] = model(batch_images, mask, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        #loss_dist = distortion_pc(batch_images, decoded_images, 525.0, 0, 1000.0, (128, 128), (319.5, 239.5))\n",
    "        loss_bit = mean_bit_per_px(x_quantized, model.phi, model.var)\n",
    "        #print(\" loss distortion : \", loss_dist)\n",
    "        #print(\"loss bit : \", loss_bit)\n",
    "        #loss = loss_dist + beta*loss_bit\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(loss)\n",
    "        \n",
    "        # check the value of the loss to see if another coefficient can be enabled\n",
    "        if (loss.item() < loss_init*threshold or iteration > 5):\n",
    "            if (nb_ones<dim_latent):\n",
    "                nb_ones +=1\n",
    "                loss_init = loss.item()\n",
    "                iteration = 0\n",
    "                mask = (compute_mask(nb_ones, tuple(x_quantized.size()[1:])).unsqueeze(0)).cuda()\n",
    "            else:\n",
    "                output_flag = True\n",
    "                break\n",
    "            \n",
    "        loss.backward()\n",
    "        #print(\"input weights conv 1 gradient: \", model.conv1.weight.grad)\n",
    "        #print(\"input bias conv 1 gradient: \",  model.conv1.bias.grad)\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        iteration += 1\n",
    "\n",
    "    if output_flag:\n",
    "        break\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "torch.save(model.state_dict(), './model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001_without_black_px.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_bis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-94555e69bfd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# transfert du model au gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel_bis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# general update of coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_bis' is not defined"
     ]
    }
   ],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "    \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "# transfert du model au gpu\n",
    "model_bis.to(device)\n",
    "\n",
    "# general update of coefficients    \n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(model_bis.parameters(), lr=0.00001)\n",
    "    # define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "    #Epochs\n",
    "n_epochs = 420\n",
    "beta = 0.00001\n",
    "nb_updates = 0\n",
    "learning_rate = 0.0001\n",
    "tau = 10000\n",
    "kappa = 0.8\n",
    "\n",
    "    # Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "          \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        learning_rate *= tau**kappa/(tau + nb_updates)**kappa\n",
    "        print(\"learning_ rate : \", learning_rate)\n",
    "        optimizer = torch.optim.Adam(model_bis.parameters(), lr=learning_rate)\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized] = model_bis(batch_images, 1, True, True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dist = distortion(decoded_images, batch_images)\n",
    "        loss_bit = mean_bit_per_px(x_quantized, model_bis.phi, model_bis.var)\n",
    "        loss = beta * loss_dist + loss_bit\n",
    "        #print(loss)\n",
    "            \n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        nb_updates += 1\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LossyCompAutoencoder_bis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c27b05dcba91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# The new network is trained each time we want to change the rate-distortion tradeoff beta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_bis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLossyCompAutoencoder_bis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel_bis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel_bis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LossyCompAutoencoder_bis' is not defined"
     ]
    }
   ],
   "source": [
    "# variable bit rate\n",
    "    # get back network parameters obtained by first training (eg for a fixed value of beta and no lambda)\n",
    "    # these parameters are used to initialize the new network and won't be changed after. \n",
    "    # The only parameter that is optimized in the new network is lambda \n",
    "    # The new network is trained each time we want to change the rate-distortion tradeoff beta\n",
    "\n",
    "model_bis = LossyCompAutoencoder_bis()\n",
    "model_bis.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model_bis.train()\n",
    "model_bis.to(device)\n",
    "\n",
    "\n",
    "weights_model_bis = list(model_bis.parameters())\n",
    "print(\"nb parameters of the model : \", len(weights_model_bis))\n",
    "for (name_bis, parameter_bis), (name, parameter) in zip(model_bis.named_parameters(), model.named_parameters()) :\n",
    "    \"\"\"\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter_bis)\n",
    "        parameter_bis.data = torch.FloatTensor(1, 96, 1, 1).uniform_(0.9, 1.1)\n",
    "        print(\"lamb new parameter : \", parameter_bis)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name_bis)\n",
    "    print(\"size of user-defined parameters : \", parameter_bis.size())\n",
    "    parameter.data = parameter_bis.data\n",
    "\n",
    "for name_bis, parameter_bis in model_bis.named_parameters():\n",
    "    if name_bis == \"lamb\":\n",
    "        print(\"check lamb parameter : \", parameter_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9d7010c95117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweights_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m39\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mweights_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights_model_bis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"size of user-defined parameters : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_model_bis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights model: \"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mweights_model_bis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "weights_model = list(model.parameters())\n",
    "for i in range(0, 39, 1):\n",
    "    weights_model[i] = weights_model_bis[i]\n",
    "    print(\"size of user-defined parameters : \", weights_model_bis[i].size())\n",
    "    print(\"weights model: \" ,weights_model_bis[i])\n",
    "model.train()    \n",
    "#weights_model[39] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "#weights_model[40] = torch.FloatTensor(96).uniform_(0, 1)\n",
    "\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    \"\"\"\n",
    "    if name == \"lamb\":\n",
    "        print(\"lamb parameter : \", parameter)\n",
    "        parameter.data = torch.FloatTensor(1, 96, 1, 1).uniform_(1, 1)\n",
    "        print(\"lamb new parameter : \", parameter)\n",
    "    \"\"\"\n",
    "    print(\"name of user-defined parameters : \", name)\n",
    "    print(\"size of user-defined parameters : \", parameter.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(41., device='cuda:0')\n",
      "min vec latent :  tensor(-20., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-15., device='cuda:0')\n",
      "max vec latent :  tensor(41., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(45., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(44., device='cuda:0')\n",
      "min vec latent :  tensor(-17., device='cuda:0')\n",
      "max vec latent :  tensor(44., device='cuda:0')\n",
      "min vec latent :  tensor(-15., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-20., device='cuda:0')\n",
      "max vec latent :  tensor(40., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-18., device='cuda:0')\n",
      "max vec latent :  tensor(44., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(40., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-15., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-16., device='cuda:0')\n",
      "max vec latent :  tensor(43., device='cuda:0')\n",
      "min vec latent :  tensor(-17., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-17., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n",
      "min vec latent :  tensor(-20., device='cuda:0')\n",
      "max vec latent :  tensor(45., device='cuda:0')\n",
      "min vec latent :  tensor(-22., device='cuda:0')\n",
      "max vec latent :  tensor(45., device='cuda:0')\n",
      "min vec latent :  tensor(-19., device='cuda:0')\n",
      "max vec latent :  tensor(42., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 48 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test set \n",
    "\"\"\"\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta000001.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "#test_dataset = ImageDataset(root_dir='./data/kodac/', transform=transforms.Compose([RandomCrop(128), ToTensor()]))\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        if test_image.size(2)>test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "        \n",
    "        [reconstructed_image, vec_latent] = model(test_image, 1, True)\n",
    "        print(\"min vec latent : \", torch.min(vec_latent))\n",
    "        print(\"max vec latent : \", torch.max(vec_latent))\n",
    "        \n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(torch.squeeze(reconstructed_image.int().cpu()))\n",
    "        \n",
    "        \"\"\"\n",
    "        # We can set the number of bins with the `bins` kwarg\n",
    "        bins_list = [-6.5, -5.5, -4.5, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n",
    "        for k in range(96):\n",
    "            # plot histograms\n",
    "            fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "            ax.hist((vec_latent[:, k, :, :]).view(-1).cpu(), bins=bins_list)\n",
    "            plt.savefig(\"D:\\\\autoencoder_data\\\\histograms\\\\depthmap\\\\\" + \"img\" + str(i)+ \"hist\" + str(k) + \".png\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = model(test_image)\n",
    "        reconstructed_depthmap = np.squeeze(reconstructed_image.cpu()).numpy()\n",
    "        cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\reconstructed\\\\beta_1\\\\\" + \"img\" + str(i)+\".png\", reconstructed_depthmap.astype(np.uint16))\n",
    "        #save_image(reconstructed_depthmap, \"D:\\\\autoencoder_data\\\\depthmaps\\\\reconstructed\\\\beta_0001\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy :  tensor(2.709279, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.063918, device='cuda:0')\n",
      "psnr :  tensor(60.218197)\n",
      "entropy :  tensor(2.813324, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.219986, device='cuda:0')\n",
      "psnr :  tensor(55.619770)\n",
      "entropy :  tensor(2.705266, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.057899, device='cuda:0')\n",
      "psnr :  tensor(62.501282)\n",
      "entropy :  tensor(2.712790, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.069184, device='cuda:0')\n",
      "psnr :  tensor(57.227882)\n",
      "entropy :  tensor(2.755549, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.133324, device='cuda:0')\n",
      "psnr :  tensor(55.564781)\n",
      "entropy :  tensor(2.704483, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.056725, device='cuda:0')\n",
      "psnr :  tensor(62.117290)\n",
      "entropy :  tensor(2.801929, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.202894, device='cuda:0')\n",
      "psnr :  tensor(54.938484)\n",
      "entropy :  tensor(2.827868, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.241803, device='cuda:0')\n",
      "psnr :  tensor(53.801464)\n",
      "entropy :  tensor(2.731619, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.097428, device='cuda:0')\n",
      "psnr :  tensor(61.901928)\n",
      "entropy :  tensor(2.747969, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.121954, device='cuda:0')\n",
      "psnr :  tensor(59.501442)\n",
      "entropy :  tensor(2.756656, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.134984, device='cuda:0')\n",
      "psnr :  tensor(58.213383)\n",
      "entropy :  tensor(2.792858, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.189287, device='cuda:0')\n",
      "psnr :  tensor(55.602898)\n",
      "entropy :  tensor(2.668185, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.002278, device='cuda:0')\n",
      "psnr :  tensor(61.186325)\n",
      "entropy :  tensor(2.728021, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.092031, device='cuda:0')\n",
      "psnr :  tensor(62.043625)\n",
      "entropy :  tensor(2.684215, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.026322, device='cuda:0')\n",
      "psnr :  tensor(62.661434)\n",
      "entropy :  tensor(2.736803, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.105205, device='cuda:0')\n",
      "psnr :  tensor(60.333271)\n",
      "entropy :  tensor(2.844352, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.266529, device='cuda:0')\n",
      "psnr :  tensor(58.247154)\n",
      "entropy :  tensor(2.919651, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.379477, device='cuda:0')\n",
      "psnr :  tensor(51.732670)\n",
      "entropy :  tensor(2.778053, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.167079, device='cuda:0')\n",
      "psnr :  tensor(49.837807)\n",
      "entropy :  tensor(2.875844, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.313766, device='cuda:0')\n",
      "psnr :  tensor(46.581654)\n",
      "entropy :  tensor(2.833498, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.250247, device='cuda:0')\n",
      "psnr :  tensor(49.168198)\n",
      "entropy :  tensor(2.940254, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.410381, device='cuda:0')\n",
      "psnr :  tensor(56.956802)\n",
      "entropy :  tensor(2.899744, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.349617, device='cuda:0')\n",
      "psnr :  tensor(57.619205)\n",
      "entropy :  tensor(2.882065, device='cuda:0')\n",
      "nb bits per pixel :  tensor(4.323098, device='cuda:0')\n",
      "psnr :  tensor(51.335922)\n",
      "mean nb bits per pixel :  tensor(4.178142, device='cuda:0')\n",
      "psnr mean :  tensor(56.871372)\n"
     ]
    }
   ],
   "source": [
    "# compute PSNR for each image of the test set and its reconstruction\n",
    "\n",
    "def write_data(filepath , tensor_data):\n",
    "    batch, channel, h, w = tensor_data.size()\n",
    "    matrix = tensor_data.cpu().numpy()\n",
    "    file = open(filepath, \"w\")\n",
    "    for image in range(batch):\n",
    "        np.savetxt(file, matrix[image, :, :, :].reshape(channel*h, w), fmt ='%.0f')\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "psnr_sum = 0.0\n",
    "bit_rate_sum = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model(test_image, 1, True)\n",
    "        #write_data('.\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2\\\\latent_vect\\\\' + 'vec' + str(i) +'.txt', im_quantized)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(2)*test_image.size(3))\n",
    "        psnr_sum+= psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0)\n",
    "        bit_rate_sum += nbpp\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 2**16-1.0))\n",
    "print( \"mean nb bits per pixel : \", bit_rate_sum/len(test_dataset))\n",
    "print(\"psnr mean : \", psnr_sum/len(test_dataset) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(tensor_data):\n",
    "    min_val = tensor_data.min()\n",
    "    max_val = tensor_data.max()\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    hist = torch.histc(tensor_data, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    hist_prob = hist/hist.sum()\n",
    "    hist_prob[hist_prob == 0] = 1\n",
    "    entropy = -(hist_prob*torch.log2(hist_prob)).sum()\n",
    "    return entropy\n",
    "       \n",
    "    \n",
    "    \n",
    "def psnr(original, compressed, max_pixel): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse)) \n",
    "    return psnr \n",
    "\n",
    "\n",
    "# Load previous model\n",
    "model_prev = LossyCompAutoencoder()\n",
    "model_prev.load_state_dict(torch.load('./model_parameters/lossy_comp_params_with_rate_beta2_incremental_2.pth'))\n",
    "model_prev.eval()\n",
    "model_prev.to(device)\n",
    "\n",
    "\n",
    "# And run test \n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, im_quantized] = model_prev(test_image,1, True)\n",
    "        nb_symbols = im_quantized.size(0)*im_quantized.size(1)*im_quantized.size(2)*im_quantized.size(3)\n",
    "        entropy = compute_entropy(im_quantized)\n",
    "        nbpp = nb_symbols*entropy/float(test_image.size(0)*test_image.size(1)*test_image.size(2)*test_image.size(3))\n",
    "        print(\"nb_symbols : \", nb_symbols)\n",
    "        print(\"entropy : \", entropy)\n",
    "        print( \"nb bits per pixel : \", nbpp)\n",
    "        print(\"psnr : \" , psnr(test_image.cpu(), reconstructed_image.cpu(), 255.0))\n",
    "    \n",
    "# And print figures\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        if test_image.size(2)<test_image.size(3):\n",
    "            test_image = test_image.permute(0, 1, 3, 2)\n",
    "        \n",
    "        reconstructed_image = model_prev(test_image, 1,  False)\n",
    "        ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.squeeze(reconstructed_image.int().cpu()).permute(1, 2, 0))\n",
    "        \n",
    "# And save figures\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        \n",
    "        reconstructed_image = np.squeeze(model_prev(test_image).cpu())\n",
    "        print(reconstructed_image.type())\n",
    "        save_image(reconstructed_image, \".\\\\reconstructed_data\\\\kodac\\\\loss_distortion_and_bitrate\\\\beta_2_incremental_bis\\\\\" + \"img\" + str(i)+\".png\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 90881.718424\n",
      "running loss : 80767.381022\n",
      "running loss : 81141.024251\n",
      "running loss : 82880.777669\n",
      "running loss : 82961.080566\n",
      "running loss : 80102.377279\n",
      "running loss : 78891.135254\n",
      "running loss : 81277.776530\n",
      "running loss : 85981.076497\n",
      "running loss : 79330.397786\n",
      "running loss : 81722.521647\n",
      "running loss : 81752.704427\n",
      "running loss : 80582.047445\n",
      "running loss : 79124.916667\n",
      "running loss : 77118.216146\n",
      "running loss : 80098.077311\n",
      "running loss : 80323.040690\n",
      "running loss : 77555.702474\n",
      "running loss : 81157.973796\n",
      "running loss : 83483.012695\n",
      "running loss : 81039.977051\n",
      "running loss : 80674.319661\n",
      "running loss : 78851.218424\n",
      "running loss : 79257.480469\n",
      "running loss : 81489.141927\n",
      "running loss : 78403.699056\n",
      "running loss : 77935.420654\n",
      "running loss : 75854.299316\n",
      "running loss : 77985.073893\n",
      "running loss : 79333.242025\n",
      "running loss : 78887.763672\n",
      "running loss : 76951.318522\n",
      "running loss : 74481.261230\n",
      "running loss : 76596.198730\n",
      "running loss : 80668.370605\n",
      "running loss : 78486.251790\n",
      "running loss : 79721.607747\n",
      "running loss : 81922.940918\n",
      "running loss : 74783.033040\n",
      "running loss : 76995.726074\n",
      "running loss : 75828.042806\n",
      "running loss : 75566.468831\n",
      "running loss : 77649.646891\n",
      "running loss : 75448.314860\n",
      "running loss : 80816.064128\n",
      "running loss : 78636.360352\n",
      "running loss : 79527.605794\n",
      "running loss : 79126.900879\n",
      "running loss : 78015.188965\n",
      "running loss : 74594.386637\n",
      "running loss : 77053.531820\n",
      "running loss : 75672.388346\n",
      "running loss : 76516.456706\n",
      "running loss : 74747.424805\n",
      "running loss : 78134.740234\n",
      "running loss : 76639.831543\n",
      "running loss : 78095.943848\n",
      "running loss : 76729.708659\n",
      "running loss : 75958.247721\n",
      "running loss : 76754.540853\n",
      "running loss : 74499.404785\n",
      "running loss : 76064.408447\n",
      "running loss : 74957.913411\n",
      "running loss : 74683.286133\n",
      "running loss : 76095.526449\n",
      "running loss : 74837.771810\n",
      "running loss : 78134.862467\n",
      "running loss : 73558.099772\n",
      "running loss : 73238.178223\n",
      "running loss : 73542.956380\n",
      "running loss : 71541.253743\n",
      "running loss : 76337.736003\n",
      "running loss : 77145.011230\n",
      "running loss : 82591.630697\n",
      "running loss : 74320.608724\n",
      "running loss : 75400.365723\n",
      "running loss : 76297.754720\n",
      "running loss : 73032.168457\n",
      "running loss : 72223.773275\n",
      "running loss : 77493.298340\n",
      "running loss : 74394.108236\n",
      "running loss : 73023.217041\n",
      "running loss : 76107.315267\n",
      "running loss : 74056.746582\n",
      "running loss : 74171.511719\n",
      "running loss : 74653.592773\n",
      "running loss : 73351.820312\n",
      "running loss : 78821.387044\n",
      "running loss : 75447.558757\n",
      "running loss : 72967.428385\n",
      "running loss : 72232.532227\n",
      "running loss : 71519.714762\n",
      "running loss : 72271.118571\n",
      "running loss : 75045.665039\n",
      "running loss : 71788.911540\n",
      "running loss : 71587.612956\n",
      "running loss : 75326.343424\n",
      "running loss : 72893.089844\n",
      "running loss : 72551.791504\n",
      "running loss : 72402.118164\n",
      "running loss : 72482.847819\n",
      "running loss : 70326.353027\n",
      "running loss : 72497.148438\n",
      "running loss : 71030.833252\n",
      "running loss : 72834.551188\n",
      "running loss : 69049.243978\n",
      "running loss : 72041.862793\n",
      "running loss : 73895.796224\n",
      "running loss : 71108.629639\n",
      "running loss : 70673.523600\n",
      "running loss : 70563.469645\n",
      "running loss : 71669.611491\n",
      "running loss : 71572.809570\n",
      "running loss : 73207.610026\n",
      "running loss : 70289.937988\n",
      "running loss : 73608.363932\n",
      "running loss : 69644.988770\n",
      "running loss : 70632.329346\n",
      "running loss : 72060.336344\n",
      "running loss : 74013.411377\n",
      "running loss : 72037.394206\n",
      "running loss : 70665.166341\n",
      "running loss : 68599.423014\n",
      "running loss : 69972.512858\n",
      "running loss : 71346.927246\n",
      "running loss : 71016.819010\n",
      "running loss : 66559.967692\n",
      "running loss : 71759.971680\n",
      "running loss : 68208.039225\n",
      "running loss : 68988.021159\n",
      "running loss : 72319.526855\n",
      "running loss : 66640.239339\n",
      "running loss : 71162.450195\n",
      "running loss : 73307.329427\n",
      "running loss : 72013.509603\n",
      "running loss : 67888.399577\n",
      "running loss : 67838.267253\n",
      "running loss : 67403.818359\n",
      "running loss : 66424.451579\n",
      "running loss : 68155.833171\n",
      "running loss : 66552.755371\n",
      "running loss : 69753.739746\n",
      "running loss : 68499.088460\n",
      "running loss : 67558.430339\n",
      "running loss : 68600.804281\n",
      "running loss : 69922.785156\n",
      "running loss : 66757.889730\n",
      "running loss : 68707.454753\n",
      "running loss : 68183.697591\n",
      "running loss : 65835.472493\n",
      "running loss : 67034.937663\n",
      "running loss : 68011.588867\n",
      "running loss : 66190.742350\n",
      "running loss : 72201.026693\n",
      "running loss : 67558.470052\n",
      "running loss : 67204.678711\n",
      "running loss : 66466.527100\n",
      "running loss : 67778.593913\n",
      "running loss : 66149.158854\n",
      "running loss : 66644.091553\n",
      "running loss : 66531.372070\n",
      "running loss : 68879.133464\n",
      "running loss : 65883.524577\n",
      "running loss : 65989.712565\n",
      "running loss : 64777.686930\n",
      "running loss : 66744.919027\n",
      "running loss : 64951.270345\n",
      "running loss : 69847.909180\n",
      "running loss : 68170.651530\n",
      "running loss : 63616.576986\n",
      "running loss : 65671.874756\n",
      "running loss : 64878.045410\n",
      "running loss : 65433.097087\n",
      "running loss : 65687.117350\n",
      "running loss : 64973.419434\n",
      "running loss : 65977.335938\n",
      "running loss : 64862.483317\n",
      "running loss : 65343.246257\n",
      "running loss : 67058.325602\n",
      "running loss : 67467.834310\n",
      "running loss : 64426.727458\n",
      "running loss : 68849.332194\n",
      "running loss : 66362.670898\n",
      "running loss : 63499.920329\n",
      "running loss : 66428.607096\n",
      "running loss : 67124.953776\n",
      "running loss : 65019.662923\n",
      "running loss : 65982.241211\n",
      "running loss : 63719.047689\n",
      "running loss : 64204.582194\n",
      "running loss : 63478.442708\n",
      "running loss : 64878.372762\n",
      "running loss : 65296.317139\n",
      "running loss : 63436.056803\n",
      "running loss : 63702.435954\n",
      "running loss : 64820.618083\n",
      "running loss : 65014.929769\n",
      "running loss : 64328.333496\n",
      "running loss : 64518.002686\n",
      "running loss : 64876.278076\n",
      "running loss : 64833.586426\n",
      "running loss : 64939.841471\n",
      "running loss : 64190.499349\n",
      "running loss : 64012.215088\n",
      "running loss : 61040.502197\n",
      "running loss : 62624.248454\n",
      "running loss : 62617.131836\n",
      "running loss : 62997.559082\n",
      "running loss : 62796.240641\n",
      "running loss : 65386.404297\n",
      "running loss : 68609.843750\n",
      "running loss : 63262.989665\n",
      "running loss : 62988.893962\n",
      "running loss : 63351.589111\n",
      "running loss : 63487.382894\n",
      "running loss : 66367.378581\n",
      "running loss : 63156.476888\n",
      "running loss : 62550.610352\n",
      "running loss : 65799.342611\n",
      "running loss : 63506.818115\n",
      "running loss : 62989.610189\n",
      "running loss : 61707.131022\n",
      "running loss : 62448.594889\n",
      "running loss : 62532.534831\n",
      "running loss : 62570.916667\n",
      "running loss : 63322.117188\n",
      "running loss : 63918.275391\n",
      "running loss : 66068.375326\n",
      "running loss : 62895.030762\n",
      "running loss : 62117.057861\n",
      "running loss : 62043.133626\n",
      "running loss : 61570.696777\n",
      "running loss : 64902.429688\n",
      "running loss : 63346.717041\n",
      "running loss : 62140.927083\n",
      "running loss : 61786.596354\n",
      "running loss : 63687.006266\n",
      "running loss : 60424.630615\n",
      "running loss : 61077.136312\n",
      "running loss : 60420.428792\n",
      "running loss : 61147.024740\n",
      "running loss : 61320.482666\n",
      "running loss : 61779.270508\n",
      "running loss : 62446.612386\n",
      "running loss : 60744.679688\n",
      "running loss : 64975.457031\n",
      "running loss : 60987.298096\n",
      "running loss : 62643.756917\n",
      "running loss : 61548.540039\n",
      "running loss : 59826.493164\n",
      "running loss : 61498.732747\n",
      "running loss : 60646.932699\n",
      "running loss : 59577.069255\n",
      "running loss : 60038.006673\n",
      "running loss : 62109.148600\n",
      "running loss : 59153.445231\n",
      "running loss : 58757.138184\n",
      "running loss : 61034.308431\n",
      "running loss : 59519.460612\n",
      "running loss : 59256.192546\n",
      "running loss : 58871.865234\n",
      "running loss : 61170.366292\n",
      "running loss : 61147.690348\n",
      "running loss : 59543.365397\n",
      "running loss : 60917.224854\n",
      "running loss : 60962.057292\n",
      "running loss : 59510.192546\n",
      "running loss : 59751.641846\n",
      "running loss : 60623.890299\n",
      "running loss : 57572.010905\n",
      "running loss : 59270.081380\n",
      "running loss : 57685.983358\n",
      "running loss : 58103.847575\n",
      "running loss : 59411.888184\n",
      "running loss : 60618.824870\n",
      "running loss : 57591.052327\n",
      "running loss : 60389.428467\n",
      "running loss : 59656.430827\n",
      "running loss : 58223.061849\n",
      "running loss : 56892.443359\n",
      "running loss : 59637.031576\n",
      "running loss : 56824.380127\n",
      "running loss : 57407.539795\n",
      "running loss : 59503.296224\n",
      "running loss : 59686.411865\n",
      "running loss : 56967.001302\n",
      "running loss : 57625.842855\n",
      "running loss : 58739.717041\n",
      "running loss : 59640.992676\n",
      "running loss : 56245.007812\n",
      "running loss : 56935.122396\n",
      "running loss : 56973.031494\n",
      "running loss : 57224.989176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 56330.414144\n",
      "running loss : 59217.237142\n",
      "running loss : 56974.153239\n",
      "running loss : 57221.444499\n",
      "running loss : 59008.677165\n",
      "running loss : 57119.617513\n",
      "running loss : 55305.609049\n",
      "running loss : 57673.525716\n",
      "running loss : 56229.853027\n",
      "running loss : 59675.997640\n",
      "running loss : 55790.359294\n",
      "running loss : 57322.470052\n",
      "running loss : 56106.875407\n",
      "running loss : 60872.270020\n",
      "running loss : 56010.167318\n",
      "running loss : 57382.687419\n",
      "running loss : 58415.753255\n",
      "running loss : 56892.956543\n",
      "running loss : 55117.039795\n",
      "running loss : 55409.036784\n",
      "running loss : 56686.735514\n",
      "running loss : 53838.631673\n",
      "running loss : 56824.481038\n",
      "running loss : 55640.784505\n",
      "running loss : 58160.728841\n",
      "running loss : 57626.534668\n",
      "running loss : 58323.094727\n",
      "running loss : 53920.407064\n",
      "running loss : 55168.505452\n",
      "running loss : 55127.270671\n",
      "running loss : 54881.633057\n",
      "running loss : 54211.694743\n",
      "running loss : 54501.730794\n",
      "running loss : 56947.436442\n",
      "running loss : 56152.468913\n",
      "running loss : 55613.786621\n",
      "running loss : 55751.203613\n",
      "running loss : 56320.617106\n",
      "running loss : 55469.636556\n",
      "running loss : 58944.215576\n",
      "running loss : 55151.181234\n",
      "running loss : 54784.307699\n",
      "running loss : 53933.224284\n",
      "running loss : 53722.103516\n",
      "running loss : 55022.672526\n",
      "running loss : 55112.404785\n",
      "running loss : 56338.940511\n",
      "running loss : 54212.332845\n",
      "running loss : 58285.784668\n",
      "running loss : 53848.245280\n",
      "running loss : 53618.175374\n",
      "running loss : 60219.200602\n",
      "running loss : 56293.832357\n",
      "running loss : 53725.534993\n",
      "running loss : 53165.360514\n",
      "running loss : 55134.238932\n",
      "running loss : 52998.208496\n",
      "running loss : 56099.515462\n",
      "running loss : 53734.485921\n",
      "running loss : 53228.031657\n",
      "running loss : 55388.830322\n",
      "running loss : 52846.800618\n",
      "running loss : 53111.263590\n",
      "running loss : 52534.631673\n",
      "running loss : 52801.039388\n",
      "running loss : 53641.130859\n",
      "running loss : 53582.857992\n",
      "running loss : 52399.858398\n",
      "running loss : 56638.229818\n",
      "running loss : 52979.531006\n",
      "running loss : 54271.182373\n",
      "running loss : 52913.174805\n",
      "running loss : 55493.278646\n",
      "running loss : 55342.254557\n",
      "running loss : 57968.763753\n",
      "running loss : 52760.720785\n",
      "running loss : 52599.817790\n",
      "running loss : 52585.163493\n",
      "running loss : 53373.625977\n",
      "running loss : 52442.100586\n",
      "running loss : 53891.315267\n",
      "running loss : 50547.944987\n",
      "running loss : 51661.942057\n",
      "running loss : 53970.030111\n",
      "running loss : 53986.946533\n",
      "running loss : 52550.642578\n",
      "running loss : 52293.479980\n",
      "running loss : 51929.335938\n",
      "running loss : 52587.779541\n",
      "running loss : 53295.073893\n",
      "running loss : 53681.895915\n",
      "running loss : 52964.613241\n",
      "running loss : 52228.323975\n",
      "running loss : 52873.979980\n",
      "running loss : 53320.415853\n",
      "running loss : 53423.901449\n",
      "running loss : 50707.938558\n",
      "running loss : 54250.009359\n",
      "running loss : 52043.060221\n",
      "running loss : 52570.794678\n",
      "running loss : 50841.515544\n",
      "running loss : 52653.602702\n",
      "running loss : 50633.415283\n",
      "running loss : 53026.918864\n",
      "running loss : 53763.368083\n",
      "running loss : 52898.638509\n",
      "running loss : 51744.891317\n",
      "running loss : 49307.681885\n",
      "running loss : 51092.994548\n",
      "running loss : 52226.794352\n",
      "running loss : 50100.759684\n",
      "running loss : 51270.523682\n",
      "running loss : 52272.166585\n",
      "running loss : 52876.253988\n",
      "running loss : 53089.156820\n",
      "running loss : 54536.442790\n",
      "running loss : 51735.277018\n",
      "running loss : 51833.324219\n",
      "running loss : 49773.806722\n",
      "running loss : 51504.332845\n",
      "running loss : 51773.265869\n",
      "running loss : 49764.508464\n",
      "running loss : 49225.610514\n",
      "running loss : 49575.354899\n",
      "running loss : 49936.283854\n",
      "running loss : 48218.027507\n",
      "running loss : 50748.913330\n",
      "running loss : 49324.360840\n",
      "running loss : 50701.173747\n",
      "running loss : 49849.854736\n",
      "running loss : 51438.284261\n",
      "running loss : 50187.976644\n",
      "running loss : 50680.334391\n",
      "running loss : 50702.315674\n",
      "running loss : 53446.914551\n",
      "running loss : 50960.229045\n",
      "running loss : 50184.661214\n",
      "running loss : 50461.203532\n",
      "running loss : 50077.770833\n",
      "running loss : 50244.267741\n",
      "running loss : 49254.449544\n",
      "running loss : 48433.246501\n",
      "running loss : 49656.638509\n",
      "running loss : 48824.016520\n",
      "running loss : 49076.353923\n",
      "running loss : 50104.597453\n",
      "running loss : 50265.500488\n",
      "running loss : 50032.427246\n",
      "running loss : 49624.999512\n",
      "running loss : 50925.185954\n",
      "running loss : 48534.235921\n",
      "running loss : 50662.632568\n",
      "running loss : 48193.469889\n",
      "running loss : 48474.606852\n",
      "running loss : 49389.584473\n",
      "running loss : 49591.266439\n",
      "running loss : 51075.706055\n",
      "running loss : 47004.641195\n",
      "running loss : 49832.155762\n",
      "running loss : 48841.529948\n",
      "running loss : 48364.793050\n",
      "running loss : 49087.891032\n",
      "running loss : 49919.277507\n",
      "running loss : 50339.088623\n",
      "running loss : 48497.556234\n",
      "running loss : 48887.618652\n",
      "running loss : 49891.339193\n",
      "running loss : 50234.233073\n",
      "running loss : 49169.051351\n",
      "running loss : 47015.986491\n",
      "running loss : 48219.990153\n",
      "running loss : 47937.049316\n",
      "running loss : 47478.959554\n",
      "running loss : 47636.843750\n",
      "running loss : 48866.298258\n",
      "running loss : 47756.621826\n",
      "running loss : 48210.740072\n",
      "running loss : 47783.646322\n",
      "running loss : 48204.722738\n",
      "running loss : 47461.079508\n",
      "running loss : 49181.149658\n",
      "running loss : 49097.230957\n",
      "running loss : 50193.783366\n",
      "running loss : 49531.762288\n",
      "running loss : 50378.979818\n",
      "running loss : 48356.587484\n",
      "running loss : 48787.566243\n",
      "running loss : 48377.106934\n",
      "running loss : 48516.516927\n",
      "running loss : 46364.435872\n",
      "running loss : 47041.051025\n",
      "running loss : 47239.184001\n",
      "running loss : 46649.845540\n",
      "running loss : 45625.941976\n",
      "running loss : 48156.247965\n",
      "running loss : 46817.323812\n",
      "running loss : 46268.982544\n",
      "running loss : 45899.153646\n",
      "running loss : 46552.740560\n",
      "running loss : 47220.430013\n",
      "running loss : 47627.274089\n",
      "running loss : 47222.320475\n",
      "running loss : 45934.450358\n",
      "running loss : 47653.203451\n",
      "running loss : 47547.944092\n",
      "running loss : 47703.819173\n",
      "running loss : 46692.880778\n",
      "running loss : 45628.091960\n",
      "running loss : 45902.782715\n",
      "running loss : 46425.142415\n",
      "running loss : 45972.709961\n",
      "running loss : 46971.066650\n",
      "running loss : 46530.842041\n",
      "running loss : 44901.349935\n",
      "running loss : 45541.446289\n",
      "running loss : 44867.612305\n",
      "running loss : 47857.843750\n",
      "running loss : 46523.240641\n",
      "running loss : 47989.493245\n",
      "running loss : 48226.034831\n",
      "running loss : 49315.416585\n",
      "running loss : 46795.908040\n",
      "running loss : 46138.067424\n",
      "running loss : 47623.291016\n",
      "running loss : 46836.219076\n",
      "running loss : 45712.408285\n",
      "running loss : 46073.073893\n",
      "running loss : 47835.127360\n",
      "running loss : 44855.554850\n",
      "running loss : 44278.727946\n",
      "running loss : 44724.735758\n",
      "running loss : 44922.763590\n",
      "running loss : 47036.272542\n",
      "running loss : 45472.129232\n",
      "running loss : 44592.049967\n",
      "running loss : 46900.262044\n",
      "running loss : 45040.774577\n",
      "running loss : 44863.964193\n",
      "running loss : 44892.395020\n",
      "running loss : 50270.711344\n",
      "running loss : 47854.146647\n",
      "running loss : 45389.122803\n",
      "running loss : 44142.645589\n",
      "running loss : 46900.283854\n",
      "running loss : 45170.434408\n",
      "running loss : 44071.296387\n",
      "running loss : 47352.829834\n",
      "running loss : 44197.769775\n",
      "running loss : 47417.472331\n",
      "running loss : 47880.432129\n",
      "running loss : 45117.166829\n",
      "running loss : 43656.764567\n",
      "running loss : 44177.691569\n",
      "running loss : 44232.839600\n",
      "running loss : 48167.512044\n",
      "running loss : 46936.025635\n",
      "running loss : 46363.481201\n",
      "running loss : 45062.203451\n",
      "running loss : 44512.832357\n",
      "running loss : 44358.731445\n",
      "running loss : 44597.669637\n",
      "running loss : 44642.365641\n",
      "running loss : 44932.551676\n",
      "running loss : 46019.332682\n",
      "running loss : 46495.985189\n",
      "running loss : 44827.285238\n",
      "running loss : 45363.432048\n",
      "running loss : 44987.661865\n",
      "running loss : 44964.790446\n",
      "running loss : 47934.064046\n",
      "running loss : 44326.019531\n",
      "running loss : 44429.802327\n",
      "running loss : 43404.033936\n",
      "running loss : 42733.738118\n",
      "running loss : 42979.190999\n",
      "running loss : 43090.233480\n",
      "running loss : 42178.195231\n",
      "running loss : 44282.277262\n",
      "running loss : 43237.059408\n",
      "running loss : 43080.399984\n",
      "running loss : 44342.421956\n",
      "running loss : 45713.942220\n",
      "running loss : 45192.666016\n",
      "running loss : 44188.002686\n",
      "running loss : 43359.221354\n",
      "running loss : 42526.843750\n",
      "running loss : 42504.014079\n",
      "running loss : 44542.493408\n",
      "running loss : 43413.537598\n",
      "running loss : 42975.689697\n",
      "running loss : 41882.025798\n",
      "running loss : 45173.108561\n",
      "running loss : 43113.059163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss : 42049.697184\n",
      "running loss : 43533.153076\n",
      "running loss : 44885.608317\n",
      "running loss : 41994.284668\n",
      "running loss : 42707.935465\n",
      "running loss : 43617.561401\n",
      "running loss : 43501.439535\n",
      "running loss : 45306.684977\n",
      "running loss : 42371.569499\n",
      "running loss : 42649.004028\n",
      "running loss : 42970.214600\n",
      "running loss : 43877.583211\n",
      "running loss : 45055.379883\n",
      "running loss : 47416.100423\n",
      "running loss : 44013.671549\n",
      "running loss : 41930.735921\n",
      "running loss : 42926.273356\n",
      "running loss : 42976.764323\n",
      "running loss : 42344.595866\n",
      "running loss : 45141.950684\n",
      "running loss : 41748.728271\n",
      "running loss : 40846.569661\n",
      "running loss : 40846.616211\n",
      "running loss : 42071.263021\n",
      "running loss : 42872.596273\n",
      "running loss : 41079.228597\n",
      "running loss : 43009.363444\n",
      "running loss : 41749.715007\n",
      "running loss : 43407.815430\n",
      "running loss : 41383.585124\n",
      "running loss : 42660.203613\n",
      "running loss : 39532.865723\n",
      "running loss : 42471.631836\n",
      "running loss : 41117.482422\n",
      "running loss : 42307.607259\n",
      "running loss : 43086.586670\n",
      "running loss : 41912.858398\n",
      "running loss : 40667.849447\n",
      "running loss : 40378.923360\n",
      "running loss : 42101.007955\n",
      "running loss : 43457.886882\n",
      "running loss : 42638.267253\n",
      "running loss : 40191.221517\n",
      "running loss : 42054.164632\n",
      "running loss : 42710.465576\n",
      "running loss : 41776.853027\n",
      "running loss : 40840.724528\n",
      "running loss : 41981.012533\n",
      "running loss : 41827.223470\n",
      "running loss : 42626.150553\n",
      "running loss : 41102.608724\n",
      "running loss : 42965.730469\n",
      "running loss : 40813.096680\n",
      "running loss : 42967.378255\n",
      "running loss : 40754.357422\n",
      "running loss : 43311.801514\n",
      "running loss : 43746.010010\n",
      "running loss : 45314.289225\n",
      "running loss : 44185.326742\n",
      "running loss : 41368.289469\n",
      "running loss : 42592.260905\n",
      "running loss : 42177.947917\n",
      "running loss : 39440.786865\n",
      "running loss : 41175.631348\n",
      "running loss : 40673.101237\n",
      "running loss : 41260.341309\n",
      "running loss : 39842.173096\n",
      "running loss : 39610.854655\n",
      "running loss : 40011.491252\n",
      "running loss : 41294.312744\n",
      "running loss : 40743.686686\n",
      "running loss : 40808.694580\n",
      "running loss : 40922.118083\n",
      "running loss : 41300.705566\n",
      "running loss : 39272.987427\n",
      "running loss : 39944.322591\n",
      "running loss : 40927.354818\n",
      "running loss : 41713.333903\n",
      "running loss : 41205.771159\n",
      "running loss : 40223.961263\n",
      "running loss : 40055.026204\n",
      "running loss : 42504.890137\n",
      "running loss : 38472.325480\n",
      "running loss : 39715.286133\n",
      "running loss : 40104.106201\n",
      "running loss : 39906.485270\n",
      "running loss : 39945.331217\n",
      "running loss : 40454.727132\n",
      "running loss : 40127.358480\n",
      "running loss : 40601.440552\n",
      "running loss : 39349.564697\n",
      "running loss : 39965.462565\n",
      "running loss : 39061.080078\n",
      "running loss : 39247.757243\n",
      "running loss : 40323.256266\n",
      "running loss : 39330.027995\n",
      "running loss : 40946.892822\n",
      "running loss : 39579.459066\n",
      "running loss : 40141.127523\n",
      "running loss : 37958.996094\n",
      "running loss : 38565.373617\n",
      "running loss : 39442.689250\n",
      "running loss : 37960.278198\n",
      "running loss : 40197.711995\n",
      "running loss : 38318.272054\n",
      "running loss : 38624.005941\n",
      "running loss : 39791.647135\n",
      "running loss : 40296.829346\n",
      "running loss : 38651.548828\n",
      "running loss : 39517.404378\n",
      "running loss : 39225.887451\n",
      "running loss : 40284.003174\n",
      "running loss : 39523.394084\n",
      "running loss : 39864.496908\n",
      "running loss : 40773.883952\n",
      "running loss : 40279.895996\n",
      "running loss : 38176.749430\n",
      "running loss : 39640.435221\n",
      "running loss : 40695.060954\n",
      "running loss : 38564.190186\n",
      "running loss : 38240.065918\n",
      "running loss : 39776.977295\n",
      "running loss : 39240.485107\n",
      "running loss : 39634.348633\n",
      "running loss : 39527.217855\n",
      "running loss : 38991.980387\n",
      "running loss : 40660.052409\n",
      "running loss : 39328.340251\n",
      "running loss : 37052.416667\n",
      "running loss : 37462.157878\n",
      "running loss : 36517.768880\n",
      "running loss : 37817.378174\n",
      "running loss : 37819.425456\n",
      "running loss : 39793.552816\n",
      "running loss : 39935.727580\n",
      "running loss : 37920.915934\n",
      "running loss : 39468.699219\n",
      "running loss : 40378.190267\n",
      "running loss : 42313.413981\n",
      "running loss : 41549.432536\n",
      "running loss : 40742.347575\n",
      "running loss : 38147.835856\n",
      "running loss : 37041.001465\n",
      "running loss : 38509.685872\n",
      "running loss : 39745.657308\n",
      "running loss : 39248.717041\n",
      "running loss : 37695.092529\n",
      "running loss : 37377.616374\n",
      "running loss : 37953.819417\n",
      "running loss : 36682.688721\n",
      "running loss : 36722.831787\n",
      "running loss : 37717.070312\n",
      "running loss : 37457.493571\n",
      "running loss : 37760.280558\n",
      "running loss : 38268.555990\n",
      "running loss : 39532.212484\n",
      "running loss : 38873.700765\n",
      "running loss : 37996.628255\n",
      "running loss : 36304.407104\n",
      "running loss : 35136.636841\n",
      "running loss : 36975.792847\n",
      "running loss : 36151.441610\n",
      "running loss : 37569.448730\n",
      "running loss : 37302.001546\n",
      "running loss : 36688.678345\n",
      "running loss : 36615.930908\n",
      "running loss : 36540.654948\n",
      "running loss : 36370.960327\n",
      "running loss : 36948.654378\n",
      "running loss : 36589.127238\n",
      "running loss : 35811.934977\n",
      "running loss : 36479.235596\n",
      "running loss : 37382.861694\n",
      "running loss : 36323.645264\n",
      "running loss : 37198.983073\n",
      "running loss : 36736.359049\n",
      "running loss : 36496.441162\n",
      "running loss : 36162.318237\n",
      "running loss : 37526.738851\n",
      "running loss : 37927.038574\n",
      "running loss : 36195.593913\n",
      "running loss : 35435.948975\n",
      "running loss : 36975.531901\n",
      "running loss : 37372.103190\n",
      "running loss : 35218.944051\n",
      "running loss : 37129.270345\n",
      "running loss : 36393.042765\n",
      "running loss : 36152.630452\n",
      "running loss : 36788.343424\n",
      "running loss : 37108.506755\n",
      "running loss : 37676.685059\n",
      "running loss : 36886.631999\n",
      "running loss : 36423.447673\n",
      "running loss : 35939.713257\n",
      "running loss : 37843.852376\n",
      "running loss : 36102.163167\n",
      "running loss : 36574.401855\n",
      "running loss : 38166.099121\n",
      "running loss : 36151.078857\n",
      "running loss : 37152.074951\n",
      "running loss : 38201.656169\n",
      "running loss : 35700.944499\n",
      "running loss : 37860.992025\n",
      "running loss : 36248.929606\n",
      "running loss : 36788.438558\n",
      "running loss : 36701.525553\n",
      "running loss : 36676.227783\n",
      "running loss : 35208.483602\n",
      "running loss : 36152.297363\n",
      "running loss : 35524.565104\n",
      "running loss : 35788.688883\n",
      "running loss : 36025.366781\n",
      "running loss : 35683.189372\n",
      "running loss : 35550.561117\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def entropy_rate(x_quantized, phi, var):\n",
    "    u = torch.arange(-0.5, 0.5+0.01, 0.01).cuda()        \n",
    "    gsm_sum = torch.zeros(len(u)).cuda()\n",
    "    for i in range(len(u)):\n",
    "        x = x_quantized + u[i]\n",
    "        gsm_sum_i = sum_gsm(x, var, phi, 6)\n",
    "        gsm_sum[i] = gsm_sum_i\n",
    "\n",
    "    entropy = torch.trapz(gsm_sum, u)\n",
    "    \n",
    "    return entropy\n",
    "\"\"\"\n",
    "\n",
    "# Load incremental model\n",
    "incremental_model = LossyCompAutoencoder()\n",
    "incremental_model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_depthmap3_beta1_without_bitrate.pth'))\n",
    "incremental_model.train()\n",
    "incremental_model.to(device)\n",
    "\n",
    "# train again the model starting form incremental-learned weights\n",
    "    #define optimizer\n",
    "optimizer = torch.optim.Adam(incremental_model.parameters(), lr=0.00001)\n",
    "\n",
    "# define loss function\n",
    "distortion = nn.MSELoss().cuda()\n",
    "\n",
    "\n",
    "# define beta\n",
    "beta = 1.0\n",
    "\n",
    "#Epochs\n",
    "n_epochs = 800\n",
    "\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "           \n",
    "    #Training\n",
    "    for i_batch, data in enumerate(dataloader):\n",
    "        batch_images = data.to(device).float()\n",
    "        [decoded_images, x_quantized] = incremental_model(batch_images, 1, True, False)\n",
    "        optimizer.zero_grad()\n",
    "        #entropy = entropy_rate(x_quantized, incremental_model.phi, incremental_model.var)\n",
    "        #print(\"entropy : \", entropy)\n",
    "        dist = distortion(decoded_images, batch_images)\n",
    "        #print(\"distortion : \", dist)\n",
    "        #loss = beta * dist + entropy\n",
    "        loss = beta*dist\n",
    "        loss.backward()\n",
    "        #print(\"conv1.weights grad: \", params[0].grad)\n",
    "        #print(model.conv1.bias.grad)\n",
    "        #print(model.conv1.weight.grad)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss/len(dataloader)\n",
    "    print('running loss : {:.06f}'.format(running_loss), )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from range_coder import RangeEncoder, RangeDecoder, prob_to_cum_freq\n",
    "import os\n",
    "\n",
    "# Load previous model\n",
    "model = LossyCompAutoencoder()\n",
    "model.load_state_dict(torch.load('./model_parameters/mean_bit_ppx/lossy_comp_params_with_rate_beta0005_incremental.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "nb_bits = 0.0\n",
    "test_dataset = ImageDataset(root_dir='./data/kodac/', transform=ToTensor())\n",
    "with torch.no_grad():  \n",
    "    for i in range(len(test_dataset)):\n",
    "        test_image = test_dataset[i].unsqueeze(0).to(device).float()\n",
    "        [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "            # compute symbol probabilities\n",
    "        min_val = data_comp.min()\n",
    "        if min_val <0:\n",
    "            data_comp -= min_val\n",
    "            min_val = 0\n",
    "        max_val = data_comp.max()\n",
    "        nb_bins = max_val - min_val + 1\n",
    "        hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "        prob = hist/hist.sum()\n",
    "        #print(\"data comp : \", data_comp)\n",
    "        #print(prob)\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(torch.nonzero(prob)) != len(prob):\n",
    "            zero_indices = ((prob == 0).nonzero())\n",
    "            for j in reversed(range(0, len(zero_indices), 1)):\n",
    "                data_comp[data_comp > int(zero_indices[j])+min_val] -=1\n",
    "            min_val = data_comp.min()\n",
    "            max_val = data_comp.max()\n",
    "            nb_bins = max_val - min_val + 1\n",
    "            hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "            prob = hist/hist.sum()\n",
    "            print(min_val)\n",
    "            print(max_val)\n",
    "            print(\"data comp : \", data_comp)\n",
    "            print(prob)\n",
    "         \"\"\" \n",
    "            \n",
    "            # convert probabilities to cumulative integer frequency table\n",
    "        #cumFreq = prob_to_cum_freq(torch.clamp(prob, min=np.finfo(np.float32).eps).cpu(), resolution=128)\n",
    "        cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "        #print(cumFreq)\n",
    "        \n",
    "        # encode data\n",
    "        filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(i) + \".bin\"\n",
    "        encoder = RangeEncoder(filepath_to_write)\n",
    "        #print(torch.flatten(data_comp).cpu().tolist())\n",
    "        encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "        encoder.close()\n",
    "        \n",
    "        \n",
    "        file_size = os.path.getsize(filepath_to_write)*8 #number of bits in the file\n",
    "        print(file_size)\n",
    "        nb_bits += file_size\n",
    "        \n",
    "    nb_bits_per_image = nb_bits/len(test_dataset)\n",
    "    print(nb_bits_per_image)\n",
    "    nb_bits_per_pixel = nb_bits_per_image/(512*768)\n",
    "    print(nb_bits_per_pixel)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "    test_image = test_dataset[1].unsqueeze(0).to(device).float()\n",
    "    [reconstructed_image, data_comp] = model(test_image, 1, True)\n",
    "        # compute symbol probabilities\n",
    "    min_val = data_comp.min()\n",
    "    print(min_val)\n",
    "    print(data_comp.max())\n",
    "    if min_val <0:\n",
    "        data_comp -= min_val\n",
    "        min_val = 0\n",
    "    max_val = data_comp.max()\n",
    "    \n",
    "    print(max_val)\n",
    "    nb_bins = max_val - min_val + 1\n",
    "    print(nb_bins)\n",
    "    hist = torch.histc(data_comp, bins=nb_bins.int(), min=min_val, max=max_val)\n",
    "    prob = hist/hist.sum()\n",
    "    print(prob)\n",
    "         # convert probabilities to cumulative integer frequency table\n",
    "    cumFreq = prob_to_cum_freq(prob.cpu(), resolution=128)\n",
    "    #print(cumFreq)\n",
    "\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    \n",
    "    \n",
    "        # encode data\n",
    "    filepath_to_write = \"D:\\\\lossy_autoencoder\\\\latent_vect_encoded\\\\\" + \"img\" + str(1) + \".bin\"\n",
    "    encoder = RangeEncoder(filepath_to_write)\n",
    "    print(torch.flatten(data_comp).cpu().tolist())\n",
    "    encoder.encode(torch.flatten(data_comp.int()).cpu().tolist(), cumFreq)\n",
    "    encoder.close()   \n",
    "\"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\ni = 0\\ntest_image = test_dataset[i].unsqueeze(0).float()\\n\\n#test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\\ntest_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\\nax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\\nplt.imshow(torch.squeeze(test_image_without_black_px))\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "test_dataset = ImageDataset(root_dir='D:/autoencoder_data/depthmaps/test', transform=transforms.Compose([RandomCrop((480, 640)), ToTensor()]))\n",
    "#fig, axes = plt.subplots(nrows=4, ncols=6, sharex=True, sharey=True, figsize=(8,8))\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    test_image = test_dataset[i].unsqueeze(0).float().cuda()\n",
    "\n",
    "    #test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "    test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "    cv2.imwrite(\"D:\\\\autoencoder_data\\\\depthmaps2\\\\dilated\\\\\" + \"img\" + str(i)+\".png\", np.squeeze(test_image_without_black_px.cpu().numpy()).astype(np.uint16))\n",
    "    #ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "    #plt.imshow(torch.squeeze(test_image_without_black_px).cpu())\n",
    "\n",
    "\"\"\"    \n",
    "i = 0\n",
    "test_image = test_dataset[i].unsqueeze(0).float()\n",
    "\n",
    "#test_image_without_black_px = black_pixels_removal_by_dilatation(test_image, torch.ones(3, 5))\n",
    "test_image_without_black_px = black_pixels_removal_by_dilatation(test_image)\n",
    "ax = fig.add_subplot(4, 6, i+1, xticks=[], yticks=[])\n",
    "plt.imshow(torch.squeeze(test_image_without_black_px))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
